<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.12">
<title>OpenSearch Reference</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/* Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment @import statement to use as custom stylesheet */
/*@import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700";*/
article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section{display:block}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
abbr[title]{border-bottom:1px dotted}
b,strong{font-weight:bold}
dfn{font-style:italic}
hr{-moz-box-sizing:content-box;box-sizing:content-box;height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,*::before,*::after{-moz-box-sizing:border-box;-webkit-box-sizing:border-box;box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;font-weight:400;font-style:normal;line-height:1;position:relative;cursor:auto;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{font-family:inherit;font-weight:400;font-size:1em;line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em;height:0}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{font-size:1em;line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0;font-size:1em}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
abbr,acronym{text-transform:uppercase;font-size:90%;color:rgba(0,0,0,.8);border-bottom:1px dotted #ddd;cursor:help}
abbr{text-transform:none}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote cite{display:block;font-size:.9375em;color:rgba(0,0,0,.6)}
blockquote cite::before{content:"\2014 \0020"}
blockquote cite a,blockquote cite a:visited{color:rgba(0,0,0,.6)}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:solid 1px #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;-webkit-border-radius:4px;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;-webkit-border-radius:3px;border-radius:3px;-webkit-box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em white inset;box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em #fff inset;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin-left:auto;margin-right:auto;margin-top:0;margin-bottom:0;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:-ms-flexbox;display:-webkit-flex;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:rgba(255,255,255,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details>summary:first-of-type{cursor:pointer;display:list-item;outline:none;margin-bottom:.75em}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class="paragraph"]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
table.tableblock #preamble>.sectionbody>[class="paragraph"]:first-of-type p{font-size:inherit}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border-style:solid;border-width:1px;border-color:#e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;-webkit-border-radius:4px;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border-style:solid;border-width:1px;border-color:#dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;-webkit-border-radius:4px;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{-webkit-border-radius:4px;border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class="highlight"],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;-webkit-border-radius:4px;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos{border-right:1px solid currentColor;opacity:.35;padding-right:.5em}
pre.pygments .lineno{border-right:1px solid currentColor;opacity:.35;display:inline-block;margin-right:.75em}
pre.pygments .lineno::before{content:"";margin-right:-.125em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all tr,table.stripes-odd tr:nth-of-type(odd),table.stripes-even tr:nth-of-type(even),table.stripes-hover tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
ol>li p,ul>li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
ul.checklist{margin-left:.625em}
ul.checklist li>p:first-child>.fa-square-o:first-child,ul.checklist li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist li>p:first-child>input[type="checkbox"]:first-child{margin-right:.25em}
ul.inline{display:-ms-flexbox;display:-webkit-box;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:solid 4px #fff;-webkit-box-shadow:0 0 0 1px #ddd;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
.gist .file-data>table{border:0;background:#fff;width:100%;margin-bottom:0}
.gist .file-data>table td.line-data{width:99%}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);-webkit-border-radius:50%;border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,span.alt{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;-webkit-box-shadow:0 1px 4px #e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{-webkit-box-shadow:none!important;box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media print,amzn-kf8{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<style>.toc-current{font-weight: bold;} .toc-root{font-family: "Open Sans","DejaVu Sans",sans-serif;
                       font-size: 0.9em;} #content{display: flex; flex-direction: column; flex: 1 1 auto;}
             .nav-footer{text-align: center; margin-top: auto;}
             .nav-footer > p > a {white-space: nowrap;}</style>
</head>
<body id="analysis-tokenfilters" class="book toc2 toc-left">
<div id="header">
<h1>OpenSearch Reference</h1>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<p><span class="toc-root"><a href="index.html">OpenSearch Reference</a></span></p><ul class="sectlevel1">
<li><a href="opensearch-intro.html">What is OpenSearch?</a>
</li>
<li><a href="getting-started.html">Getting started with OpenSearch</a>
</li>
<li><a href="setup.html">Set up OpenSearch</a>
</li>
<li><a href="setup-upgrade.html">Upgrade OpenSearch</a>
</li>
<li><a href="index-modules.html">Index modules</a>
</li>
<li><a href="mapping.html">Mapping</a>
</li>
<li><a href="analysis.html">Text analysis</a>
<ul class="sectlevel1">
<li><a href="analysis-overview.html">Text analysis overview</a>
</li>
<li><a href="analysis-concepts.html">Text analysis concepts</a>
</li>
<li><a href="configure-text-analysis.html">Configure text analysis</a>
</li>
<li><a href="analysis-analyzers.html">Built-in analyzer reference</a>
</li>
<li><a href="analysis-tokenizers.html">Tokenizer reference</a>
</li>
<li><a href="analysis-tokenfilters.html"><span class="toc-current">Token filter reference</span></a>
<ul class="sectlevel2">
<li><a href="analysis-tokenfilters.html#analysis-apostrophe-tokenfilter">Apostrophe token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-asciifolding-tokenfilter">ASCII folding token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-cjk-bigram-tokenfilter">CJK bigram token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-cjk-width-tokenfilter">CJK width token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-classic-tokenfilter">Classic token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-common-grams-tokenfilter">Common grams token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-condition-tokenfilter">Conditional token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-decimal-digit-tokenfilter">Decimal digit token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-delimited-payload-tokenfilter">Delimited payload token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-dict-decomp-tokenfilter">Dictionary decompounder token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-edgengram-tokenfilter">Edge n-gram token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-elision-tokenfilter">Elision token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-fingerprint-tokenfilter">Fingerprint token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-flatten-graph-tokenfilter">Flatten graph token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-hunspell-tokenfilter">Hunspell token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-hyp-decomp-tokenfilter">Hyphenation decompounder token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-keep-types-tokenfilter">Keep types token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-keep-words-tokenfilter">Keep words token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-keyword-marker-tokenfilter">Keyword marker token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-keyword-repeat-tokenfilter">Keyword repeat token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-kstem-tokenfilter">KStem token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-length-tokenfilter">Length token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-limit-token-count-tokenfilter">Limit token count token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-lowercase-tokenfilter">Lowercase token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-minhash-tokenfilter">MinHash token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-multiplexer-tokenfilter">Multiplexer token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-ngram-tokenfilter">N-gram token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-normalization-tokenfilter">Normalization token filters</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-pattern-capture-tokenfilter">Pattern capture token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-pattern_replace-tokenfilter">Pattern replace token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-phonetic-tokenfilter">Phonetic token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-porterstem-tokenfilter">Porter stem token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-predicatefilter-tokenfilter">Predicate script token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-remove-duplicates-tokenfilter">Remove duplicates token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-reverse-tokenfilter">Reverse token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-shingle-tokenfilter">Shingle token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-snowball-tokenfilter">Snowball token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter">Stemmer token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-stemmer-override-tokenfilter">Stemmer override token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-stop-tokenfilter">Stop token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-synonym-tokenfilter">Synonym token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-synonym-graph-tokenfilter">Synonym graph token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-trim-tokenfilter">Trim token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-truncate-tokenfilter">Truncate token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-unique-tokenfilter">Unique token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-uppercase-tokenfilter">Uppercase token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-word-delimiter-tokenfilter">Word delimiter token filter</a>
</li>
<li><a href="analysis-tokenfilters.html#analysis-word-delimiter-graph-tokenfilter">Word delimiter graph token filter</a>
</li>
</ul>
</li>
<li><a href="analysis-charfilters.html">Character filters reference</a>
</li>
<li><a href="analysis-normalizers.html">Normalizers</a>
</li>
</ul>
</li>
<li><a href="index-templates.html">Index templates</a>
</li>
<li><a href="ingest.html">Ingest node</a>
</li>
<li><a href="search-your-data.html">Search your data</a>
</li>
<li><a href="query-dsl.html">Query DSL</a>
</li>
<li><a href="search-aggregations.html">Aggregations</a>
</li>
<li><a href="modules-scripting.html">Scripting</a>
</li>
<li><a href="high-availability.html">Set up a cluster for high availability</a>
</li>
<li><a href="snapshot-restore.html">Snapshot and restore</a>
</li>
<li><a href="commands.html">Command line tools</a>
</li>
<li><a href="how-to.html">How To</a>
</li>
<li><a href="glossary.html">Glossary of terms</a>
</li>
<li><a href="rest-apis.html">REST APIs</a>
</li>
<li><a href="breaking-changes.html">Migration guide</a>
</li>
<li><a href="opensearch-release-notes.html">Release notes</a>
</li>
</ul>
</div>
</div>
<div id="content">
<div class="sect1">
<h2 id="analysis-tokenfilters">Token filter reference</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Token filters accept a stream of tokens from a
<a href="analysis-tokenizers.html">tokenizer</a> and can modify tokens
(eg lowercasing), delete tokens (eg remove stopwords)
or add tokens (eg synonyms).</p>
</div>
<div class="paragraph">
<p>OpenSearch has a number of built-in token filters you can use
to build <a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzers</a>.</p>
</div>
<div class="sect2">
<h3 id="analysis-apostrophe-tokenfilter">Apostrophe token filter</h3>
<titleabbrev>Apostrophe</titleabbrev>
<div class="paragraph">
<p>Strips all characters after an apostrophe, including the apostrophe itself.</p>
</div>
<div class="paragraph">
<p>This filter is included in OpenSearch&#8217;s built-in <a href="analysis-analyzers.html#turkish-analyzer">Turkish language
analyzer</a>. It uses Lucene&#8217;s
{lucene-analysis-docs}/tr/ApostropheFilter.html[ApostropheFilter], which was
built for the Turkish language.</p>
</div>
<div class="sect3">
<h4 id="analysis-apostrophe-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request demonstrates how the
apostrophe token filter works.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer" : "standard",
  "filter" : ["apostrophe"],
  "text" : "Istanbul'a veya Istanbul'dan"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ Istanbul, veya, Istanbul ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-apostrophe-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
apostrophe token filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /apostrophe_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_apostrophe": {
          "tokenizer": "standard",
          "filter": [ "apostrophe" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-asciifolding-tokenfilter">ASCII folding token filter</h3>
<titleabbrev>ASCII folding</titleabbrev>
<div class="paragraph">
<p>Converts alphabetic, numeric, and symbolic characters that are not in the Basic
Latin Unicode block (first 127 ASCII characters) to their ASCII equivalent, if
one exists. For example, the filter changes <code>à</code> to <code>a</code>.</p>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/miscellaneous/ASCIIFoldingFilter.html[ASCIIFoldingFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-asciifolding-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>asciifolding</code>
filter to drop the diacritical marks in <code>açaí à la carte</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer" : "standard",
  "filter" : ["asciifolding"],
  "text" : "açaí à la carte"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ acai, a, la, carte ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-asciifolding-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>asciifolding</code> filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /asciifold_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_asciifolding": {
          "tokenizer": "standard",
          "filter": [ "asciifolding" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-asciifolding-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>preserve_original</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, emit both original tokens and folded tokens.
Defaults to <code>false</code>.</p>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-asciifolding-tokenfilter-customize">Customize</h4>
<div class="paragraph">
<p>To customize the <code>asciifolding</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following request creates a custom <code>asciifolding</code> filter with
<code>preserve_original</code> set to true:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /asciifold_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_asciifolding": {
          "tokenizer": "standard",
          "filter": [ "my_ascii_folding" ]
        }
      },
      "filter": {
        "my_ascii_folding": {
          "type": "asciifolding",
          "preserve_original": true
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-cjk-bigram-tokenfilter">CJK bigram token filter</h3>
<titleabbrev>CJK bigram</titleabbrev>
<div class="paragraph">
<p>Forms <a href="https://en.wikipedia.org/wiki/Bigram">bigrams</a> out of CJK (Chinese,
Japanese, and Korean) tokens.</p>
</div>
<div class="paragraph">
<p>This filter is included in OpenSearch&#8217;s built-in <a href="analysis-analyzers.html#cjk-analyzer">CJK language
analyzer</a>. It uses Lucene&#8217;s
{lucene-analysis-docs}/cjk/CJKBigramFilter.html[CJKBigramFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-cjk-bigram-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request demonstrates how the
CJK bigram token filter works.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer" : "standard",
  "filter" : ["cjk_bigram"],
  "text" : "東京都は、日本の首都であり"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ 東京, 京都, 都は, 日本, 本の, の首, 首都, 都で, であ, あり ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-cjk-bigram-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
CJK bigram token filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /cjk_bigram_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_cjk_bigram": {
          "tokenizer": "standard",
          "filter": [ "cjk_bigram" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-cjk-bigram-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>ignored_scripts</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, array of character scripts)
Array of character scripts for which to disable bigrams.
Possible values:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>han</code></p>
</li>
<li>
<p><code>hangul</code></p>
</li>
<li>
<p><code>hiragana</code></p>
</li>
<li>
<p><code>katakana</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>All non-CJK input is passed through unmodified.</p>
</div>
</div>
</div>
</dd>
</dl>
</div>
<div class="paragraph">
<p><code>output_unigrams</code>
(Optional, Boolean)
If <code>true</code>, emit tokens in both bigram and
<a href="https://en.wikipedia.org/wiki/N-gram">unigram</a> form. If <code>false</code>, a CJK character
is output in unigram form when it has no adjacent characters. Defaults to
<code>false</code>.</p>
</div>
</div>
<div class="sect3">
<h4 id="analysis-cjk-bigram-tokenfilter-customize">Customize</h4>
<div class="paragraph">
<p>To customize the CJK bigram token filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /cjk_bigram_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "han_bigrams": {
          "tokenizer": "standard",
          "filter": [ "han_bigrams_filter" ]
        }
      },
      "filter": {
        "han_bigrams_filter": {
          "type": "cjk_bigram",
          "ignored_scripts": [
            "hangul",
            "hiragana",
            "katakana"
          ],
          "output_unigrams": true
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-cjk-width-tokenfilter">CJK width token filter</h3>
<titleabbrev>CJK width</titleabbrev>
<div class="paragraph">
<p>Normalizes width differences in CJK (Chinese, Japanese, and Korean) characters
as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Folds full-width ASCII character variants into the equivalent basic Latin
characters</p>
</li>
<li>
<p>Folds half-width Katakana character variants into the equivalent Kana
characters</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This filter is included in OpenSearch&#8217;s built-in <a href="analysis-analyzers.html#cjk-analyzer">CJK language
analyzer</a>. It uses Lucene&#8217;s
{lucene-analysis-docs}/cjk/CJKWidthFilter.html[CJKWidthFilter].</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
This token filter can be viewed as a subset of NFKC/NFKD Unicode
normalization. See the
<a href="https://www.opensearch.org/guide/en/opensearch/plugins/{branch}/analysis-icu-normalization-charfilter.html"><code>analysis-icu</code> plugin</a> for
full normalization support.
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="analysis-cjk-width-tokenfilter-analyze-ex">Example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer" : "standard",
  "filter" : ["cjk_width"],
  "text" : "ｼｰｻｲﾄﾞﾗｲﾅｰ"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following token:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">シーサイドライナー</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-cjk-width-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
CJK width token filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /cjk_width_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_cjk_width": {
          "tokenizer": "standard",
          "filter": [ "cjk_width" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-classic-tokenfilter">Classic token filter</h3>
<titleabbrev>Classic</titleabbrev>
<div class="paragraph">
<p>Performs optional post-processing of terms generated by the
<a href="analysis-tokenizers.html#analysis-classic-tokenizer"><code>classic</code> tokenizer</a>.</p>
</div>
<div class="paragraph">
<p>This filter removes the english possessive (<code>'s</code>) from the end of words and
removes dots from acronyms. It uses Lucene&#8217;s
{lucene-analysis-docs}/standard/ClassicFilter.html[ClassicFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-classic-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request demonstrates how the
classic token filter works.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer" : "classic",
  "filter" : ["classic"],
  "text" : "The 2 Q.U.I.C.K. Brown-Foxes jumped over the lazy dog's bone."
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ The, 2, QUICK, Brown, Foxes, jumped, over, the, lazy, dog, bone ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-classic-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
classic token filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /classic_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "classic_analyzer": {
          "tokenizer": "classic",
          "filter": [ "classic" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-common-grams-tokenfilter">Common grams token filter</h3>
<titleabbrev>Common grams</titleabbrev>
<div class="paragraph">
<p>Generates <a href="https://en.wikipedia.org/wiki/Bigram">bigrams</a> for a specified set of
common words.</p>
</div>
<div class="paragraph">
<p>For example, you can specify <code>is</code> and <code>the</code> as common words. This filter then
converts the tokens <code>[the, quick, fox, is, brown]</code> to <code>[the, the_quick, quick,
fox, fox_is, is, is_brown, brown]</code>.</p>
</div>
<div class="paragraph">
<p>You can use the <code>common_grams</code> filter in place of the
<a href="analysis-tokenfilters.html#analysis-stop-tokenfilter">stop token filter</a> when you don&#8217;t want to
completely ignore common words.</p>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/commongrams/CommonGramsFilter.html[CommonGramsFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-common-grams-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request creates bigrams for <code>is</code>
and <code>the</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer" : "whitespace",
  "filter" : [
    {
      "type": "common_grams",
      "common_words": ["is", "the"]
    }
  ],
  "text" : "the quick fox is brown"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ the, the_quick, quick, fox, fox_is, is, is_brown, brown ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-common-grams-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>common_grams</code> filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /common_grams_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "index_grams": {
          "tokenizer": "whitespace",
          "filter": [ "common_grams" ]
        }
      },
      "filter": {
        "common_grams": {
          "type": "common_grams",
          "common_words": [ "a", "is", "the" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-common-grams-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>common_words</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Required*, array of strings)
A list of tokens. The filter generates bigrams for these tokens.</p>
</div>
<div class="paragraph">
<p>Either this or the <code>common_words_path</code> parameter is required.</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>common_words_path</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Required*, string)
Path to a file containing a list of tokens. The filter generates bigrams for
these tokens.</p>
</div>
<div class="paragraph">
<p>This path must be absolute or relative to the <code>config</code> location. The file must
be UTF-8 encoded. Each token in the file must be separated by a line break.</p>
</div>
<div class="paragraph">
<p>Either this or the <code>common_words</code> parameter is required.</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>ignore_case</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, matches for common words matching are case-insensitive.
Defaults to <code>false</code>.</p>
</dd>
<dt class="hdlist1"><code>query_mode</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, Boolean)
If <code>true</code>, the filter excludes the following tokens from the output:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Unigrams for common words</p>
</li>
<li>
<p>Unigrams for terms followed by common words</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Defaults to <code>false</code>. We recommend enabling this parameter for
<a href="mapping-params.html#search-analyzer">search analyzers</a>.</p>
</div>
<div class="paragraph">
<p>For example, you can enable this parameter and specify <code>is</code> and <code>the</code> as
common words. This filter converts the tokens <code>[the, quick, fox, is, brown]</code> to
<code>[the_quick, quick, fox_is, is_brown,]</code>.</p>
</div>
</div>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-common-grams-tokenfilter-customize">Customize</h4>
<div class="paragraph">
<p>To customize the <code>common_grams</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following request creates a custom <code>common_grams</code> filter with
<code>ignore_case</code> and <code>query_mode</code> set to <code>true</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /common_grams_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "index_grams": {
          "tokenizer": "whitespace",
          "filter": [ "common_grams_query" ]
        }
      },
      "filter": {
        "common_grams_query": {
          "type": "common_grams",
          "common_words": [ "a", "is", "the" ],
          "ignore_case": true,
          "query_mode": true
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-condition-tokenfilter">Conditional token filter</h3>
<titleabbrev>Conditional</titleabbrev>
<div class="paragraph">
<p>Applies a set of token filters to tokens that match conditions in a provided
predicate script.</p>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/miscellaneous/ConditionalTokenFilter.html[ConditionalTokenFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-condition-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>condition</code>
filter to match tokens with fewer than 5 characters in <code>THE QUICK BROWN FOX</code>.
It then applies the <a href="analysis-tokenfilters.html#analysis-lowercase-tokenfilter"><code>lowercase</code></a> filter to
those matching tokens, converting them to lowercase.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "condition",
      "filter": [ "lowercase" ],
      "script": {
        "source": "token.getTerm().length() &lt; 5"
      }
    }
  ],
  "text": "THE QUICK BROWN FOX"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ the, QUICK, BROWN, fox ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-condition-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>filter</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Required, array of token filters)
Array of token filters. If a token matches the predicate script in the <code>script</code>
parameter, these filters are applied to the token in the order provided.</p>
</div>
<div class="paragraph">
<p>These filters can include custom token filters defined in the index mapping.</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>script</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Required, <a href="modules-scripting-using.html">script object</a>)
Predicate script used to apply token filters. If a token
matches this script, the filters in the <code>filter</code> parameter are applied to the
token.</p>
</div>
<div class="paragraph">
<p>For valid parameters, see <a href="modules-scripting-using.html#_script_parameters">Script parameters</a>. Only inline scripts are
supported. Painless scripts are executed in the
<a href="https://www.opensearch.org/guide/en/opensearch/painless/{branch}/painless-analysis-predicate-context.html">analysis predicate context</a>
and require a <code>token</code> property.</p>
</div>
</div>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-condition-tokenfilter-customize">Customize and add to an analyzer</h4>
<div class="paragraph">
<p>To customize the <code>condition</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following <a href="indices.html#indices-create-index">create index API</a> request
uses a custom <code>condition</code> filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>. The custom <code>condition</code> filter
matches the first token in a stream. It then reverses that matching token using
the <a href="analysis-tokenfilters.html#analysis-reverse-tokenfilter"><code>reverse</code></a> filter.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /palindrome_list
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_reverse_first_token": {
          "tokenizer": "whitespace",
          "filter": [ "reverse_first_token" ]
        }
      },
      "filter": {
        "reverse_first_token": {
          "type": "condition",
          "filter": [ "reverse" ],
          "script": {
            "source": "token.getPosition() === 0"
          }
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-decimal-digit-tokenfilter">Decimal digit token filter</h3>
<titleabbrev>Decimal digit</titleabbrev>
<div class="paragraph">
<p>Converts all digits in the Unicode <code>Decimal_Number</code> General Category to <code>0-9</code>.
For example, the filter changes the Bengali numeral <code>৩</code> to <code>3</code>.</p>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/core/DecimalDigitFilter.html[DecimalDigitFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-decimal-digit-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>decimal_digit</code>
filter to convert Devanagari numerals to <code>0-9</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer" : "whitespace",
  "filter" : ["decimal_digit"],
  "text" : "१-one two-२ ३"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ 1-one, two-2, 3]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-decimal-digit-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>decimal_digit</code> filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /decimal_digit_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_decimal_digit": {
          "tokenizer": "whitespace",
          "filter": [ "decimal_digit" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-delimited-payload-tokenfilter">Delimited payload token filter</h3>
<titleabbrev>Delimited payload</titleabbrev>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph">
<p>The older name <code>delimited_payload_filter</code> is deprecated and should not be used
with new indices. Use <code>delimited_payload</code> instead.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Separates a token stream into tokens and payloads based on a specified
delimiter.</p>
</div>
<div class="paragraph">
<p>For example, you can use the <code>delimited_payload</code> filter with a <code>|</code> delimiter to
split <code>the|1 quick|2 fox|3</code> into the tokens <code>the</code>, <code>quick</code>, and <code>fox</code>
with respective payloads of <code>1</code>, <code>2</code>, and <code>3</code>.</p>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/payloads/DelimitedPayloadTokenFilter.html[DelimitedPayloadTokenFilter].</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="title">Payloads</div>
<div class="paragraph">
<p>A payload is user-defined binary data associated with a token position and
stored as base64-encoded bytes.</p>
</div>
<div class="paragraph">
<p>OpenSearch does not store token payloads by default. To store payloads, you must:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Set the <a href="mapping-params.html#term-vector"><code>term_vector</code></a> mapping parameter to
<code>with_positions_payloads</code> or <code>with_positions_offsets_payloads</code> for any field
storing payloads.</p>
</li>
<li>
<p>Use an index analyzer that includes the <code>delimited_payload</code> filter</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can view stored payloads using the <a href="docs.html#docs-termvectors">term vectors API</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="analysis-delimited-payload-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the
<code>delimited_payload</code> filter with the default <code>|</code> delimiter to split
<code>the|0 brown|10 fox|5 is|0 quick|10</code> into tokens and payloads.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer": "whitespace",
  "filter": ["delimited_payload"],
  "text": "the|0 brown|10 fox|5 is|0 quick|10"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ the, brown, fox, is, quick ]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note that the analyze API does not return stored payloads. For an example that
includes returned payloads, see
<a href="analysis-tokenfilters.html#analysis-delimited-payload-tokenfilter-return-stored-payloads">Return stored payloads</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="analysis-delimited-payload-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>delimited-payload</code> filter to configure a new <a href="configure-text-analysis.html#analysis-custom-analyzer">custom
analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT delimited_payload
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_delimited_payload": {
          "tokenizer": "whitespace",
          "filter": [ "delimited_payload" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-delimited-payload-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>delimiter</code></dt>
<dd>
<p>(Optional, string)
Character used to separate tokens from payloads. Defaults to <code>|</code>.</p>
</dd>
<dt class="hdlist1"><code>encoding</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, string)
Data type for the stored payload. Valid values are:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>float</code></dt>
<dd>
<p>(Default) Float</p>
</dd>
<dt class="hdlist1"><code>identity</code></dt>
<dd>
<p>Characters</p>
</dd>
<dt class="hdlist1"><code>int</code></dt>
<dd>
<p>Integer</p>
</dd>
</dl>
</div>
</div>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-delimited-payload-tokenfilter-customize">Customize and add to an analyzer</h4>
<div class="paragraph">
<p>To customize the <code>delimited_payload</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following <a href="indices.html#indices-create-index">create index API</a> request
uses a custom <code>delimited_payload</code> filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>. The custom <code>delimited_payload</code>
filter uses the <code>+</code> delimiter to separate tokens from payloads. Payloads are
encoded as integers.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT delimited_payload_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_plus_delimited": {
          "tokenizer": "whitespace",
          "filter": [ "plus_delimited" ]
        }
      },
      "filter": {
        "plus_delimited": {
          "type": "delimited_payload",
          "delimiter": "+",
          "encoding": "int"
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-delimited-payload-tokenfilter-return-stored-payloads">Return stored payloads</h4>
<div class="paragraph">
<p>Use the <a href="indices.html#indices-create-index">create index API</a> to create an index that:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Includes a field that stores term vectors with payloads.</p>
</li>
<li>
<p>Uses a <a href="configure-text-analysis.html#analysis-custom-analyzer">custom index analyzer</a> with the
<code>delimited_payload</code> filter.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT text_payloads
{
  "mappings": {
    "properties": {
      "text": {
        "type": "text",
        "term_vector": "with_positions_payloads",
        "analyzer": "payload_delimiter"
      }
    }
  },
  "settings": {
    "analysis": {
      "analyzer": {
        "payload_delimiter": {
          "tokenizer": "whitespace",
          "filter": [ "delimited_payload" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Add a document containing payloads to the index.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">POST text_payloads/_doc/1
{
  "text": "the|0 brown|3 fox|4 is|0 quick|10"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Use the <a href="docs.html#docs-termvectors">term vectors API</a> to return the document&#8217;s tokens
and base64-encoded payloads.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET text_payloads/_termvectors/1
{
  "fields": [ "text" ],
  "payloads": true
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The API returns the following response:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console-result" data-lang="console-result">{
  "_index": "text_payloads",
  "_type": "_doc",
  "_id": "1",
  "_version": 1,
  "found": true,
  "took": 8,
  "term_vectors": {
    "text": {
      "field_statistics": {
        "sum_doc_freq": 5,
        "doc_count": 1,
        "sum_ttf": 5
      },
      "terms": {
        "brown": {
          "term_freq": 1,
          "tokens": [
            {
              "position": 1,
              "payload": "QEAAAA=="
            }
          ]
        },
        "fox": {
          "term_freq": 1,
          "tokens": [
            {
              "position": 2,
              "payload": "QIAAAA=="
            }
          ]
        },
        "is": {
          "term_freq": 1,
          "tokens": [
            {
              "position": 3,
              "payload": "AAAAAA=="
            }
          ]
        },
        "quick": {
          "term_freq": 1,
          "tokens": [
            {
              "position": 4,
              "payload": "QSAAAA=="
            }
          ]
        },
        "the": {
          "term_freq": 1,
          "tokens": [
            {
              "position": 0,
              "payload": "AAAAAA=="
            }
          ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-dict-decomp-tokenfilter">Dictionary decompounder token filter</h3>
<titleabbrev>Dictionary decompounder</titleabbrev>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>In most cases, we recommend using the faster
<a href="analysis-tokenfilters.html#analysis-hyp-decomp-tokenfilter"><code>hyphenation_decompounder</code></a> token filter
in place of this filter. However, you can use the
<code>dictionary_decompounder</code> filter to check the quality of a word list before
implementing it in the <code>hyphenation_decompounder</code> filter.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Uses a specified list of words and a brute force approach to find subwords in
compound words. If found, these subwords are included in the token output.</p>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/compound/DictionaryCompoundWordTokenFilter.html[DictionaryCompoundWordTokenFilter],
which was built for Germanic languages.</p>
</div>
<div class="sect3">
<h4 id="analysis-dict-decomp-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the
<code>dictionary_decompounder</code> filter to find subwords in <code>Donaudampfschiff</code>. The
filter then checks these subwords against the specified list of words: <code>Donau</code>,
<code>dampf</code>, <code>meer</code>, and <code>schiff</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "dictionary_decompounder",
      "word_list": ["Donau", "dampf", "meer", "schiff"]
    }
  ],
  "text": "Donaudampfschiff"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ Donaudampfschiff, Donau, dampf, schiff ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-dict-decomp-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>word_list</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Required*, array of strings)
A list of subwords to look for in the token stream. If found, the subword is
included in the token output.</p>
</div>
<div class="paragraph">
<p>Either this parameter or <code>word_list_path</code> must be specified.</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>word_list_path</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Required*, string)
Path to a file that contains a list of subwords to find in the token stream. If
found, the subword is included in the token output.</p>
</div>
<div class="paragraph">
<p>This path must be absolute or relative to the <code>config</code> location, and the file
must be UTF-8 encoded. Each token in the file must be separated by a line break.</p>
</div>
<div class="paragraph">
<p>Either this parameter or <code>word_list</code> must be specified.</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>max_subword_size</code></dt>
<dd>
<p>(Optional, integer)
Maximum subword character length. Longer subword tokens are excluded from the
output. Defaults to <code>15</code>.</p>
</dd>
<dt class="hdlist1"><code>min_subword_size</code></dt>
<dd>
<p>(Optional, integer)
Minimum subword character length. Shorter subword tokens are excluded from the
output. Defaults to <code>2</code>.</p>
</dd>
<dt class="hdlist1"><code>min_word_size</code></dt>
<dd>
<p>(Optional, integer)
Minimum word character length. Shorter word tokens are excluded from the
output. Defaults to <code>5</code>.</p>
</dd>
<dt class="hdlist1"><code>only_longest_match</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, only include the longest matching subword. Defaults to <code>false</code>.</p>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-dict-decomp-tokenfilter-customize">Customize and add to an analyzer</h4>
<div class="paragraph">
<p>To customize the <code>dictionary_decompounder</code> filter, duplicate it to create the
basis for a new custom token filter. You can modify the filter using its
configurable parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following <a href="indices.html#indices-create-index">create index API</a> request
uses a custom <code>dictionary_decompounder</code> filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="paragraph">
<p>The custom <code>dictionary_decompounder</code> filter find subwords in the
<code>analysis/example_word_list.txt</code> file. Subwords longer than 22 characters are
excluded from the token output.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT dictionary_decompound_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_dictionary_decompound": {
          "tokenizer": "standard",
          "filter": [ "22_char_dictionary_decompound" ]
        }
      },
      "filter": {
        "22_char_dictionary_decompound": {
          "type": "dictionary_decompounder",
          "word_list_path": "analysis/example_word_list.txt",
          "max_subword_size": 22
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-edgengram-tokenfilter">Edge n-gram token filter</h3>
<titleabbrev>Edge n-gram</titleabbrev>
<div class="paragraph">
<p>Forms an <a href="https://en.wikipedia.org/wiki/N-gram">n-gram</a> of a specified length from
the beginning of a token.</p>
</div>
<div class="paragraph">
<p>For example, you can use the <code>edge_ngram</code> token filter to change <code>quick</code> to
<code>qu</code>.</p>
</div>
<div class="paragraph">
<p>When not customized, the filter creates 1-character edge n-grams by default.</p>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/ngram/EdgeNGramTokenFilter.html[EdgeNGramTokenFilter].</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>The <code>edge_ngram</code> filter is similar to the <a href="analysis-tokenizers.html#analysis-ngram-tokenizer"><code>ngram</code>
token filter</a>. However, the <code>edge_ngram</code> only outputs n-grams that start at the
beginning of a token. These edge n-grams are useful for
<a href="mapping-types.html#search-as-you-type">search-as-you-type</a> queries.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="analysis-edgengram-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>edge_ngram</code>
filter to convert <code>the quick brown fox jumps</code> to 1-character and 2-character
edge n-grams:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer": "standard",
  "filter": [
    { "type": "edge_ngram",
      "min_gram": 1,
      "max_gram": 2
    }
  ],
  "text": "the quick brown fox jumps"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ t, th, q, qu, b, br, f, fo, j, ju ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-edgengram-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>edge_ngram</code> filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT edge_ngram_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_edge_ngram": {
          "tokenizer": "standard",
          "filter": [ "edge_ngram" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-edgengram-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>max_gram</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, integer)
Maximum character length of a gram. For custom token filters, defaults to <code>2</code>.
For the built-in <code>edge_ngram</code> filter, defaults to <code>1</code>.</p>
</div>
<div class="paragraph">
<p>See <a href="analysis-tokenfilters.html#analysis-edgengram-tokenfilter-max-gram-limits">Limitations of the <code>max_gram</code> parameter</a>.</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>min_gram</code></dt>
<dd>
<p>(Optional, integer)
Minimum character length of a gram. Defaults to <code>1</code>.</p>
</dd>
<dt class="hdlist1"><code>preserve_original</code></dt>
<dd>
<p>(Optional, Boolean)
Emits original token when set to <code>true</code>. Defaults to <code>false</code>.</p>
</dd>
<dt class="hdlist1"><code>side</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, string)
Deprecated. Indicates whether to truncate tokens from the <code>front</code> or <code>back</code>.
Defaults to <code>front</code>.</p>
</div>
<div class="paragraph">
<p>Instead of using the <code>back</code> value, you can use the
<a href="analysis-tokenfilters.html#analysis-reverse-tokenfilter"><code>reverse</code></a> token filter before and after the
<code>edge_ngram</code> filter to achieve the same results.</p>
</div>
</div>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-edgengram-tokenfilter-customize">Customize</h4>
<div class="paragraph">
<p>To customize the <code>edge_ngram</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following request creates a custom <code>edge_ngram</code>
filter that forms n-grams between 3-5 characters.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT edge_ngram_custom_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "default": {
          "tokenizer": "whitespace",
          "filter": [ "3_5_edgegrams" ]
        }
      },
      "filter": {
        "3_5_edgegrams": {
          "type": "edge_ngram",
          "min_gram": 3,
          "max_gram": 5
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-edgengram-tokenfilter-max-gram-limits">Limitations of the <code>max_gram</code> parameter</h4>
<div class="paragraph">
<p>The <code>edge_ngram</code> filter&#8217;s <code>max_gram</code> value limits the character length of
tokens. When the <code>edge_ngram</code> filter is used with an index analyzer, this
means search terms longer than the <code>max_gram</code> length may not match any indexed
terms.</p>
</div>
<div class="paragraph">
<p>For example, if the <code>max_gram</code> is <code>3</code>, searches for <code>apple</code> won&#8217;t match the
indexed term <code>app</code>.</p>
</div>
<div class="paragraph">
<p>To account for this, you can use the
<a href="analysis-tokenfilters.html#analysis-truncate-tokenfilter"><code>truncate</code></a> filter with a search analyzer
to shorten search terms to the <code>max_gram</code> character length. However, this could
return irrelevant results.</p>
</div>
<div class="paragraph">
<p>For example, if the <code>max_gram</code> is <code>3</code> and search terms are truncated to three
characters, the search term <code>apple</code> is shortened to <code>app</code>. This means searches
for <code>apple</code> return any indexed terms matching <code>app</code>, such as <code>apply</code>, <code>snapped</code>,
and <code>apple</code>.</p>
</div>
<div class="paragraph">
<p>We recommend testing both approaches to see which best fits your
use case and desired search experience.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-elision-tokenfilter">Elision token filter</h3>
<titleabbrev>Elision</titleabbrev>
<div class="paragraph">
<p>Removes specified <a href="https://en.wikipedia.org/wiki/Elision">elisions</a> from
the beginning of tokens. For example, you can use this filter to change
<code>l&#8217;avion</code> to <code>avion</code>.</p>
</div>
<div class="paragraph">
<p>When not customized, the filter removes the following French elisions by default:</p>
</div>
<div class="paragraph">
<p><code>l'</code>, <code>m'</code>, <code>t'</code>, <code>qu'</code>, <code>n'</code>, <code>s'</code>, <code>j'</code>, <code>d'</code>, <code>c'</code>, <code>jusqu'</code>, <code>quoiqu'</code>,
<code>lorsqu'</code>, <code>puisqu'</code></p>
</div>
<div class="paragraph">
<p>Customized versions of this filter are included in several of OpenSearch&#8217;s built-in
<a href="analysis-analyzers.html#analysis-lang-analyzer">language analyzers</a>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="analysis-analyzers.html#catalan-analyzer">Catalan analyzer</a></p>
</li>
<li>
<p><a href="analysis-analyzers.html#french-analyzer">French analyzer</a></p>
</li>
<li>
<p><a href="analysis-analyzers.html#irish-analyzer">Irish analyzer</a></p>
</li>
<li>
<p><a href="analysis-analyzers.html#italian-analyzer">Italian analyzer</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/util/ElisionFilter.html[ElisionFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-elision-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>elision</code>
filter to remove <code>j'</code> from <code>j’examine près du wharf</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer" : "standard",
  "filter" : ["elision"],
  "text" : "j’examine près du wharf"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ examine, près, du, wharf ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-elision-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>elision</code> filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /elision_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_elision": {
          "tokenizer": "whitespace",
          "filter": [ "elision" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-elision-tokenfilter-configure-parms">Configurable parameters</h4>
<div id="analysis-elision-tokenfilter-articles" class="dlist">
<dl>
<dt class="hdlist1"><code>articles</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Required*, array of string)
List of elisions to remove.</p>
</div>
<div class="paragraph">
<p>To be removed, the elision must be at the beginning of a token and be
immediately followed by an apostrophe. Both the elision and apostrophe are
removed.</p>
</div>
<div class="paragraph">
<p>For custom <code>elision</code> filters, either this parameter or <code>articles_path</code> must be
specified.</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>articles_path</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Required*, string)
Path to a file that contains a list of elisions to remove.</p>
</div>
<div class="paragraph">
<p>This path must be absolute or relative to the <code>config</code> location, and the file
must be UTF-8 encoded. Each elision in the file must be separated by a line
break.</p>
</div>
<div class="paragraph">
<p>To be removed, the elision must be at the beginning of a token and be
immediately followed by an apostrophe. Both the elision and apostrophe are
removed.</p>
</div>
<div class="paragraph">
<p>For custom <code>elision</code> filters, either this parameter or <code>articles</code> must be
specified.</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>articles_case</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, the filter treats any provided elisions as case sensitive.
Defaults to <code>false</code>.</p>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-elision-tokenfilter-customize">Customize</h4>
<div class="paragraph">
<p>To customize the <code>elision</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following request creates a custom case-sensitive <code>elision</code>
filter that removes the <code>l'</code>, <code>m'</code>, <code>t'</code>, <code>qu'</code>, <code>n'</code>, <code>s'</code>,
and <code>j'</code> elisions:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /elision_case_sensitive_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "default": {
          "tokenizer": "whitespace",
          "filter": [ "elision_case_sensitive" ]
        }
      },
      "filter": {
        "elision_case_sensitive": {
          "type": "elision",
          "articles": [ "l", "m", "t", "qu", "n", "s", "j" ],
          "articles_case": true
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-fingerprint-tokenfilter">Fingerprint token filter</h3>
<titleabbrev>Fingerprint</titleabbrev>
<div class="paragraph">
<p>Sorts and removes duplicate tokens from a token stream, then concatenates the
stream into a single output token.</p>
</div>
<div class="paragraph">
<p>For example, this filter changes the <code>[ the, fox, was, very, very, quick ]</code>
token stream as follows:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Sorts the tokens alphabetically to <code>[ fox, quick, the, very, very, was ]</code></p>
</li>
<li>
<p>Removes a duplicate instance of the <code>very</code> token.</p>
</li>
<li>
<p>Concatenates the token stream to a output single token: <code>[fox quick the very was ]</code></p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Output tokens produced by this filter are useful for
fingerprinting and clustering a body of text as described in the
<a href="https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth#fingerprint">OpenRefine
project</a>.</p>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/miscellaneous/FingerprintFilter.html[FingerprintFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-fingerprint-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>fingerprint</code>
filter to create a single output token for the text <code>zebra jumps over resting
resting dog</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer" : "whitespace",
  "filter" : ["fingerprint"],
  "text" : "zebra jumps over resting resting dog"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following token:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ dog jumps over resting zebra ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-fingerprint-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>fingerprint</code> filter to configure a new <a href="configure-text-analysis.html#analysis-custom-analyzer">custom
analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT fingerprint_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_fingerprint": {
          "tokenizer": "whitespace",
          "filter": [ "fingerprint" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-fingerprint-tokenfilter-configure-parms">Configurable parameters</h4>
<div id="analysis-fingerprint-tokenfilter-max-size" class="dlist">
<dl>
<dt class="hdlist1"><code>max_output_size</code></dt>
<dd>
<p>(Optional, integer)
Maximum character length, including whitespace, of the output token. Defaults to
<code>255</code>. Concatenated tokens longer than this will result in no token output.</p>
</dd>
<dt class="hdlist1"><code>separator</code></dt>
<dd>
<p>(Optional, string)
Character to use to concatenate the token stream input. Defaults to a space.</p>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-fingerprint-tokenfilter-customize">Customize</h4>
<div class="paragraph">
<p>To customize the <code>fingerprint</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following request creates a custom <code>fingerprint</code> filter with
that use <code>+</code> to concatenate token streams. The filter also limits
output tokens to <code>100</code> characters or fewer.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT custom_fingerprint_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_": {
          "tokenizer": "whitespace",
          "filter": [ "fingerprint_plus_concat" ]
        }
      },
      "filter": {
        "fingerprint_plus_concat": {
          "type": "fingerprint",
          "max_output_size": 100,
          "separator": "+"
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-flatten-graph-tokenfilter">Flatten graph token filter</h3>
<titleabbrev>Flatten graph</titleabbrev>
<div class="paragraph">
<p>Flattens a <a href="analysis-concepts.html#token-graphs">token graph</a> produced by a graph token filter, such
as <a href="analysis-tokenfilters.html#analysis-synonym-graph-tokenfilter"><code>synonym_graph</code></a> or
<a href="analysis-tokenfilters.html#analysis-word-delimiter-graph-tokenfilter"><code>word_delimiter_graph</code></a>.</p>
</div>
<div class="paragraph">
<p>Flattening a token graph containing
<a href="analysis-concepts.html#token-graphs-multi-position-tokens">multi-position tokens</a> makes the graph
suitable for <a href="analysis-concepts.html#analysis-index-search-time">indexing</a>. Otherwise, indexing does
not support token graphs containing multi-position tokens.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph">
<p>Flattening graphs is a lossy process.</p>
</div>
<div class="paragraph">
<p>If possible, avoid using the <code>flatten_graph</code> filter. Instead, use graph token
filters in <a href="analysis-concepts.html#analysis-index-search-time">search analyzers</a> only. This eliminates
the need for the <code>flatten_graph</code> filter.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The <code>flatten_graph</code> filter uses Lucene&#8217;s
{lucene-analysis-docs}/core/FlattenGraphFilter.html[FlattenGraphFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-flatten-graph-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>To see how the <code>flatten_graph</code> filter works, you first need to produce a token
graph containing multi-position tokens.</p>
</div>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>synonym_graph</code>
filter to add <code>dns</code> as a multi-position synonym for <code>domain name system</code> in the
text <code>domain name system is fragile</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "synonym_graph",
      "synonyms": [ "dns, domain name system" ]
    }
  ],
  "text": "domain name system is fragile"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following token graph with <code>dns</code> as a multi-position
token.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="images/analysis/token-graph-dns-synonym-ex.svg" alt="token graph dns synonym ex">
</div>
</div>
<div class="paragraph">
<p>Indexing does not support token graphs containing multi-position tokens. To make
this token graph suitable for indexing, it needs to be flattened.</p>
</div>
<div class="paragraph">
<p>To flatten the token graph, add the <code>flatten_graph</code> filter after the
<code>synonym_graph</code> filter in the previous analyze API request.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "synonym_graph",
      "synonyms": [ "dns, domain name system" ]
    },
    "flatten_graph"
  ],
  "text": "domain name system is fragile"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following flattened token graph, which is suitable for
indexing.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="images/analysis/token-graph-dns-invalid-ex.svg" alt="token graph dns invalid ex">
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-keyword-marker-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>flatten_graph</code> token filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="paragraph">
<p>In this analyzer, a custom <code>word_delimiter_graph</code> filter produces token graphs
containing catenated, multi-position tokens. The <code>flatten_graph</code> filter flattens
these token graphs, making them suitable for indexing.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_index_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "filter": [
            "my_custom_word_delimiter_graph_filter",
            "flatten_graph"
          ]
        }
      },
      "filter": {
        "my_custom_word_delimiter_graph_filter": {
          "type": "word_delimiter_graph",
          "catenate_all": true
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-hunspell-tokenfilter">Hunspell token filter</h3>
<titleabbrev>Hunspell</titleabbrev>
<div class="paragraph">
<p>Provides <a href="analysis-concepts.html#dictionary-stemmers">dictionary stemming</a> based on a provided
<a href="https://en.wikipedia.org/wiki/Hunspell">Hunspell dictionary</a>. The <code>hunspell</code>
filter requires
<a href="analysis-tokenfilters.html#analysis-hunspell-tokenfilter-dictionary-config">configuration</a> of one or more
language-specific Hunspell dictionaries.</p>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/hunspell/HunspellStemFilter.html[HunspellStemFilter].</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>If available, we recommend trying an algorithmic stemmer for your language
before using the <a href="analysis-tokenfilters.html#analysis-hunspell-tokenfilter"><code>hunspell</code></a> token filter.
In practice, algorithmic stemmers typically outperform dictionary stemmers.
See <a href="analysis-concepts.html#dictionary-stemmers">Dictionary stemmers</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="analysis-hunspell-tokenfilter-dictionary-config">Configure Hunspell dictionaries</h4>
<div class="paragraph">
<p>By default, Hunspell dictionaries are stored and detected on a dedicated
hunspell directory on the filesystem: <code>&lt;path.config&gt;/hunspell</code>. Each dictionary
is expected to have its own directory, named after its associated language and
locale (e.g., <code>pt_BR</code>, <code>en_GB</code>). This dictionary directory is expected to hold a
single <code>.aff</code> and one or more <code>.dic</code> files, all of which will automatically be
picked up. For example, assuming the default <code>&lt;path.config&gt;/hunspell</code> path
is used, the following directory layout will define the <code>en_US</code> dictionary:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-txt" data-lang="txt">- config
    |-- hunspell
    |    |-- en_US
    |    |    |-- en_US.dic
    |    |    |-- en_US.aff</code></pre>
</div>
</div>
<div class="paragraph">
<p>Each dictionary can be configured with one setting:</p>
</div>
<div id="analysis-hunspell-ignore-case-settings" class="dlist">
<dl>
<dt class="hdlist1"><code>ignore_case</code></dt>
<dd>
<p>(Static, Boolean)
If true, dictionary matching will be case insensitive. Defaults to <code>false</code>.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>This setting can be configured globally in <code>opensearch.yml</code> using
<code>indices.analysis.hunspell.dictionary.ignore_case</code>.</p>
</div>
<div class="paragraph">
<p>To configure the setting for a specific locale, use the
<code>indices.analysis.hunspell.dictionary.&lt;locale&gt;.ignore_case</code> setting (e.g., for
the <code>en_US</code> (American English) locale, the setting is
<code>indices.analysis.hunspell.dictionary.en_US.ignore_case</code>).</p>
</div>
<div class="paragraph">
<p>It is also possible to add <code>settings.yml</code> file under the dictionary
directory which holds these settings. This overrides any other <code>ignore_case</code>
settings defined in <code>opensearch.yml</code>.</p>
</div>
</div>
<div class="sect3">
<h4 id="analysis-hunspell-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following analyze API request uses the <code>hunspell</code> filter to stem
<code>the foxes jumping quickly</code> to <code>the fox jump quick</code>.</p>
</div>
<div class="paragraph">
<p>The request specifies the <code>en_US</code> locale, meaning that the
<code>.aff</code> and <code>.dic</code> files in the <code>&lt;path.config&gt;/hunspell/en_US</code> directory are used
for the Hunspell dictionary.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "hunspell",
      "locale": "en_US"
    }
  ],
  "text": "the foxes jumping quickly"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ the, fox, jump, quick ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-hunspell-tokenfilter-configure-parms">Configurable parameters</h4>
<div id="analysis-hunspell-tokenfilter-dictionary-param" class="dlist">
<dl>
<dt class="hdlist1"><code>dictionary</code></dt>
<dd>
<p>(Optional, string or array of strings)
One or more <code>.dic</code> files (e.g, <code>en_US.dic, my_custom.dic</code>) to use for the
Hunspell dictionary.</p>
<div class="paragraph">
<p>By default, the <code>hunspell</code> filter uses all <code>.dic</code> files in the
<code>&lt;path.config&gt;/hunspell/&lt;locale&gt;</code> directory specified specified using the
<code>lang</code>, <code>language</code>, or <code>locale</code> parameter. To use another directory, the
directory&#8217;s path must be registered using the
<a href="analysis-tokenfilters.html#indices-analysis-hunspell-dictionary-location"><code>indices.analysis.hunspell.dictionary.location</code></a> setting.</p>
</div>
</dd>
<dt class="hdlist1"><code>dedup</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, duplicate tokens are removed from the filter&#8217;s output. Defaults to
<code>true</code>.</p>
</dd>
<dt class="hdlist1"><code>lang</code></dt>
<dd>
<p>(Required*, string)
An alias for the <a href="analysis-tokenfilters.html#analysis-hunspell-tokenfilter-locale-param"><code>locale</code>
parameter</a>.</p>
<div class="paragraph">
<p>If this parameter is not specified, the <code>language</code> or <code>locale</code> parameter is
required.</p>
</div>
</dd>
<dt class="hdlist1"><code>language</code></dt>
<dd>
<p>(Required*, string)
An alias for the <a href="analysis-tokenfilters.html#analysis-hunspell-tokenfilter-locale-param"><code>locale</code>
parameter</a>.</p>
<div class="paragraph">
<p>If this parameter is not specified, the <code>lang</code> or <code>locale</code> parameter is
required.</p>
</div>
</dd>
</dl>
</div>
<div id="analysis-hunspell-tokenfilter-locale-param" class="dlist">
<dl>
<dt class="hdlist1"><code>locale</code></dt>
<dd>
<p>(Required*, string)
Locale directory used to specify the <code>.aff</code> and <code>.dic</code> files for a Hunspell
dictionary. See <a href="analysis-tokenfilters.html#analysis-hunspell-tokenfilter-dictionary-config">Configure Hunspell dictionaries</a>.</p>
<div class="paragraph">
<p>If this parameter is not specified, the <code>lang</code> or <code>language</code> parameter is
required.</p>
</div>
</dd>
<dt class="hdlist1"><code>longest_only</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, only the longest stemmed version of each token is
included in the output. If <code>false</code>, all stemmed versions of the token are
included. Defaults to <code>false</code>.</p>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-hunspell-tokenfilter-analyzer-ex">Customize and add to an analyzer</h4>
<div class="paragraph">
<p>To customize the <code>hunspell</code> filter, duplicate it to create the
basis for a new custom token filter. You can modify the filter using its
configurable parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following <a href="indices.html#indices-create-index">create index API</a> request
uses a custom <code>hunspell</code> filter, <code>my_en_US_dict_stemmer</code>, to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="paragraph">
<p>The <code>my_en_US_dict_stemmer</code> filter uses a <code>locale</code> of <code>en_US</code>, meaning that the
<code>.aff</code> and <code>.dic</code> files in the <code>&lt;path.config&gt;/hunspell/en_US</code> directory are
used. The filter also includes a <code>dedup</code> argument of <code>false</code>, meaning that
duplicate tokens added from the dictionary are not removed from the filter&#8217;s
output.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "en": {
          "tokenizer": "standard",
          "filter": [ "my_en_US_dict_stemmer" ]
        }
      },
      "filter": {
        "my_en_US_dict_stemmer": {
          "type": "hunspell",
          "locale": "en_US",
          "dedup": false
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-hunspell-tokenfilter-settings">Settings</h4>
<div class="paragraph">
<p>In addition to the <a href="analysis-tokenfilters.html#analysis-hunspell-ignore-case-settings"><code>ignore_case</code>
settings</a>, you can configure the following global settings for the <code>hunspell</code>
filter using <code>opensearch.yml</code>:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>indices.analysis.hunspell.dictionary.lazy</code></dt>
<dd>
<p>(Static, Boolean)
If <code>true</code>, the loading of Hunspell dictionaries is deferred until a dictionary
is used. If <code>false</code>, the dictionary directory is checked for dictionaries when
the node starts, and any dictionaries are automatically loaded. Defaults to
<code>false</code>.</p>
</dd>
</dl>
</div>
<div id="indices-analysis-hunspell-dictionary-location" class="dlist">
<dl>
<dt class="hdlist1"><code>indices.analysis.hunspell.dictionary.location</code></dt>
<dd>
<p>(Static, string)
Path to a Hunspell dictionary directory. This path must be absolute or
relative to the <code>config</code> location.</p>
<div class="paragraph">
<p>By default, the <code>&lt;path.config&gt;/hunspell</code> directory is used, as described in
<a href="analysis-tokenfilters.html#analysis-hunspell-tokenfilter-dictionary-config">Configure Hunspell dictionaries</a>.</p>
</div>
</dd>
</dl>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-hyp-decomp-tokenfilter">Hyphenation decompounder token filter</h3>
<titleabbrev>Hyphenation decompounder</titleabbrev>
<div class="paragraph">
<p>Uses XML-based hyphenation patterns to find potential subwords in compound
words. These subwords are then checked against the specified word list. Subwords not
in the list are excluded from the token output.</p>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/compound/HyphenationCompoundWordTokenFilter.html[HyphenationCompoundWordTokenFilter],
which was built for Germanic languages.</p>
</div>
<div class="sect3">
<h4 id="analysis-hyp-decomp-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the
<code>hyphenation_decompounder</code> filter to find subwords in <code>Kaffeetasse</code> based on
German hyphenation patterns in the <code>analysis/hyphenation_patterns.xml</code> file. The
filter then checks these subwords against a list of specified words: <code>kaffee</code>,
<code>zucker</code>, and <code>tasse</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "hyphenation_decompounder",
      "hyphenation_patterns_path": "analysis/hyphenation_patterns.xml",
      "word_list": ["Kaffee", "zucker", "tasse"]
    }
  ],
  "text": "Kaffeetasse"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ Kaffeetasse, Kaffee, tasse ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-hyp-decomp-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>hyphenation_patterns_path</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Required, string)
Path to an Apache FOP (Formatting Objects Processor) XML hyphenation pattern file.</p>
</div>
<div class="paragraph">
<p>This path must be absolute or relative to the <code>config</code> location. Only FOP v1.2
compatible files are supported.</p>
</div>
<div class="paragraph">
<p>For example FOP XML hyphenation pattern files, refer to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="http://offo.sourceforge.net/#FOP+XML+Hyphenation+Patterns">Objects For Formatting Objects (OFFO) Sourceforge project</a></p>
</li>
<li>
<p><a href="https://sourceforge.net/projects/offo/files/offo-hyphenation/1.2/offo-hyphenation_v1.2.zip/download">offo-hyphenation_v1.2.zip direct download</a> (v2.0 and above hyphenation pattern files are not supported)</p>
</li>
</ul>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>word_list</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Required*, array of strings)
A list of subwords. Subwords found using the hyphenation pattern but not in this
list are excluded from the token output.</p>
</div>
<div class="paragraph">
<p>You can use the <a href="analysis-tokenfilters.html#analysis-dict-decomp-tokenfilter"><code>dictionary_decompounder</code></a>
filter to test the quality of word lists before implementing them.</p>
</div>
<div class="paragraph">
<p>Either this parameter or <code>word_list_path</code> must be specified.</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>word_list_path</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Required*, string)
Path to a file containing a list of subwords. Subwords found using the
hyphenation pattern but not in this list are excluded from the token output.</p>
</div>
<div class="paragraph">
<p>This path must be absolute or relative to the <code>config</code> location, and the file
must be UTF-8 encoded. Each token in the file must be separated by a line break.</p>
</div>
<div class="paragraph">
<p>You can use the <a href="analysis-tokenfilters.html#analysis-dict-decomp-tokenfilter"><code>dictionary_decompounder</code></a>
filter to test the quality of word lists before implementing them.</p>
</div>
<div class="paragraph">
<p>Either this parameter or <code>word_list</code> must be specified.</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>max_subword_size</code></dt>
<dd>
<p>(Optional, integer)
Maximum subword character length. Longer subword tokens are excluded from the
output. Defaults to <code>15</code>.</p>
</dd>
<dt class="hdlist1"><code>min_subword_size</code></dt>
<dd>
<p>(Optional, integer)
Minimum subword character length. Shorter subword tokens are excluded from the
output. Defaults to <code>2</code>.</p>
</dd>
<dt class="hdlist1"><code>min_word_size</code></dt>
<dd>
<p>(Optional, integer)
Minimum word character length. Shorter word tokens are excluded from the
output. Defaults to <code>5</code>.</p>
</dd>
<dt class="hdlist1"><code>only_longest_match</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, only include the longest matching subword. Defaults to <code>false</code>.</p>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-hyp-decomp-tokenfilter-customize">Customize and add to an analyzer</h4>
<div class="paragraph">
<p>To customize the <code>hyphenation_decompounder</code> filter, duplicate it to create the
basis for a new custom token filter. You can modify the filter using its
configurable parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following <a href="indices.html#indices-create-index">create index API</a> request
uses a custom <code>hyphenation_decompounder</code> filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="paragraph">
<p>The custom <code>hyphenation_decompounder</code> filter find subwords based on hyphenation
patterns in the <code>analysis/hyphenation_patterns.xml</code> file. The filter then checks
these subwords against the list of words specified in the
<code>analysis/example_word_list.txt</code> file. Subwords longer than 22 characters are
excluded from the token output.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT hyphenation_decompound_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_hyphenation_decompound": {
          "tokenizer": "standard",
          "filter": [ "22_char_hyphenation_decompound" ]
        }
      },
      "filter": {
        "22_char_hyphenation_decompound": {
          "type": "hyphenation_decompounder",
          "word_list_path": "analysis/example_word_list.txt",
          "hyphenation_patterns_path": "analysis/hyphenation_patterns.xml",
          "max_subword_size": 22
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-keep-types-tokenfilter">Keep types token filter</h3>
<titleabbrev>Keep types</titleabbrev>
<div class="paragraph">
<p>Keeps or removes tokens of a specific type. For example, you can use this filter
to change <code>3 quick foxes</code> to <code>quick foxes</code> by keeping only <code>&lt;ALPHANUM&gt;</code>
(alphanumeric) tokens.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="title">Token types</div>
<div class="paragraph">
<p>Token types are set by the <a href="analysis-tokenizers.html">tokenizer</a> when converting
characters to tokens. Token types can vary between tokenizers.</p>
</div>
<div class="paragraph">
<p>For example, the <a href="analysis-tokenizers.html#analysis-standard-tokenizer"><code>standard</code></a> tokenizer can
produce a variety of token types, including <code>&lt;ALPHANUM&gt;</code>, <code>&lt;HANGUL&gt;</code>, and
<code>&lt;NUM&gt;</code>. Simpler analyzers, like the
<a href="analysis-tokenizers.html#analysis-lowercase-tokenizer"><code>lowercase</code></a> tokenizer, only produce the <code>word</code>
token type.</p>
</div>
<div class="paragraph">
<p>Certain token filters can also add token types. For example, the
<a href="analysis-tokenfilters.html#analysis-synonym-tokenfilter"><code>synonym</code></a> filter can add the <code>&lt;SYNONYM&gt;</code> token
type.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/core/TypeTokenFilter.html[TypeTokenFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-keep-types-tokenfilter-analyze-include-ex">Include example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>keep_types</code>
filter to keep only <code>&lt;NUM&gt;</code> (numeric) tokens from <code>1 quick fox 2 lazy dogs</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "keep_types",
      "types": [ "&lt;NUM&gt;" ]
    }
  ],
  "text": "1 quick fox 2 lazy dogs"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ 1, 2 ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-keep-types-tokenfilter-analyze-exclude-ex">Exclude example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>keep_types</code>
filter to remove <code>&lt;NUM&gt;</code> tokens from <code>1 quick fox 2 lazy dogs</code>. Note the <code>mode</code>
parameter is set to <code>exclude</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "keep_types",
      "types": [ "&lt;NUM&gt;" ],
      "mode": "exclude"
    }
  ],
  "text": "1 quick fox 2 lazy dogs"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ quick, fox, lazy, dogs ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-keep-types-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>types</code></dt>
<dd>
<p>(Required, array of strings)
List of token types to keep or remove.</p>
</dd>
<dt class="hdlist1"><code>mode</code></dt>
<dd>
<p>(Optional, string)
Indicates whether to keep or remove the specified token types.
Valid values are:</p>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>include</code></dt>
<dd>
<p>(Default) Keep only the specified token types.</p>
</dd>
<dt class="hdlist1"><code>exclude</code></dt>
<dd>
<p>Remove the specified token types.</p>
</dd>
</dl>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-keep-types-tokenfilter-customize">Customize and add to an analyzer</h4>
<div class="paragraph">
<p>To customize the <code>keep_types</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following <a href="indices.html#indices-create-index">create index API</a> request
uses a custom <code>keep_types</code> filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>. The custom <code>keep_types</code> filter
keeps only <code>&lt;ALPHANUM&gt;</code> (alphanumeric) tokens.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT keep_types_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [ "extract_alpha" ]
        }
      },
      "filter": {
        "extract_alpha": {
          "type": "keep_types",
          "types": [ "&lt;ALPHANUM&gt;" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-keep-words-tokenfilter">Keep words token filter</h3>
<titleabbrev>Keep words</titleabbrev>
<div class="paragraph">
<p>Keeps only tokens contained in a specified word list.</p>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/miscellaneous/KeepWordFilter.html[KeepWordFilter].</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>To remove a list of words from a token stream, use the
<a href="analysis-tokenfilters.html#analysis-stop-tokenfilter"><code>stop</code></a> filter.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="analysis-keep-words-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>keep</code> filter to
keep only the <code>fox</code> and <code>dog</code> tokens from
<code>the quick fox jumps over the lazy dog</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "keep",
      "keep_words": [ "dog", "elephant", "fox" ]
    }
  ],
  "text": "the quick fox jumps over the lazy dog"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ fox, dog ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-keep-words-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>keep_words</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Required*, array of strings)
List of words to keep. Only tokens that match words in this list are included in
the output.</p>
</div>
<div class="paragraph">
<p>Either this parameter or <code>keep_words_path</code> must be specified.</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>keep_words_path</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Required*, array of strings)
Path to a file that contains a list of words to keep. Only tokens that match
words in this list are included in the output.</p>
</div>
<div class="paragraph">
<p>This path must be absolute or relative to the <code>config</code> location, and the file
must be UTF-8 encoded. Each word in the file must be separated by a line break.</p>
</div>
<div class="paragraph">
<p>Either this parameter or <code>keep_words</code> must be specified.</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>keep_words_case</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, lowercase all keep words. Defaults to <code>false</code>.</p>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-keep-words-tokenfilter-customize">Customize and add to an analyzer</h4>
<div class="paragraph">
<p>To customize the <code>keep</code> filter, duplicate it to create the basis for a new
custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following <a href="indices.html#indices-create-index">create index API</a> request
uses custom <code>keep</code> filters to configure two new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzers</a>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>standard_keep_word_array</code>, which uses a custom <code>keep</code> filter with an inline
array of keep words</p>
</li>
<li>
<p><code>standard_keep_word_file</code>, which uses a customer <code>keep</code> filter with a keep
words file</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT keep_words_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_keep_word_array": {
          "tokenizer": "standard",
          "filter": [ "keep_word_array" ]
        },
        "standard_keep_word_file": {
          "tokenizer": "standard",
          "filter": [ "keep_word_file" ]
        }
      },
      "filter": {
        "keep_word_array": {
          "type": "keep",
          "keep_words": [ "one", "two", "three" ]
        },
        "keep_word_file": {
          "type": "keep",
          "keep_words_path": "analysis/example_word_list.txt"
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-keyword-marker-tokenfilter">Keyword marker token filter</h3>
<titleabbrev>Keyword marker</titleabbrev>
<div class="paragraph">
<p>Marks specified tokens as keywords, which are not stemmed.</p>
</div>
<div class="paragraph">
<p>The <code>keyword_marker</code> filter assigns specified tokens a <code>keyword</code> attribute of
<code>true</code>. Stemmer token filters, such as
<a href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter"><code>stemmer</code></a> or
<a href="analysis-tokenfilters.html#analysis-porterstem-tokenfilter"><code>porter_stem</code></a>, skip tokens with a <code>keyword</code>
attribute of <code>true</code>.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
<div class="paragraph">
<p>To work properly, the <code>keyword_marker</code> filter must be listed before any stemmer
token filters in the <a href="configure-text-analysis.html#analysis-custom-analyzer">analyzer configuration</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The <code>keyword_marker</code> filter uses Lucene&#8217;s
{lucene-analysis-docs}/miscellaneous/KeywordMarkerFilter.html[KeywordMarkerFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-keyword-marker-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>To see how the <code>keyword_marker</code> filter works, you first need to produce a token
stream containing stemmed tokens.</p>
</div>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the
<a href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter"><code>stemmer</code></a> filter to create stemmed tokens for
<code>fox running and jumping</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [ "stemmer" ],
  "text": "fox running and jumping"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The request produces the following tokens. Note that <code>running</code> was stemmed to
<code>run</code> and <code>jumping</code> was stemmed to <code>jump</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ fox, run, and, jump ]</code></pre>
</div>
</div>
<div class="paragraph">
<p>To prevent <code>jumping</code> from being stemmed, add the <code>keyword_marker</code> filter before
the <code>stemmer</code> filter in the previous analyze API request. Specify <code>jumping</code> in
the <code>keywords</code> parameter of the <code>keyword_marker</code> filter.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "keyword_marker",
      "keywords": [ "jumping" ]
    },
    "stemmer"
  ],
  "text": "fox running and jumping"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The request produces the following tokens. <code>running</code> is still stemmed to <code>run</code>,
but <code>jumping</code> is not stemmed.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ fox, run, and, jumping ]</code></pre>
</div>
</div>
<div class="paragraph">
<p>To see the <code>keyword</code> attribute for these tokens, add the following arguments to
the analyze API request:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>explain</code>: <code>true</code></p>
</li>
<li>
<p><code>attributes</code>: <code>keyword</code></p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "keyword_marker",
      "keywords": [ "jumping" ]
    },
    "stemmer"
  ],
  "text": "fox running and jumping",
  "explain": true,
  "attributes": "keyword"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The API returns the following response. Note the <code>jumping</code> token has a
<code>keyword</code> attribute of <code>true</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console-result" data-lang="console-result">{
  "detail": {
    "custom_analyzer": true,
    "charfilters": [],
    "tokenizer": {
      "name": "whitespace",
      "tokens": [
        {
          "token": "fox",
          "start_offset": 0,
          "end_offset": 3,
          "type": "word",
          "position": 0
        },
        {
          "token": "running",
          "start_offset": 4,
          "end_offset": 11,
          "type": "word",
          "position": 1
        },
        {
          "token": "and",
          "start_offset": 12,
          "end_offset": 15,
          "type": "word",
          "position": 2
        },
        {
          "token": "jumping",
          "start_offset": 16,
          "end_offset": 23,
          "type": "word",
          "position": 3
        }
      ]
    },
    "tokenfilters": [
      {
        "name": "__anonymous__keyword_marker",
        "tokens": [
          {
            "token": "fox",
            "start_offset": 0,
            "end_offset": 3,
            "type": "word",
            "position": 0,
            "keyword": false
          },
          {
            "token": "running",
            "start_offset": 4,
            "end_offset": 11,
            "type": "word",
            "position": 1,
            "keyword": false
          },
          {
            "token": "and",
            "start_offset": 12,
            "end_offset": 15,
            "type": "word",
            "position": 2,
            "keyword": false
          },
          {
            "token": "jumping",
            "start_offset": 16,
            "end_offset": 23,
            "type": "word",
            "position": 3,
            "keyword": true
          }
        ]
      },
      {
        "name": "stemmer",
        "tokens": [
          {
            "token": "fox",
            "start_offset": 0,
            "end_offset": 3,
            "type": "word",
            "position": 0,
            "keyword": false
          },
          {
            "token": "run",
            "start_offset": 4,
            "end_offset": 11,
            "type": "word",
            "position": 1,
            "keyword": false
          },
          {
            "token": "and",
            "start_offset": 12,
            "end_offset": 15,
            "type": "word",
            "position": 2,
            "keyword": false
          },
          {
            "token": "jumping",
            "start_offset": 16,
            "end_offset": 23,
            "type": "word",
            "position": 3,
            "keyword": true
          }
        ]
      }
    ]
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-keyword-marker-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>ignore_case</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, matching for the <code>keywords</code> and <code>keywords_path</code> parameters ignores
letter case. Defaults to <code>false</code>.</p>
</dd>
<dt class="hdlist1"><code>keywords</code></dt>
<dd>
<p>(Required*, array of strings)
Array of keywords. Tokens that match these keywords are not stemmed.</p>
<div class="paragraph">
<p>This parameter, <code>keywords_path</code>, or <code>keywords_pattern</code> must be specified.
You cannot specify this parameter and <code>keywords_pattern</code>.</p>
</div>
</dd>
<dt class="hdlist1"><code>keywords_path</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Required*, string)
Path to a file that contains a list of keywords. Tokens that match these
keywords are not stemmed.</p>
</div>
<div class="paragraph">
<p>This path must be absolute or relative to the <code>config</code> location, and the file
must be UTF-8 encoded. Each word in the file must be separated by a line break.</p>
</div>
<div class="paragraph">
<p>This parameter, <code>keywords</code>, or <code>keywords_pattern</code> must be specified.
You cannot specify this parameter and <code>keywords_pattern</code>.</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>keywords_pattern</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Required*, string)
<a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html">Java
regular expression</a> used to match tokens. Tokens that match this expression are
marked as keywords and not stemmed.</p>
</div>
<div class="paragraph">
<p>This parameter, <code>keywords</code>, or <code>keywords_path</code> must be specified. You
cannot specify this parameter and <code>keywords</code> or <code>keywords_pattern</code>.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph">
<p>Poorly written regular expressions can cause OpenSearch to run slowly or result
in stack overflow errors, causing the running node to suddenly exit.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-keyword-marker-tokenfilter-customize">Customize and add to an analyzer</h4>
<div class="paragraph">
<p>To customize the <code>keyword_marker</code> filter, duplicate it to create the basis for a
new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following <a href="indices.html#indices-create-index">create index API</a> request
uses a custom <code>keyword_marker</code> filter and the <code>porter_stem</code>
filter to configure a new <a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="paragraph">
<p>The custom <code>keyword_marker</code> filter marks tokens specified in the
<code>analysis/example_word_list.txt</code> file as keywords. The <code>porter_stem</code> filter does
not stem these tokens.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "filter": [
            "my_custom_keyword_marker_filter",
            "porter_stem"
          ]
        }
      },
      "filter": {
        "my_custom_keyword_marker_filter": {
          "type": "keyword_marker",
          "keywords_path": "analysis/example_word_list.txt"
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-keyword-repeat-tokenfilter">Keyword repeat token filter</h3>
<titleabbrev>Keyword repeat</titleabbrev>
<div class="paragraph">
<p>Outputs a keyword version of each token in a stream. These keyword tokens are
not stemmed.</p>
</div>
<div class="paragraph">
<p>The <code>keyword_repeat</code> filter assigns keyword tokens a <code>keyword</code> attribute of
<code>true</code>. Stemmer token filters, such as
<a href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter"><code>stemmer</code></a> or
<a href="analysis-tokenfilters.html#analysis-porterstem-tokenfilter"><code>porter_stem</code></a>, skip tokens with a <code>keyword</code>
attribute of <code>true</code>.</p>
</div>
<div class="paragraph">
<p>You can use the <code>keyword_repeat</code> filter with a stemmer token filter to output a
stemmed and unstemmed version of each token in a stream.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
<div class="paragraph">
<p>To work properly, the <code>keyword_repeat</code> filter must be listed before any stemmer
token filters in the <a href="configure-text-analysis.html#analysis-custom-analyzer">analyzer configuration</a>.</p>
</div>
<div class="paragraph">
<p>Stemming does not affect all tokens. This means streams could contain duplicate
tokens in the same position, even after stemming.</p>
</div>
<div class="paragraph">
<p>To remove these duplicate tokens, add the
<a href="analysis-tokenfilters.html#analysis-remove-duplicates-tokenfilter"><code>remove_duplicates</code></a> filter after the
stemmer filter in the analyzer configuration.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The <code>keyword_repeat</code> filter uses Lucene&#8217;s
{lucene-analysis-docs}/miscellaneous/KeywordRepeatFilter.html[KeywordRepeatFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-keyword-repeat-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>keyword_repeat</code>
filter to output a keyword and non-keyword version of each token in
<code>fox running and jumping</code>.</p>
</div>
<div class="paragraph">
<p>To return the <code>keyword</code> attribute for these tokens, the analyze API request also
includes the following arguments:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>explain</code>:  <code>true</code></p>
</li>
<li>
<p><code>attributes</code>: <code>keyword</code></p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    "keyword_repeat"
  ],
  "text": "fox running and jumping",
  "explain": true,
  "attributes": "keyword"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The API returns the following response. Note that one version of each token has
a <code>keyword</code> attribute of <code>true</code>.</p>
</div>
<details>
<summary class="title"><strong>Response</strong></summary>
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console-result" data-lang="console-result">{
  "detail": {
    "custom_analyzer": true,
    "charfilters": [],
    "tokenizer": ...,
    "tokenfilters": [
      {
        "name": "keyword_repeat",
        "tokens": [
          {
            "token": "fox",
            "start_offset": 0,
            "end_offset": 3,
            "type": "word",
            "position": 0,
            "keyword": true
          },
          {
            "token": "fox",
            "start_offset": 0,
            "end_offset": 3,
            "type": "word",
            "position": 0,
            "keyword": false
          },
          {
            "token": "running",
            "start_offset": 4,
            "end_offset": 11,
            "type": "word",
            "position": 1,
            "keyword": true
          },
          {
            "token": "running",
            "start_offset": 4,
            "end_offset": 11,
            "type": "word",
            "position": 1,
            "keyword": false
          },
          {
            "token": "and",
            "start_offset": 12,
            "end_offset": 15,
            "type": "word",
            "position": 2,
            "keyword": true
          },
          {
            "token": "and",
            "start_offset": 12,
            "end_offset": 15,
            "type": "word",
            "position": 2,
            "keyword": false
          },
          {
            "token": "jumping",
            "start_offset": 16,
            "end_offset": 23,
            "type": "word",
            "position": 3,
            "keyword": true
          },
          {
            "token": "jumping",
            "start_offset": 16,
            "end_offset": 23,
            "type": "word",
            "position": 3,
            "keyword": false
          }
        ]
      }
    ]
  }
}</code></pre>
</div>
</div>
</div>
</details>
<div class="paragraph">
<p>To stem the non-keyword tokens, add the <code>stemmer</code> filter after the
<code>keyword_repeat</code> filter in the previous analyze API request.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    "keyword_repeat",
    "stemmer"
  ],
  "text": "fox running and jumping",
  "explain": true,
  "attributes": "keyword"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The API returns the following response. Note the following changes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The non-keyword version of <code>running</code> was stemmed to <code>run</code>.</p>
</li>
<li>
<p>The non-keyword version of <code>jumping</code> was stemmed to <code>jump</code>.</p>
</li>
</ul>
</div>
<details>
<summary class="title"><strong>Response</strong></summary>
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console-result" data-lang="console-result">{
  "detail": {
    "custom_analyzer": true,
    "charfilters": [],
    "tokenizer": ...,
    "tokenfilters": [
      {
        "name": "keyword_repeat",
        "tokens": ...
      },
      {
        "name": "stemmer",
        "tokens": [
          {
            "token": "fox",
            "start_offset": 0,
            "end_offset": 3,
            "type": "word",
            "position": 0,
            "keyword": true
          },
          {
            "token": "fox",
            "start_offset": 0,
            "end_offset": 3,
            "type": "word",
            "position": 0,
            "keyword": false
          },
          {
            "token": "running",
            "start_offset": 4,
            "end_offset": 11,
            "type": "word",
            "position": 1,
            "keyword": true
          },
          {
            "token": "run",
            "start_offset": 4,
            "end_offset": 11,
            "type": "word",
            "position": 1,
            "keyword": false
          },
          {
            "token": "and",
            "start_offset": 12,
            "end_offset": 15,
            "type": "word",
            "position": 2,
            "keyword": true
          },
          {
            "token": "and",
            "start_offset": 12,
            "end_offset": 15,
            "type": "word",
            "position": 2,
            "keyword": false
          },
          {
            "token": "jumping",
            "start_offset": 16,
            "end_offset": 23,
            "type": "word",
            "position": 3,
            "keyword": true
          },
          {
            "token": "jump",
            "start_offset": 16,
            "end_offset": 23,
            "type": "word",
            "position": 3,
            "keyword": false
          }
        ]
      }
    ]
  }
}</code></pre>
</div>
</div>
</div>
</details>
<div class="paragraph">
<p>However, the keyword and non-keyword versions of <code>fox</code> and <code>and</code> are
identical and in the same respective positions.</p>
</div>
<div class="paragraph">
<p>To remove these duplicate tokens, add the <code>remove_duplicates</code> filter after
<code>stemmer</code> in the analyze API request.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    "keyword_repeat",
    "stemmer",
    "remove_duplicates"
  ],
  "text": "fox running and jumping",
  "explain": true,
  "attributes": "keyword"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The API returns the following response. Note that the duplicate tokens for <code>fox</code>
and <code>and</code> have been removed.</p>
</div>
<details>
<summary class="title"><strong>Response</strong></summary>
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console-result" data-lang="console-result">{
  "detail": {
    "custom_analyzer": true,
    "charfilters": [],
    "tokenizer": ...,
    "tokenfilters": [
      {
        "name": "keyword_repeat",
        "tokens": ...
      },
      {
        "name": "stemmer",
        "tokens": ...
      },
      {
        "name": "remove_duplicates",
        "tokens": [
          {
            "token": "fox",
            "start_offset": 0,
            "end_offset": 3,
            "type": "word",
            "position": 0,
            "keyword": true
          },
          {
            "token": "running",
            "start_offset": 4,
            "end_offset": 11,
            "type": "word",
            "position": 1,
            "keyword": true
          },
          {
            "token": "run",
            "start_offset": 4,
            "end_offset": 11,
            "type": "word",
            "position": 1,
            "keyword": false
          },
          {
            "token": "and",
            "start_offset": 12,
            "end_offset": 15,
            "type": "word",
            "position": 2,
            "keyword": true
          },
          {
            "token": "jumping",
            "start_offset": 16,
            "end_offset": 23,
            "type": "word",
            "position": 3,
            "keyword": true
          },
          {
            "token": "jump",
            "start_offset": 16,
            "end_offset": 23,
            "type": "word",
            "position": 3,
            "keyword": false
          }
        ]
      }
    ]
  }
}</code></pre>
</div>
</div>
</div>
</details>
</div>
<div class="sect3">
<h4 id="analysis-keyword-repeat-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>keyword_repeat</code> filter to configure a new <a href="configure-text-analysis.html#analysis-custom-analyzer">custom
analyzer</a>.</p>
</div>
<div class="paragraph">
<p>This custom analyzer uses the <code>keyword_repeat</code> and <code>porter_stem</code> filters to
create a stemmed and unstemmed version of each token in a stream. The
<code>remove_duplicates</code> filter then removes any duplicate tokens from the stream.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "tokenizer": "standard",
          "filter": [
            "keyword_repeat",
            "porter_stem",
            "remove_duplicates"
          ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-kstem-tokenfilter">KStem token filter</h3>
<titleabbrev>KStem</titleabbrev>
<div class="paragraph">
<p>Provides <a href="https://ciir.cs.umass.edu/pubfiles/ir-35.pdf">KStem</a>-based stemming for
the English language. The <code>kstem</code> filter combines
<a href="analysis-concepts.html#algorithmic-stemmers">algorithmic stemming</a> with a built-in
<a href="analysis-concepts.html#dictionary-stemmers">dictionary</a>.</p>
</div>
<div class="paragraph">
<p>The <code>kstem</code> filter tends to stem less aggressively than other English stemmer
filters, such as the <a href="analysis-tokenfilters.html#analysis-porterstem-tokenfilter"><code>porter_stem</code></a> filter.</p>
</div>
<div class="paragraph">
<p>The <code>kstem</code> filter is equivalent to the
<a href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter"><code>stemmer</code></a> filter&#8217;s
<a href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter-language-parm"><code>light_english</code></a> variant.</p>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/en/KStemFilter.html[KStemFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-kstem-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following analyze API request uses the <code>kstem</code> filter to stem <code>the foxes
jumping quickly</code> to <code>the fox jump quick</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "standard",
  "filter": [ "kstem" ],
  "text": "the foxes jumping quickly"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ the, fox, jump, quick ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-kstem-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>kstem</code> filter to configure a new <a href="configure-text-analysis.html#analysis-custom-analyzer">custom
analyzer</a>.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
<div class="paragraph">
<p>To work properly, the <code>kstem</code> filter requires lowercase tokens. To ensure tokens
are lowercased, add the <a href="analysis-tokenfilters.html#analysis-lowercase-tokenfilter"><code>lowercase</code></a> filter
before the <code>kstem</code> filter in the analyzer configuration.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "whitespace",
          "filter": [
            "lowercase",
            "kstem"
          ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-length-tokenfilter">Length token filter</h3>
<titleabbrev>Length</titleabbrev>
<div class="paragraph">
<p>Removes tokens shorter or longer than specified character lengths.
For example, you can use the <code>length</code> filter to exclude tokens shorter than 2
characters and tokens longer than 5 characters.</p>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/miscellaneous/LengthFilter.html[LengthFilter].</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>The <code>length</code> filter removes entire tokens. If you&#8217;d prefer to shorten tokens to
a specific length, use the <a href="analysis-tokenfilters.html#analysis-truncate-tokenfilter"><code>truncate</code></a> filter.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="analysis-length-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>length</code>
filter to remove tokens longer than 4 characters:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "length",
      "min": 0,
      "max": 4
    }
  ],
  "text": "the quick brown fox jumps over the lazy dog"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ the, fox, over, the, lazy, dog ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-length-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>length</code> filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT length_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_length": {
          "tokenizer": "standard",
          "filter": [ "length" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-length-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>min</code></dt>
<dd>
<p>(Optional, integer)
Minimum character length of a token. Shorter tokens are excluded from the
output. Defaults to <code>0</code>.</p>
</dd>
<dt class="hdlist1"><code>max</code></dt>
<dd>
<p>(Optional, integer)
Maximum character length of a token. Longer tokens are excluded from the output.
Defaults to <code>Integer.MAX_VALUE</code>, which is <code>2^31-1</code> or <code>2147483647</code>.</p>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-length-tokenfilter-customize">Customize</h4>
<div class="paragraph">
<p>To customize the <code>length</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following request creates a custom <code>length</code> filter that removes
tokens shorter than 2 characters and tokens longer than 10 characters:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT length_custom_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_length_2_to_10_char": {
          "tokenizer": "whitespace",
          "filter": [ "length_2_to_10_char" ]
        }
      },
      "filter": {
        "length_2_to_10_char": {
          "type": "length",
          "min": 2,
          "max": 10
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-limit-token-count-tokenfilter">Limit token count token filter</h3>
<titleabbrev>Limit token count</titleabbrev>
<div class="paragraph">
<p>Limits the number of output tokens. The <code>limit</code> filter is commonly used to limit
the size of document field values based on token count.</p>
</div>
<div class="paragraph">
<p>By default, the <code>limit</code> filter keeps only the first token in a stream. For
example, the filter can change the token stream <code>[ one, two, three ]</code> to
<code>[ one ]</code>.</p>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/miscellaneous/LimitTokenCountFilter.html[LimitTokenCountFilter].</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="literalblock">
<div class="content">
<pre> If you want to limit the size of field values based on
_character length_, use the &lt;&lt;ignore-above,`ignore_above`&gt;&gt; mapping parameter.</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="analysis-limit-token-count-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>max_token_count</code></dt>
<dd>
<p>(Optional, integer)
Maximum number of tokens to keep. Once this limit is reached, any remaining
tokens are excluded from the output. Defaults to <code>1</code>.</p>
</dd>
<dt class="hdlist1"><code>consume_all_tokens</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, the <code>limit</code> filter exhausts the token stream, even if the
<code>max_token_count</code> has already been reached. Defaults to <code>false</code>.</p>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-limit-token-count-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>limit</code>
filter to keep only the first two tokens in <code>quick fox jumps over lazy dog</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer": "standard",
    "filter": [
    {
      "type": "limit",
      "max_token_count": 2
    }
  ],
  "text": "quick fox jumps over lazy dog"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ quick, fox ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-limit-token-count-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>limit</code> filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT limit_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_one_token_limit": {
          "tokenizer": "standard",
          "filter": [ "limit" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-limit-token-count-tokenfilter-customize">Customize</h4>
<div class="paragraph">
<p>To customize the <code>limit</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following request creates a custom <code>limit</code> filter that keeps
only the first five tokens of a stream:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT custom_limit_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_five_token_limit": {
          "tokenizer": "whitespace",
          "filter": [ "five_token_limit" ]
        }
      },
      "filter": {
        "five_token_limit": {
          "type": "limit",
          "max_token_count": 5
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-lowercase-tokenfilter">Lowercase token filter</h3>
<titleabbrev>Lowercase</titleabbrev>
<div class="paragraph">
<p>Changes token text to lowercase. For example, you can use the <code>lowercase</code> filter
to change <code>THE Lazy DoG</code> to <code>the lazy dog</code>.</p>
</div>
<div class="paragraph">
<p>In addition to a default filter, the <code>lowercase</code> token filter provides access to
Lucene&#8217;s language-specific lowercase filters for Greek, Irish, and Turkish.</p>
</div>
<div class="sect3">
<h4 id="analysis-lowercase-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the default
<code>lowercase</code> filter to change the <code>THE Quick FoX JUMPs</code> to lowercase:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer" : "standard",
  "filter" : ["lowercase"],
  "text" : "THE Quick FoX JUMPs"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ the, quick, fox, jumps ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-lowercase-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>lowercase</code> filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT lowercase_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_lowercase": {
          "tokenizer": "whitespace",
          "filter": [ "lowercase" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-lowercase-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>language</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, string)
Language-specific lowercase token filter to use. Valid values include:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>greek</code></dt>
<dd>
<p>Uses Lucene&#8217;s
{lucene-analysis-docs}/el/GreekLowerCaseFilter.html[GreekLowerCaseFilter]</p>
</dd>
<dt class="hdlist1"><code>irish</code></dt>
<dd>
<p>Uses Lucene&#8217;s
{lucene-analysis-docs}/ga/IrishLowerCaseFilter.html[IrishLowerCaseFilter]</p>
</dd>
<dt class="hdlist1"><code>turkish</code></dt>
<dd>
<p>Uses Lucene&#8217;s
{lucene-analysis-docs}/tr/TurkishLowerCaseFilter.html[TurkishLowerCaseFilter]</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>If not specified, defaults to Lucene&#8217;s
{lucene-analysis-docs}/core/LowerCaseFilter.html[LowerCaseFilter].</p>
</div>
</div>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-lowercase-tokenfilter-customize">Customize</h4>
<div class="paragraph">
<p>To customize the <code>lowercase</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following request creates a custom <code>lowercase</code> filter for the
Greek language:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT custom_lowercase_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "greek_lowercase_example": {
          "type": "custom",
          "tokenizer": "standard",
          "filter": ["greek_lowercase"]
        }
      },
      "filter": {
        "greek_lowercase": {
          "type": "lowercase",
          "language": "greek"
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-minhash-tokenfilter">MinHash token filter</h3>
<titleabbrev>MinHash</titleabbrev>
<div class="paragraph">
<p>Uses the <a href="https://en.wikipedia.org/wiki/MinHash">MinHash</a> technique to produce a
signature for a token stream. You can use MinHash signatures to estimate the
similarity of documents. See <a href="analysis-tokenfilters.html#analysis-minhash-tokenfilter-similarity-search">Using the <code>min_hash</code> token filter for similarity search</a>.</p>
</div>
<div class="paragraph">
<p>The <code>min_hash</code> filter performs the following operations on a token stream in
order:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Hashes each token in the stream.</p>
</li>
<li>
<p>Assigns the hashes to buckets, keeping only the smallest hashes of each
bucket.</p>
</li>
<li>
<p>Outputs the smallest hash from each bucket as a token stream.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/minhash/MinHashFilter.html[MinHashFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-minhash-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>bucket_count</code></dt>
<dd>
<p>(Optional, integer)
Number of buckets to which hashes are assigned. Defaults to <code>512</code>.</p>
</dd>
<dt class="hdlist1"><code>hash_count</code></dt>
<dd>
<p>(Optional, integer)
Number of ways to hash each token in the stream. Defaults to <code>1</code>.</p>
</dd>
<dt class="hdlist1"><code>hash_set_size</code></dt>
<dd>
<p>(Optional, integer)
Number of hashes to keep from each bucket. Defaults to <code>1</code>.</p>
<div class="paragraph">
<p>Hashes are retained by ascending size, starting with the bucket&#8217;s smallest hash
first.</p>
</div>
</dd>
<dt class="hdlist1"><code>with_rotation</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, the filter fills empty buckets with the value of the first non-empty
bucket to its circular right if the <code>hash_set_size</code> is <code>1</code>. If the
<code>bucket_count</code> argument is greater than <code>1</code>, this parameter defaults to <code>true</code>.
Otherwise, this parameter defaults to <code>false</code>.</p>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-minhash-tokenfilter-configuration-tips">Tips for configuring the <code>min_hash</code> filter</h4>
<div class="ulist">
<ul>
<li>
<p><code>min_hash</code> filter input tokens should typically be k-words shingles produced
from <a href="analysis-tokenfilters.html#analysis-shingle-tokenfilter">shingle token filter</a>. You should
choose <code>k</code> large enough so that the probability of any given shingle
occurring in a document is low. At the same time, as
internally each shingle is hashed into to 128-bit hash, you should choose
<code>k</code> small enough so that all possible
different k-words shingles can be hashed to 128-bit hash with
minimal collision.</p>
</li>
<li>
<p>We recommend you test different arguments for the <code>hash_count</code>, <code>bucket_count</code> and
<code>hash_set_size</code> parameters:</p>
<div class="ulist">
<ul>
<li>
<p>To improve precision, increase the <code>bucket_count</code> or
<code>hash_set_size</code> arguments. Higher <code>bucket_count</code> and <code>hash_set_size</code> values
increase the likelihood that different tokens are indexed to different
buckets.</p>
</li>
<li>
<p>To improve the recall, increase the value of the <code>hash_count</code> argument. For
example, setting <code>hash_count</code> to <code>2</code> hashes each token in two different ways,
increasing the number of potential candidates for search.</p>
</li>
</ul>
</div>
</li>
<li>
<p>By default, the <code>min_hash</code> filter produces 512 tokens for each document. Each
token is 16 bytes in size. This means each document&#8217;s size will be increased by
around 8Kb.</p>
</li>
<li>
<p>The <code>min_hash</code> filter is used for Jaccard similarity. This means
that it doesn&#8217;t matter how many times a document contains a certain token,
only that if it contains it or not.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="analysis-minhash-tokenfilter-similarity-search">Using the <code>min_hash</code> token filter for similarity search</h4>
<div class="paragraph">
<p>The <code>min_hash</code> token filter allows you to hash documents for similarity search.
Similarity search, or nearest neighbor search is a complex problem.
A naive solution requires an exhaustive pairwise comparison between a query
document and every document in an index. This is a prohibitive operation
if the index is large. A number of approximate nearest neighbor search
solutions have been developed to make similarity search more practical and
computationally feasible. One of these solutions involves hashing of documents.</p>
</div>
<div class="paragraph">
<p>Documents are hashed in a way that similar documents are more likely
to produce the same hash code and are put into the same hash bucket,
while dissimilar documents are more likely to be hashed into
different hash buckets. This type of hashing is known as
locality sensitive hashing (LSH).</p>
</div>
<div class="paragraph">
<p>Depending on what constitutes the similarity between documents,
various LSH functions <a href="https://arxiv.org/abs/1408.2927">have been proposed</a>.
For <a href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard similarity</a>, a popular
LSH function is <a href="https://en.wikipedia.org/wiki/MinHash">MinHash</a>.
A general idea of the way MinHash produces a signature for a document
is by applying a random permutation over the whole index vocabulary (random
numbering for the vocabulary), and recording the minimum value for this permutation
for the document (the minimum number for a vocabulary word that is present
in the document). The permutations are run several times;
combining the minimum values for all of them will constitute a
signature for the document.</p>
</div>
<div class="paragraph">
<p>In practice, instead of random permutations, a number of hash functions
are chosen. A hash function calculates a hash code for each of a
document&#8217;s tokens and chooses the minimum hash code among them.
The minimum hash codes from all hash functions are combined
to form a signature for the document.</p>
</div>
</div>
<div class="sect3">
<h4 id="analysis-minhash-tokenfilter-customize">Customize and add to an analyzer</h4>
<div class="paragraph">
<p>To customize the <code>min_hash</code> filter, duplicate it to create the basis for a new
custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following <a href="indices.html#indices-create-index">create index API</a> request
uses the following custom token filters to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>my_shingle_filter</code>, a custom <a href="analysis-tokenfilters.html#analysis-shingle-tokenfilter"><code>shingle</code>
filter</a>. <code>my_shingle_filter</code> only outputs five-word shingles.</p>
</li>
<li>
<p><code>my_minhash_filter</code>, a custom <code>min_hash</code> filter. <code>my_minhash_filter</code> hashes
each five-word shingle once. It then assigns the hashes into 512 buckets,
keeping only the smallest hash from each bucket.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The request also assigns the custom analyzer to the <code>fingerprint</code> field mapping.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "filter": {
        "my_shingle_filter": {      <b class="conum">(1)</b>
          "type": "shingle",
          "min_shingle_size": 5,
          "max_shingle_size": 5,
          "output_unigrams": false
        },
        "my_minhash_filter": {
          "type": "min_hash",
          "hash_count": 1,          <b class="conum">(2)</b>
          "bucket_count": 512,      <b class="conum">(3)</b>
          "hash_set_size": 1,       <b class="conum">(4)</b>
          "with_rotation": true     <b class="conum">(5)</b>
        }
      },
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [
            "my_shingle_filter",
            "my_minhash_filter"
          ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "fingerprint": {
        "type": "text",
        "analyzer": "my_analyzer"
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Configures a custom shingle filter to output only five-word shingles.</p>
</li>
<li>
<p>Each five-word shingle in the stream is hashed once.</p>
</li>
<li>
<p>The hashes are assigned to 512 buckets.</p>
</li>
<li>
<p>Only the smallest hash in each bucket is retained.</p>
</li>
<li>
<p>The filter fills empty buckets with the values of neighboring buckets.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-multiplexer-tokenfilter">Multiplexer token filter</h3>
<titleabbrev>Multiplexer</titleabbrev>
<div class="paragraph">
<p>A token filter of type <code>multiplexer</code> will emit multiple tokens at the same position,
each version of the token having been run through a different filter.  Identical
output tokens at the same position will be removed.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
If the incoming token stream has duplicate tokens, then these will also be
removed by the multiplexer
</td>
</tr>
</table>
</div>
<h3 id="_options" class="discrete">Options</h3>
<div class="hdlist">
<table>
<tr>
<td class="hdlist1">
filters
</td>
<td class="hdlist2">
<p>a list of token filters to apply to incoming tokens.  These can be any
token filters defined elsewhere in the index mappings.  Filters can be chained
using a comma-delimited string, so for example <code>"lowercase, porter_stem"</code> would
apply the <code>lowercase</code> filter and then the <code>porter_stem</code> filter to a single token.</p>
</td>
</tr>
</table>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<a href="analysis-tokenfilters.html#analysis-shingle-tokenfilter">Shingle</a> or multi-word synonym token filters will not function normally
  when they are declared in the filters array because they read ahead internally
  which is unsupported by the multiplexer
</td>
</tr>
</table>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">preserve_original</dt>
<dd>
<p>if <code>true</code> (the default) then emit the original token in
addition to the filtered tokens</p>
</dd>
</dl>
</div>
<h3 id="_settings_example" class="discrete">Settings example</h3>
<div class="paragraph">
<p>You can set it up like:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /multiplexer_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [ "my_multiplexer" ]
        }
      },
      "filter": {
        "my_multiplexer": {
          "type": "multiplexer",
          "filters": [ "lowercase", "lowercase, porter_stem" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>And test it like:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">POST /multiplexer_example/_analyze
{
  "analyzer" : "my_analyzer",
  "text" : "Going HOME"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>And it&#8217;d respond:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console-result" data-lang="console-result">{
  "tokens": [
    {
      "token": "Going",
      "start_offset": 0,
      "end_offset": 5,
      "type": "&lt;ALPHANUM&gt;",
      "position": 0
    },
    {
      "token": "going",
      "start_offset": 0,
      "end_offset": 5,
      "type": "&lt;ALPHANUM&gt;",
      "position": 0
    },
    {
      "token": "go",
      "start_offset": 0,
      "end_offset": 5,
      "type": "&lt;ALPHANUM&gt;",
      "position": 0
    },
    {
      "token": "HOME",
      "start_offset": 6,
      "end_offset": 10,
      "type": "&lt;ALPHANUM&gt;",
      "position": 1
    },
    {
      "token": "home",          <b class="conum">(1)</b>
      "start_offset": 6,
      "end_offset": 10,
      "type": "&lt;ALPHANUM&gt;",
      "position": 1
    }
  ]
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>The stemmer has also emitted a token <code>home</code> at position 1, but because it is a
duplicate of this token it has been removed from the token stream</p>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
The synonym and synonym_graph filters use their preceding analysis chain to
parse and analyse their synonym lists, and will throw an exception if that chain
contains token filters that produce multiple tokens at the same position.
If you want to apply synonyms to a token stream containing a multiplexer, then you
should append the synonym filter to each relevant multiplexer filter list, rather than
placing it after the multiplexer in the main token chain definition.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="analysis-ngram-tokenfilter">N-gram token filter</h3>
<titleabbrev>N-gram</titleabbrev>
<div class="paragraph">
<p>Forms <a href="https://en.wikipedia.org/wiki/N-gram">n-grams</a> of specified lengths from
a token.</p>
</div>
<div class="paragraph">
<p>For example, you can use the <code>ngram</code> token filter to change <code>fox</code> to
<code>[ f, fo, o, ox, x ]</code>.</p>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/ngram/NGramTokenFilter.html[NGramTokenFilter].</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>The <code>ngram</code> filter is similar to the
<a href="analysis-tokenfilters.html#analysis-edgengram-tokenfilter"><code>edge_ngram</code> token filter</a>. However, the
<code>edge_ngram</code> only outputs n-grams that start at the beginning of a token.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="analysis-ngram-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>ngram</code>
filter to convert <code>Quick fox</code> to 1-character and 2-character n-grams:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer": "standard",
  "filter": [ "ngram" ],
  "text": "Quick fox"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ Q, Qu, u, ui, i, ic, c, ck, k, f, fo, o, ox, x ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-ngram-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the <code>ngram</code>
filter to configure a new <a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT ngram_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_ngram": {
          "tokenizer": "standard",
          "filter": [ "ngram" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-ngram-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>max_gram</code></dt>
<dd>
<p>(Optional, integer)
Maximum length of characters in a gram. Defaults to <code>2</code>.</p>
</dd>
<dt class="hdlist1"><code>min_gram</code></dt>
<dd>
<p>(Optional, integer)
Minimum length of characters in a gram. Defaults to <code>1</code>.</p>
</dd>
<dt class="hdlist1"><code>preserve_original</code></dt>
<dd>
<p>(Optional, Boolean)
Emits original token when set to <code>true</code>. Defaults to <code>false</code>.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>You can use the <a href="index-modules.html#index-max-ngram-diff"><code>index.max_ngram_diff</code></a> index-level
setting to control the maximum allowed difference between the <code>max_gram</code> and
<code>min_gram</code> values.</p>
</div>
</div>
<div class="sect3">
<h4 id="analysis-ngram-tokenfilter-customize">Customize</h4>
<div class="paragraph">
<p>To customize the <code>ngram</code> filter, duplicate it to create the basis for a new
custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following request creates a custom <code>ngram</code> filter that forms
n-grams between 3-5 characters. The request also increases the
<code>index.max_ngram_diff</code> setting to <code>2</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT ngram_custom_example
{
  "settings": {
    "index": {
      "max_ngram_diff": 2
    },
    "analysis": {
      "analyzer": {
        "default": {
          "tokenizer": "whitespace",
          "filter": [ "3_5_grams" ]
        }
      },
      "filter": {
        "3_5_grams": {
          "type": "ngram",
          "min_gram": 3,
          "max_gram": 5
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-normalization-tokenfilter">Normalization token filters</h3>
<titleabbrev>Normalization</titleabbrev>
<div class="paragraph">
<p>There are several token filters available which try to normalize special
characters of a certain language.</p>
</div>
<div class="hdlist">
<table>
<tr>
<td class="hdlist1">
Arabic
</td>
<td class="hdlist2">
<p>{lucene-analysis-docs}/ar/ArabicNormalizer.html[<code>arabic_normalization</code>]</p>
</td>
</tr>
<tr>
<td class="hdlist1">
German
</td>
<td class="hdlist2">
<p>{lucene-analysis-docs}/de/GermanNormalizationFilter.html[<code>german_normalization</code>]</p>
</td>
</tr>
<tr>
<td class="hdlist1">
Hindi
</td>
<td class="hdlist2">
<p>{lucene-analysis-docs}/hi/HindiNormalizer.html[<code>hindi_normalization</code>]</p>
</td>
</tr>
<tr>
<td class="hdlist1">
Indic
</td>
<td class="hdlist2">
<p>{lucene-analysis-docs}/in/IndicNormalizer.html[<code>indic_normalization</code>]</p>
</td>
</tr>
<tr>
<td class="hdlist1">
Kurdish (Sorani)
</td>
<td class="hdlist2">
<p>{lucene-analysis-docs}/ckb/SoraniNormalizer.html[<code>sorani_normalization</code>]</p>
</td>
</tr>
<tr>
<td class="hdlist1">
Persian
</td>
<td class="hdlist2">
<p>{lucene-analysis-docs}/fa/PersianNormalizer.html[<code>persian_normalization</code>]</p>
</td>
</tr>
<tr>
<td class="hdlist1">
Scandinavian
</td>
<td class="hdlist2">
<p>{lucene-analysis-docs}/miscellaneous/ScandinavianNormalizationFilter.html[<code>scandinavian_normalization</code>],
{lucene-analysis-docs}/miscellaneous/ScandinavianFoldingFilter.html[<code>scandinavian_folding</code>]</p>
</td>
</tr>
<tr>
<td class="hdlist1">
Serbian
</td>
<td class="hdlist2">
<p>{lucene-analysis-docs}/sr/SerbianNormalizationFilter.html[<code>serbian_normalization</code>]</p>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="analysis-pattern-capture-tokenfilter">Pattern capture token filter</h3>
<titleabbrev>Pattern capture</titleabbrev>
<div class="paragraph">
<p>The <code>pattern_capture</code> token filter, unlike the <code>pattern</code> tokenizer,
emits a token for every capture group in the regular expression.
Patterns are not anchored to the beginning and end of the string, so
each pattern can match multiple times, and matches are allowed to
overlap.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="title">Beware of Pathological Regular Expressions</div>
<div class="paragraph">
<p>The pattern capture token filter uses
<a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html">Java Regular Expressions</a>.</p>
</div>
<div class="paragraph">
<p>A badly written regular expression could run very slowly or even throw a
StackOverflowError and cause the node it is running on to exit suddenly.</p>
</div>
<div class="paragraph">
<p>Read more about <a href="https://www.regular-expressions.info/catastrophic.html">pathological regular expressions and how to avoid them</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>For instance a pattern like :</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">"(([a-z]+)(\d*))"</code></pre>
</div>
</div>
<div class="paragraph">
<p>when matched against:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">"abc123def456"</code></pre>
</div>
</div>
<div class="paragraph">
<p>would produce the tokens: [ <code>abc123</code>, <code>abc</code>, <code>123</code>, <code>def456</code>, <code>def</code>,
<code>456</code> ]</p>
</div>
<div class="paragraph">
<p>If <code>preserve_original</code> is set to <code>true</code> (the default) then it would also
emit the original token: <code>abc123def456</code>.</p>
</div>
<div class="paragraph">
<p>This is particularly useful for indexing text like camel-case code, eg
<code>stripHTML</code> where a user may search for <code>"strip html"</code> or <code>"striphtml"</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT test
{
   "settings" : {
      "analysis" : {
         "filter" : {
            "code" : {
               "type" : "pattern_capture",
               "preserve_original" : true,
               "patterns" : [
                  "(\\p{Ll}+|\\p{Lu}\\p{Ll}+|\\p{Lu}+)",
                  "(\\d+)"
               ]
            }
         },
         "analyzer" : {
            "code" : {
               "tokenizer" : "pattern",
               "filter" : [ "code", "lowercase" ]
            }
         }
      }
   }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>When used to analyze the text</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">import static org.apache.commons.lang.StringEscapeUtils.escapeHtml</code></pre>
</div>
</div>
<div class="paragraph">
<p>this emits the tokens: [ <code>import</code>, <code>static</code>, <code>org</code>, <code>apache</code>, <code>commons</code>,
<code>lang</code>, <code>stringescapeutils</code>, <code>string</code>, <code>escape</code>, <code>utils</code>, <code>escapehtml</code>,
<code>escape</code>, <code>html</code> ]</p>
</div>
<div class="paragraph">
<p>Another example is analyzing email addresses:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT test
{
   "settings" : {
      "analysis" : {
         "filter" : {
            "email" : {
               "type" : "pattern_capture",
               "preserve_original" : true,
               "patterns" : [
                  "([^@]+)",
                  "(\\p{L}+)",
                  "(\\d+)",
                  "@(.+)"
               ]
            }
         },
         "analyzer" : {
            "email" : {
               "tokenizer" : "uax_url_email",
               "filter" : [ "email", "lowercase",  "unique" ]
            }
         }
      }
   }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>When the above analyzer is used on an email address like:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">john-smith_123@foo-bar.com</code></pre>
</div>
</div>
<div class="paragraph">
<p>it would produce the following tokens:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>john-smith_123@foo-bar.com, john-smith_123,
john, smith, 123, foo-bar.com, foo, bar, com</pre>
</div>
</div>
<div class="paragraph">
<p>Multiple patterns are required to allow overlapping captures, but also
means that patterns are less dense and easier to understand.</p>
</div>
<div class="paragraph">
<p><strong>Note:</strong> All tokens are emitted in the same position, and with the same
character offsets. This means, for example, that a <code>match</code> query for
<code>john-smith_123@foo-bar.com</code> that uses this analyzer will return documents
containing any of these tokens, even when using the <code>and</code> operator.
Also, when combined with highlighting, the whole original token will
be highlighted, not just the matching subset. For instance, querying
the above email address for <code>"smith"</code> would highlight:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-html" data-lang="html">  &lt;em&gt;john-smith_123@foo-bar.com&lt;/em&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>not:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-html" data-lang="html">  john-&lt;em&gt;smith&lt;/em&gt;_123@foo-bar.com</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-pattern_replace-tokenfilter">Pattern replace token filter</h3>
<titleabbrev>Pattern replace</titleabbrev>
<div class="paragraph">
<p>Uses a regular expression to match and replace token substrings.</p>
</div>
<div class="paragraph">
<p>The <code>pattern_replace</code> filter uses
<a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html">Java&#8217;s
regular expression syntax</a>. By default, the filter replaces matching substrings
with an empty substring (<code>""</code>). Replacement substrings can use Java&#8217;s
<a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Matcher.html#appendReplacement-java.lang.StringBuffer-java.lang.String-"><code>$g</code>
syntax</a> to reference capture groups from the original token text.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph">
<p>A poorly-written regular expression may run slowly or return a
StackOverflowError, causing the node running the expression to exit suddenly.</p>
</div>
<div class="paragraph">
<p>Read more about
<a href="https://www.regular-expressions.info/catastrophic.html">pathological regular
expressions and how to avoid them</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/pattern/PatternReplaceFilter.html[PatternReplaceFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-pattern-replace-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>pattern_replace</code>
filter to prepend <code>watch</code> to the substring <code>dog</code> in <code>foxes jump lazy dogs</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "pattern_replace",
      "pattern": "(dog)",
      "replacement": "watch$1"
    }
  ],
  "text": "foxes jump lazy dogs"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ foxes, jump, lazy, watchdogs ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-pattern-replace-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>all</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, all substrings matching the <code>pattern</code> parameter&#8217;s regular expression
are replaced. If <code>false</code>, the filter replaces only the first matching substring
in each token. Defaults to <code>true</code>.</p>
</dd>
<dt class="hdlist1"><code>pattern</code></dt>
<dd>
<p>(Required, string)
Regular expression, written in
<a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html">Java&#8217;s
regular expression syntax</a>. The filter replaces token substrings matching this
pattern with the substring in the <code>replacement</code> parameter.</p>
</dd>
<dt class="hdlist1"><code>replacement</code></dt>
<dd>
<p>(Optional, string)
Replacement substring. Defaults to an empty substring (<code>""</code>).</p>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-pattern-replace-tokenfilter-customize">Customize and add to an analyzer</h4>
<div class="paragraph">
<p>To customize the <code>pattern_replace</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request
configures a new <a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a> using a custom
<code>pattern_replace</code> filter, <code>my_pattern_replace_filter</code>.</p>
</div>
<div class="paragraph">
<p>The <code>my_pattern_replace_filter</code> filter uses the regular expression <code>[£|€]</code> to
match and remove the currency symbols <code>£</code> and <code>€</code>. The filter&#8217;s <code>all</code>
parameter is <code>false</code>, meaning only the first matching symbol in each token is
removed.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "keyword",
          "filter": [
            "my_pattern_replace_filter"
          ]
        }
      },
      "filter": {
        "my_pattern_replace_filter": {
          "type": "pattern_replace",
          "pattern": "[£|€]",
          "replacement": "",
          "all": false
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-phonetic-tokenfilter">Phonetic token filter</h3>
<titleabbrev>Phonetic</titleabbrev>
<div class="paragraph">
<p>The <code>phonetic</code> token filter is provided as the <a href="https://www.opensearch.org/guide/en/opensearch/plugins/{branch}/analysis-phonetic.html"><code>analysis-phonetic</code></a> plugin.</p>
</div>
</div>
<div class="sect2">
<h3 id="analysis-porterstem-tokenfilter">Porter stem token filter</h3>
<titleabbrev>Porter stem</titleabbrev>
<div class="paragraph">
<p>Provides <a href="analysis-concepts.html#algorithmic-stemmers">algorithmic stemming</a> for the English language,
based on the <a href="https://snowballstem.org/algorithms/porter/stemmer.html">Porter
stemming algorithm</a>.</p>
</div>
<div class="paragraph">
<p>This filter tends to stem more aggressively than other English
stemmer filters, such as the <a href="analysis-tokenfilters.html#analysis-kstem-tokenfilter"><code>kstem</code></a> filter.</p>
</div>
<div class="paragraph">
<p>The <code>porter_stem</code> filter is equivalent to the
<a href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter"><code>stemmer</code></a> filter&#8217;s
<a href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter-language-parm"><code>english</code></a> variant.</p>
</div>
<div class="paragraph">
<p>The <code>porter_stem</code> filter uses Lucene&#8217;s
{lucene-analysis-docs}/en/PorterStemFilter.html[PorterStemFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-porterstem-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following analyze API request uses the <code>porter_stem</code> filter to stem
<code>the foxes jumping quickly</code> to <code>the fox jump quickli</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "standard",
  "filter": [ "porter_stem" ],
  "text": "the foxes jumping quickly"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ the, fox, jump, quickli ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-porterstem-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>porter_stem</code> filter to configure a new <a href="configure-text-analysis.html#analysis-custom-analyzer">custom
analyzer</a>.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
<div class="paragraph">
<p>To work properly, the <code>porter_stem</code> filter requires lowercase tokens. To ensure
tokens are lowercased, add the <a href="analysis-tokenfilters.html#analysis-lowercase-tokenfilter"><code>lowercase</code></a>
filter before the <code>porter_stem</code> filter in the analyzer configuration.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "whitespace",
          "filter": [
            "lowercase",
            "porter_stem"
          ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-predicatefilter-tokenfilter">Predicate script token filter</h3>
<titleabbrev>Predicate script</titleabbrev>
<div class="paragraph">
<p>Removes tokens that don&#8217;t match a provided predicate script. The filter supports
inline <a href="https://www.opensearch.org/guide/en/opensearch/painless/{branch}/index.html">Painless</a> scripts only. Scripts are evaluated in
the <a href="https://www.opensearch.org/guide/en/opensearch/painless/{branch}/painless-analysis-predicate-context.html">analysis predicate
context</a>.</p>
</div>
<div class="sect3">
<h4 id="analysis-predicatefilter-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the
<code>predicate_token_filter</code> filter to only output tokens longer than three
characters from <code>the fox jumps the lazy dog</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "predicate_token_filter",
      "script": {
        "source": """
          token.term.length() &gt; 3
        """
      }
    }
  ],
  "text": "the fox jumps the lazy dog"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ jumps, lazy ]</code></pre>
</div>
</div>
<div class="paragraph">
<p>The API response contains the position and offsets of each output token. Note
the <code>predicate_token_filter</code> filter does not change the tokens' original
positions or offets.</p>
</div>
<details>
<summary class="title"><strong>Response</strong></summary>
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console-result" data-lang="console-result">{
  "tokens" : [
    {
      "token" : "jumps",
      "start_offset" : 8,
      "end_offset" : 13,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "lazy",
      "start_offset" : 18,
      "end_offset" : 22,
      "type" : "word",
      "position" : 4
    }
  ]
}</code></pre>
</div>
</div>
</div>
</details>
</div>
<div class="sect3">
<h4 id="analysis-predicatefilter-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>script</code></dt>
<dd>
<p>(Required, <a href="modules-scripting-using.html">script object</a>)
Script containing a condition used to filter incoming tokens. Only tokens that
match this script are included in the output.</p>
<div class="paragraph">
<p>This parameter supports inline <a href="https://www.opensearch.org/guide/en/opensearch/painless/{branch}/index.html">Painless</a> scripts only. The
script is evaluated in the
<a href="https://www.opensearch.org/guide/en/opensearch/painless/{branch}/painless-analysis-predicate-context.html">analysis predicate context</a>.</p>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-predicatefilter-tokenfilter-customize">Customize and add to an analyzer</h4>
<div class="paragraph">
<p>To customize the <code>predicate_token_filter</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request
configures a new <a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a> using a custom
<code>predicate_token_filter</code> filter, <code>my_script_filter</code>.</p>
</div>
<div class="paragraph">
<p>The <code>my_script_filter</code> filter removes tokens with of any type other than
<code>ALPHANUM</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [
            "my_script_filter"
          ]
        }
      },
      "filter": {
        "my_script_filter": {
          "type": "predicate_token_filter",
          "script": {
            "source": """
              token.type.contains("ALPHANUM")
            """
          }
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-remove-duplicates-tokenfilter">Remove duplicates token filter</h3>
<titleabbrev>Remove duplicates</titleabbrev>
<div class="paragraph">
<p>Removes duplicate tokens in the same position.</p>
</div>
<div class="paragraph">
<p>The <code>remove_duplicates</code> filter uses Lucene&#8217;s
{lucene-analysis-docs}/miscellaneous/RemoveDuplicatesTokenFilter.html[RemoveDuplicatesTokenFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-remove-duplicates-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>To see how the <code>remove_duplicates</code> filter works, you first need to produce a
token stream containing duplicate tokens in the same position.</p>
</div>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the
<a href="analysis-tokenfilters.html#analysis-keyword-repeat-tokenfilter"><code>keyword_repeat</code></a> and
<a href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter"><code>stemmer</code></a> filters to create stemmed and
unstemmed tokens for <code>jumping dog</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer": "whitespace",
  "filter": [
    "keyword_repeat",
    "stemmer"
  ],
  "text": "jumping dog"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The API returns the following response. Note that the <code>dog</code> token in position
<code>1</code> is duplicated.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console-result" data-lang="console-result">{
  "tokens": [
    {
      "token": "jumping",
      "start_offset": 0,
      "end_offset": 7,
      "type": "word",
      "position": 0
    },
    {
      "token": "jump",
      "start_offset": 0,
      "end_offset": 7,
      "type": "word",
      "position": 0
    },
    {
      "token": "dog",
      "start_offset": 8,
      "end_offset": 11,
      "type": "word",
      "position": 1
    },
    {
      "token": "dog",
      "start_offset": 8,
      "end_offset": 11,
      "type": "word",
      "position": 1
    }
  ]
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>To remove one of the duplicate <code>dog</code> tokens, add the <code>remove_duplicates</code> filter
to the previous analyze API request.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer": "whitespace",
  "filter": [
    "keyword_repeat",
    "stemmer",
    "remove_duplicates"
  ],
  "text": "jumping dog"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The API returns the following response. There is now only one <code>dog</code> token in
position <code>1</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console-result" data-lang="console-result">{
  "tokens": [
    {
      "token": "jumping",
      "start_offset": 0,
      "end_offset": 7,
      "type": "word",
      "position": 0
    },
    {
      "token": "jump",
      "start_offset": 0,
      "end_offset": 7,
      "type": "word",
      "position": 0
    },
    {
      "token": "dog",
      "start_offset": 8,
      "end_offset": 11,
      "type": "word",
      "position": 1
    }
  ]
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-remove-duplicates-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>remove_duplicates</code> filter to configure a new <a href="configure-text-analysis.html#analysis-custom-analyzer">custom
analyzer</a>.</p>
</div>
<div class="paragraph">
<p>This custom analyzer uses the <code>keyword_repeat</code> and <code>stemmer</code> filters to create a
stemmed and unstemmed version of each token in a stream. The <code>remove_duplicates</code>
filter then removes any duplicate tokens in the same position.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "tokenizer": "standard",
          "filter": [
            "keyword_repeat",
            "stemmer",
            "remove_duplicates"
          ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-reverse-tokenfilter">Reverse token filter</h3>
<titleabbrev>Reverse</titleabbrev>
<div class="paragraph">
<p>Reverses each token in a stream. For example, you can use the <code>reverse</code> filter
to change <code>cat</code> to <code>tac</code>.</p>
</div>
<div class="paragraph">
<p>Reversed tokens are useful for suffix-based searches,
such as finding words that end in <code>-ion</code> or searching file names by their
extension.</p>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/reverse/ReverseStringFilter.html[ReverseStringFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-reverse-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>reverse</code>
filter to reverse each token in <code>quick fox jumps</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer" : "standard",
  "filter" : ["reverse"],
  "text" : "quick fox jumps"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ kciuq, xof, spmuj ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-reverse-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>reverse</code> filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT reverse_example
{
  "settings" : {
    "analysis" : {
      "analyzer" : {
        "whitespace_reverse" : {
          "tokenizer" : "whitespace",
          "filter" : ["reverse"]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-shingle-tokenfilter">Shingle token filter</h3>
<titleabbrev>Shingle</titleabbrev>
<div class="paragraph">
<p>Add shingles, or word <a href="https://en.wikipedia.org/wiki/N-gram">n-grams</a>, to a token
stream by concatenating adjacent tokens. By default, the <code>shingle</code> token filter
outputs two-word shingles and unigrams.</p>
</div>
<div class="paragraph">
<p>For example, many tokenizers convert <code>the lazy dog</code> to <code>[ the, lazy, dog ]</code>. You
can use the <code>shingle</code> filter to add two-word shingles to this stream:
<code>[ the, the lazy, lazy, lazy dog, dog ]</code>.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
Shingles are often used to help speed up phrase queries, such as
<a href="full-text-queries.html#query-dsl-match-query-phrase"><code>match_phrase</code></a>. Rather than creating shingles
using the <code>shingles</code> filter, we recommend you use the
<a href="mapping-params.html#index-phrases"><code>index-phrases</code></a> mapping parameter on the appropriate
<a href="mapping-types.html#text">text</a> field instead.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/shingle/ShingleFilter.html[ShingleFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-shingle-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>shingle</code>
filter to add two-word shingles to the token stream for <code>quick brown fox jumps</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [ "shingle" ],
  "text": "quick brown fox jumps"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ quick, quick brown, brown, brown fox, fox, fox jumps, jumps ]</code></pre>
</div>
</div>
<div class="paragraph">
<p>To produce shingles of 2-3 words, add the following arguments to the analyze API
request:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>min_shingle_size</code>: <code>2</code></p>
</li>
<li>
<p><code>max_shingle_size</code>: <code>3</code></p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "shingle",
      "min_shingle_size": 2,
      "max_shingle_size": 3
    }
  ],
  "text": "quick brown fox jumps"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ quick, quick brown, quick brown fox, brown, brown fox, brown fox jumps, fox, fox jumps, jumps ]</code></pre>
</div>
</div>
<div class="paragraph">
<p>To only include shingles in the output, add an <code>output_unigrams</code> argument of
<code>false</code> to the request.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "shingle",
      "min_shingle_size": 2,
      "max_shingle_size": 3,
      "output_unigrams": false
    }
  ],
  "text": "quick brown fox jumps"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ quick brown, quick brown fox, brown fox, brown fox jumps, fox jumps ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-shingle-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>shingle</code> filter to configure a new <a href="configure-text-analysis.html#analysis-custom-analyzer">custom
analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_shingle": {
          "tokenizer": "standard",
          "filter": [ "shingle" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-shingle-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>max_shingle_size</code></dt>
<dd>
<p>(Optional, integer)
Maximum number of tokens to concatenate when creating shingles. Defaults to <code>2</code>.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
This value cannot be lower than the <code>min_shingle_size</code> argument, which
defaults to <code>2</code>. The difference between this value and the <code>min_shingle_size</code>
argument cannot exceed the <a href="index-modules.html#index-max-shingle-diff"><code>index.max_shingle_diff</code></a>
index-level setting, which defaults to <code>3</code>.
</td>
</tr>
</table>
</div>
</dd>
<dt class="hdlist1"><code>min_shingle_size</code></dt>
<dd>
<p>(Optional, integer)
Minimum number of tokens to concatenate when creating shingles. Defaults to <code>2</code>.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
This value cannot exceed the <code>max_shingle_size</code> argument, which defaults
to <code>2</code>. The difference between the <code>max_shingle_size</code> argument and this value
cannot exceed the <a href="index-modules.html#index-max-shingle-diff"><code>index.max_shingle_diff</code></a>
index-level setting, which defaults to <code>3</code>.
</td>
</tr>
</table>
</div>
</dd>
<dt class="hdlist1"><code>output_unigrams</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, the output includes the original input tokens. If <code>false</code>, the output
only includes shingles; the original input tokens are removed. Defaults to
<code>true</code>.</p>
</dd>
<dt class="hdlist1"><code>output_unigrams_if_no_shingles</code></dt>
<dd>
<p>If <code>true</code>, the output includes the original input tokens only if no shingles are
produced; if shingles are produced, the output only includes shingles. Defaults
to <code>false</code>.</p>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
If both this and the <code>output_unigrams</code> parameter are <code>true</code>, only the
<code>output_unigrams</code> argument is used.
</td>
</tr>
</table>
</div>
</dd>
<dt class="hdlist1"><code>token_separator</code></dt>
<dd>
<p>(Optional, string)
Separator used to concatenate adjacent tokens to form a shingle. Defaults to a
space (<code>" "</code>).</p>
</dd>
<dt class="hdlist1"><code>filler_token</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, string)
String used in shingles as a replacement for empty positions that do not contain
a token. This filler token is only used in shingles, not original unigrams.
Defaults to an underscore (<code>_</code>).</p>
</div>
<div class="paragraph">
<p>Some token filters, such as the <code>stop</code> filter, create empty positions when
removing stop words with a position increment greater than one.</p>
</div>
<details>
<summary class="title"><strong>Example</strong></summary>
<div class="content">
<div class="paragraph">
<p>In the following <a href="indices.html#indices-analyze">analyze API</a> request, the <code>stop</code> filter
removes the stop word <code>a</code> from <code>fox jumps a lazy dog</code>, creating an empty
position. The subsequent <code>shingle</code> filter replaces this empty position with a
plus sign (<code>+</code>) in shingles.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "stop",
      "stopwords": [ "a" ]
    },
    {
      "type": "shingle",
      "filler_token": "+"
    }
  ],
  "text": "fox jumps a lazy dog"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ fox, fox jumps, jumps, jumps +, + lazy, lazy, lazy dog, dog ]</code></pre>
</div>
</div>
</div>
</details>
</div>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-shingle-tokenfilter-customize">Customize</h4>
<div class="paragraph">
<p>To customize the <code>shingle</code> filter, duplicate it to create the basis for a new
custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following <a href="indices.html#indices-create-index">create index API</a> request
uses a custom <code>shingle</code> filter, <code>my_shingle_filter</code>, to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="paragraph">
<p>The <code>my_shingle_filter</code> filter uses a <code>min_shingle_size</code> of <code>2</code> and a
<code>max_shingle_size</code> of <code>5</code>, meaning it produces shingles of 2-5 words.
The filter also includes a <code>output_unigrams</code> argument of <code>false</code>, meaning that
only shingles are included in the output.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "en": {
          "tokenizer": "standard",
          "filter": [ "my_shingle_filter" ]
        }
      },
      "filter": {
        "my_shingle_filter": {
          "type": "shingle",
          "min_shingle_size": 2,
          "max_shingle_size": 5,
          "output_unigrams": false
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-snowball-tokenfilter">Snowball token filter</h3>
<titleabbrev>Snowball</titleabbrev>
<div class="paragraph">
<p>A filter that stems words using a Snowball-generated stemmer. The
<code>language</code> parameter controls the stemmer with the following available
values: <code>Arabic</code>, <code>Armenian</code>, <code>Basque</code>, <code>Catalan</code>, <code>Danish</code>, <code>Dutch</code>, <code>English</code>,
<code>Estonian</code>, <code>Finnish</code>, <code>French</code>, <code>German</code>, <code>German2</code>, <code>Hungarian</code>, <code>Italian</code>, <code>Irish</code>, <code>Kp</code>,
<code>Lithuanian</code>, <code>Lovins</code>, <code>Norwegian</code>, <code>Porter</code>, <code>Portuguese</code>, <code>Romanian</code>,
<code>Russian</code>, <code>Spanish</code>, <code>Swedish</code>, <code>Turkish</code>.</p>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [ "lowercase", "my_snow" ]
        }
      },
      "filter": {
        "my_snow": {
          "type": "snowball",
          "language": "Lovins"
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-stemmer-tokenfilter">Stemmer token filter</h3>
<titleabbrev>Stemmer</titleabbrev>
<div class="paragraph">
<p>Provides <a href="analysis-concepts.html#algorithmic-stemmers">algorithmic stemming</a> for several languages,
some with additional variants. For a list of supported languages, see the
<a href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter-language-parm"><code>language</code></a> parameter.</p>
</div>
<div class="paragraph">
<p>When not customized, the filter uses the
<a href="https://snowballstem.org/algorithms/porter/stemmer.html">porter stemming
algorithm</a> for English.</p>
</div>
<div class="sect3">
<h4 id="analysis-stemmer-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following analyze API request uses the <code>stemmer</code> filter&#8217;s default porter
stemming algorithm to stem <code>the foxes jumping quickly</code> to <code>the fox jump
quickli</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "standard",
  "filter": [ "stemmer" ],
  "text": "the foxes jumping quickly"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ the, fox, jump, quickli ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-stemmer-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>stemmer</code> filter to configure a new <a href="configure-text-analysis.html#analysis-custom-analyzer">custom
analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "whitespace",
          "filter": [ "stemmer" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3 child_attributes">
<h4 id="analysis-stemmer-tokenfilter-configure-parms">Configurable parameters</h4>
<div id="analysis-stemmer-tokenfilter-language-parm" class="dlist">
<dl>
<dt class="hdlist1"><code>language</code></dt>
<dd>
<p>(Optional, string)
Language-dependent stemming algorithm used to stem tokens. If both this and the
<code>name</code> parameter are specified, the <code>language</code> parameter argument is used.</p>
<details open>
<summary class="title">Valid values for <code>language</code></summary>
<div class="content">
<div class="paragraph">
<p>Valid values are sorted by language. Defaults to
<a href="https://snowballstem.org/algorithms/porter/stemmer.html"><strong><code>english</code></strong></a>.
Recommended algorithms are <strong>bolded</strong>.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Arabic</dt>
<dd>
<p>{lucene-analysis-docs}/ar/ArabicStemmer.html[<strong><code>arabic</code></strong>]</p>
</dd>
<dt class="hdlist1">Armenian</dt>
<dd>
<p><a href="https://snowballstem.org/algorithms/armenian/stemmer.html"><strong><code>armenian</code></strong></a></p>
</dd>
<dt class="hdlist1">Basque</dt>
<dd>
<p><a href="https://snowballstem.org/algorithms/basque/stemmer.html"><strong><code>basque</code></strong></a></p>
</dd>
<dt class="hdlist1">Bengali</dt>
<dd>
<p><a href="https://www.tandfonline.com/doi/abs/10.1080/02564602.1993.11437284"><strong><code>bengali</code></strong></a></p>
</dd>
<dt class="hdlist1">Brazilian Portuguese</dt>
<dd>
<p>{lucene-analysis-docs}/br/BrazilianStemmer.html[<strong><code>brazilian</code></strong>]</p>
</dd>
<dt class="hdlist1">Bulgarian</dt>
<dd>
<p><a href="http://members.unine.ch/jacques.savoy/Papers/BUIR.pdf"><strong><code>bulgarian</code></strong></a></p>
</dd>
<dt class="hdlist1">Catalan</dt>
<dd>
<p><a href="https://snowballstem.org/algorithms/catalan/stemmer.html"><strong><code>catalan</code></strong></a></p>
</dd>
<dt class="hdlist1">Czech</dt>
<dd>
<p><a href="https://dl.acm.org/doi/10.1016/j.ipm.2009.06.001"><strong><code>czech</code></strong></a></p>
</dd>
<dt class="hdlist1">Danish</dt>
<dd>
<p><a href="https://snowballstem.org/algorithms/danish/stemmer.html"><strong><code>danish</code></strong></a></p>
</dd>
<dt class="hdlist1">Dutch</dt>
<dd>
<p><a href="https://snowballstem.org/algorithms/dutch/stemmer.html"><strong><code>dutch</code></strong></a>,
<a href="https://snowballstem.org/algorithms/kraaij_pohlmann/stemmer.html"><code>dutch_kp</code></a></p>
</dd>
<dt class="hdlist1">English</dt>
<dd>
<p><a href="https://snowballstem.org/algorithms/porter/stemmer.html"><strong><code>english</code></strong></a>,
<a href="https://ciir.cs.umass.edu/pubfiles/ir-35.pdf"><code>light_english</code></a>,
<a href="https://snowballstem.org/algorithms/lovins/stemmer.html"><code>lovins</code></a>,
<a href="https://www.researchgate.net/publication/220433848_How_effective_is_suffixing"><code>minimal_english</code></a>,
<a href="https://snowballstem.org/algorithms/english/stemmer.html"><code>porter2</code></a>,
{lucene-analysis-docs}/en/EnglishPossessiveFilter.html[<code>possessive_english</code>]</p>
</dd>
<dt class="hdlist1">Estonian</dt>
<dd>
<p><a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/tartarus/snowball/ext/EstonianStemmer.html"><strong><code>estonian</code></strong></a></p>
</dd>
<dt class="hdlist1">Finnish</dt>
<dd>
<p><a href="https://snowballstem.org/algorithms/finnish/stemmer.html"><strong><code>finnish</code></strong></a>,
<a href="http://clef.isti.cnr.it/2003/WN_web/22.pdf"><code>light_finnish</code></a></p>
</dd>
<dt class="hdlist1">French</dt>
<dd>
<p><a href="https://dl.acm.org/citation.cfm?id=1141523"><strong><code>light_french</code></strong></a>,
<a href="https://snowballstem.org/algorithms/french/stemmer.html"><code>french</code></a>,
<a href="https://dl.acm.org/citation.cfm?id=318984"><code>minimal_french</code></a></p>
</dd>
<dt class="hdlist1">Galician</dt>
<dd>
<p><a href="http://bvg.udc.es/recursos_lingua/stemming.jsp"><strong><code>galician</code></strong></a>,
<a href="http://bvg.udc.es/recursos_lingua/stemming.jsp"><code>minimal_galician</code></a> (Plural step only)</p>
</dd>
<dt class="hdlist1">German</dt>
<dd>
<p><a href="https://dl.acm.org/citation.cfm?id=1141523"><strong><code>light_german</code></strong></a>,
<a href="https://snowballstem.org/algorithms/german/stemmer.html"><code>german</code></a>,
<a href="https://snowballstem.org/algorithms/german2/stemmer.html"><code>german2</code></a>,
<a href="http://members.unine.ch/jacques.savoy/clef/morpho.pdf"><code>minimal_german</code></a></p>
</dd>
<dt class="hdlist1">Greek</dt>
<dd>
<p><a href="https://sais.se/mthprize/2007/ntais2007.pdf"><strong><code>greek</code></strong></a></p>
</dd>
<dt class="hdlist1">Hindi</dt>
<dd>
<p><a href="http://computing.open.ac.uk/Sites/EACLSouthAsia/Papers/p6-Ramanathan.pdf"><strong><code>hindi</code></strong></a></p>
</dd>
<dt class="hdlist1">Hungarian</dt>
<dd>
<p><a href="https://snowballstem.org/algorithms/hungarian/stemmer.html"><strong><code>hungarian</code></strong></a>,
<a href="https://dl.acm.org/citation.cfm?id=1141523&amp;dl=ACM&amp;coll=DL&amp;CFID=179095584&amp;CFTOKEN=80067181"><code>light_hungarian</code></a></p>
</dd>
<dt class="hdlist1">Indonesian</dt>
<dd>
<p><a href="http://www.illc.uva.nl/Publications/ResearchReports/MoL-2003-02.text.pdf"><strong><code>indonesian</code></strong></a></p>
</dd>
<dt class="hdlist1">Irish</dt>
<dd>
<p><a href="https://snowballstem.org/otherapps/oregan/"><strong><code>irish</code></strong></a></p>
</dd>
<dt class="hdlist1">Italian</dt>
<dd>
<p><a href="https://www.ercim.eu/publication/ws-proceedings/CLEF2/savoy.pdf"><strong><code>light_italian</code></strong></a>,
<a href="https://snowballstem.org/algorithms/italian/stemmer.html"><code>italian</code></a></p>
</dd>
<dt class="hdlist1">Kurdish (Sorani)</dt>
<dd>
<p>{lucene-analysis-docs}/ckb/SoraniStemmer.html[<strong><code>sorani</code></strong>]</p>
</dd>
<dt class="hdlist1">Latvian</dt>
<dd>
<p>{lucene-analysis-docs}/lv/LatvianStemmer.html[<strong><code>latvian</code></strong>]</p>
</dd>
<dt class="hdlist1">Lithuanian</dt>
<dd>
<p><a href="https://svn.apache.org/viewvc/lucene/dev/branches/lucene_solr_5_3/lucene/analysis/common/src/java/org/apache/lucene/analysis/lt/stem_ISO_8859_1.sbl?view=markup"><strong><code>lithuanian</code></strong></a></p>
</dd>
<dt class="hdlist1">Norwegian (Bokmål)</dt>
<dd>
<p><a href="https://snowballstem.org/algorithms/norwegian/stemmer.html"><strong><code>norwegian</code></strong></a>,
{lucene-analysis-docs}/no/NorwegianLightStemmer.html[<strong><code>light_norwegian</code></strong>],
{lucene-analysis-docs}/no/NorwegianMinimalStemmer.html[<code>minimal_norwegian</code>]</p>
</dd>
<dt class="hdlist1">Norwegian (Nynorsk)</dt>
<dd>
<p>{lucene-analysis-docs}/no/NorwegianLightStemmer.html[<strong><code>light_nynorsk</code></strong>],
{lucene-analysis-docs}/no/NorwegianMinimalStemmer.html[<code>minimal_nynorsk</code>]</p>
</dd>
<dt class="hdlist1">Portuguese</dt>
<dd>
<p><a href="https://dl.acm.org/citation.cfm?id=1141523&amp;dl=ACM&amp;coll=DL&amp;CFID=179095584&amp;CFTOKEN=80067181"><strong><code>light_portuguese</code></strong></a>,
<a href="http://www.inf.ufrgs.br/~buriol/papers/Orengo_CLEF07.pdf">`minimal_portuguese`</a>,
<a href="https://snowballstem.org/algorithms/portuguese/stemmer.html"><code>portuguese</code></a>,
<a href="https://www.inf.ufrgs.br/\~viviane/rslp/index.htm"><code>portuguese_rslp</code></a></p>
</dd>
<dt class="hdlist1">Romanian</dt>
<dd>
<p><a href="https://snowballstem.org/algorithms/romanian/stemmer.html"><strong><code>romanian</code></strong></a></p>
</dd>
<dt class="hdlist1">Russian</dt>
<dd>
<p><a href="https://snowballstem.org/algorithms/russian/stemmer.html"><strong><code>russian</code></strong></a>,
<a href="https://doc.rero.ch/lm.php?url=1000%2C43%2C4%2C20091209094227-CA%2FDolamic_Ljiljana_-_Indexing_and_Searching_Strategies_for_the_Russian_20091209.pdf"><code>light_russian</code></a></p>
</dd>
<dt class="hdlist1">Spanish</dt>
<dd>
<p><a href="https://www.ercim.eu/publication/ws-proceedings/CLEF2/savoy.pdf"><strong><code>light_spanish</code></strong></a>,
<a href="https://snowballstem.org/algorithms/spanish/stemmer.html"><code>spanish</code></a></p>
</dd>
<dt class="hdlist1">Swedish</dt>
<dd>
<p><a href="https://snowballstem.org/algorithms/swedish/stemmer.html"><strong><code>swedish</code></strong></a>,
<a href="http://clef.isti.cnr.it/2003/WN_web/22.pdf"><code>light_swedish</code></a></p>
</dd>
<dt class="hdlist1">Turkish</dt>
<dd>
<p><a href="https://snowballstem.org/algorithms/turkish/stemmer.html"><strong><code>turkish</code></strong></a></p>
</dd>
</dl>
</div>
</div>
</details>
</dd>
<dt class="hdlist1"><code>name</code></dt>
<dd>
<p>An alias for the <a href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter-language-parm"><code>language</code></a>
parameter. If both this and the <code>language</code> parameter are specified, the
<code>language</code> parameter argument is used.</p>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-stemmer-tokenfilter-customize">Customize</h4>
<div class="paragraph">
<p>To customize the <code>stemmer</code> filter, duplicate it to create the basis for a new
custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following request creates a custom <code>stemmer</code> filter that stems
words using the <code>light_german</code> algorithm:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "my_stemmer"
          ]
        }
      },
      "filter": {
        "my_stemmer": {
          "type": "stemmer",
          "language": "light_german"
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-stemmer-override-tokenfilter">Stemmer override token filter</h3>
<titleabbrev>Stemmer override</titleabbrev>
<div class="paragraph">
<p>Overrides stemming algorithms, by applying a custom mapping, then
protecting these terms from being modified by stemmers. Must be placed
before any stemming filters.</p>
</div>
<div class="paragraph">
<p>Rules are mappings in the form of <code>token1[, &#8230;&#8203;, tokenN] &#8658; override</code>.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Setting</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>rules</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A list of mapping rules to use.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>rules_path</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A path (either relative to <code>config</code> location, or
absolute) to a list of mappings.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Here is an example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [ "lowercase", "custom_stems", "porter_stem" ]
        }
      },
      "filter": {
        "custom_stems": {
          "type": "stemmer_override",
          "rules_path": "analysis/stemmer_override.txt"
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Where the file looks like:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-stemmer_override" data-lang="stemmer_override">running, runs =&gt; run

stemmer =&gt; stemmer</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can also define the overrides rules inline:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [ "lowercase", "custom_stems", "porter_stem" ]
        }
      },
      "filter": {
        "custom_stems": {
          "type": "stemmer_override",
          "rules": [
            "running, runs =&gt; run",
            "stemmer =&gt; stemmer"
          ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-stop-tokenfilter">Stop token filter</h3>
<titleabbrev>Stop</titleabbrev>
<div class="paragraph">
<p>Removes <a href="https://en.wikipedia.org/wiki/Stop_words">stop words</a> from a token
stream.</p>
</div>
<div class="paragraph">
<p>When not customized, the filter removes the following English stop words by
default:</p>
</div>
<div class="paragraph">
<p><code>a</code>, <code>an</code>, <code>and</code>, <code>are</code>, <code>as</code>, <code>at</code>, <code>be</code>, <code>but</code>, <code>by</code>, <code>for</code>, <code>if</code>, <code>in</code>,
<code>into</code>, <code>is</code>, <code>it</code>, <code>no</code>, <code>not</code>, <code>of</code>, <code>on</code>, <code>or</code>, <code>such</code>, <code>that</code>, <code>the</code>,
<code>their</code>, <code>then</code>, <code>there</code>, <code>these</code>, <code>they</code>, <code>this</code>, <code>to</code>, <code>was</code>, <code>will</code>, <code>with</code></p>
</div>
<div class="paragraph">
<p>In addition to English, the <code>stop</code> filter supports predefined
<a href="analysis-tokenfilters.html#analysis-stop-tokenfilter-stop-words-by-lang">stop word lists for several
languages</a>. You can also specify your own stop words as an array or file.</p>
</div>
<div class="paragraph">
<p>The <code>stop</code> filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/core/org/apache/lucene/analysis/StopFilter.html">StopFilter</a>.</p>
</div>
<div class="sect3">
<h4 id="analysis-stop-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following analyze API request uses the <code>stop</code> filter to remove the stop words
<code>a</code> and <code>the</code> from <code>a quick fox jumps over the lazy dog</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "standard",
  "filter": [ "stop" ],
  "text": "a quick fox jumps over the lazy dog"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ quick, fox, jumps, over, lazy, dog ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-stop-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the <code>stop</code>
filter to configure a new <a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "whitespace",
          "filter": [ "stop" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-stop-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>stopwords</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, string or array of strings)
Language value, such as <code><em>arabic</em></code> or <code><em>thai</em></code>. Defaults to
<a href="analysis-tokenfilters.html#english-stop-words"><code><em>english</em></code></a>.</p>
</div>
<div class="paragraph">
<p>Each language value corresponds to a predefined list of stop words in Lucene.
See <a href="analysis-tokenfilters.html#analysis-stop-tokenfilter-stop-words-by-lang">Stop words by language</a> for supported language
values and their stop words.</p>
</div>
<div class="paragraph">
<p>Also accepts an array of stop words.</p>
</div>
<div class="paragraph">
<p>For an empty list of stop words, use <code><em>none</em></code>.</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>stopwords_path</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, string)
Path to a file that contains a list of stop words to remove.</p>
</div>
<div class="paragraph">
<p>This path must be absolute or relative to the <code>config</code> location, and the file
must be UTF-8 encoded. Each stop word in the file must be separated by a line
break.</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>ignore_case</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, stop word matching is case insensitive. For example, if <code>true</code>, a
stop word of <code>the</code> matches and removes <code>The</code>, <code>THE</code>, or <code>the</code>. Defaults to
<code>false</code>.</p>
</dd>
<dt class="hdlist1"><code>remove_trailing</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, Boolean)
If <code>true</code>, the last token of a stream is removed if it&#8217;s a stop word. Defaults
to <code>true</code>.</p>
</div>
<div class="paragraph">
<p>This parameter should be <code>false</code> when using the filter with a
<a href="search.html#completion-suggester">completion suggester</a>. This would ensure a query like
<code>green a</code> matches and suggests <code>green apple</code> while still removing other stop
words.</p>
</div>
</div>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-stop-tokenfilter-customize">Customize</h4>
<div class="paragraph">
<p>To customize the <code>stop</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following request creates a custom case-insensitive <code>stop</code>
filter that removes stop words from the <a href="analysis-tokenfilters.html#english-stop-words"><code><em>english</em></code></a> stop
words list:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "default": {
          "tokenizer": "whitespace",
          "filter": [ "my_custom_stop_words_filter" ]
        }
      },
      "filter": {
        "my_custom_stop_words_filter": {
          "type": "stop",
          "ignore_case": true
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can also specify your own list of stop words. For example, the following
request creates a custom case-sensitive <code>stop</code> filter that removes only the stop
words <code>and</code>, <code>is</code>, and <code>the</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "default": {
          "tokenizer": "whitespace",
          "filter": [ "my_custom_stop_words_filter" ]
        }
      },
      "filter": {
        "my_custom_stop_words_filter": {
          "type": "stop",
          "ignore_case": true,
          "stopwords": [ "and", "is", "the" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-stop-tokenfilter-stop-words-by-lang">Stop words by language</h4>
<div class="paragraph">
<p>The following list contains supported language values for the <code>stopwords</code>
parameter and a link to their predefined stop words in Lucene.</p>
</div>
<div id="arabic-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>arabic</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/ar/stopwords.txt[Arabic stop words]</p>
</dd>
</dl>
</div>
<div id="armenian-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>armenian</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/hy/stopwords.txt[Armenian stop words]</p>
</dd>
</dl>
</div>
<div id="basque-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>basque</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/eu/stopwords.txt[Basque stop words]</p>
</dd>
</dl>
</div>
<div id="bengali-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>bengali</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/bn/stopwords.txt[Bengali stop words]</p>
</dd>
</dl>
</div>
<div id="brazilian-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>brazilian</em></code> (Brazilian Portuguese)</dt>
<dd>
<p>{lucene-stop-word-link}/br/stopwords.txt[Brazilian Portuguese stop words]</p>
</dd>
</dl>
</div>
<div id="bulgarian-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>bulgarian</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/bg/stopwords.txt[Bulgarian stop words]</p>
</dd>
</dl>
</div>
<div id="catalan-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>catalan</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/ca/stopwords.txt[Catalan stop words]</p>
</dd>
</dl>
</div>
<div id="cjk-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>cjk</em></code> (Chinese, Japanese, and Korean)</dt>
<dd>
<p>{lucene-stop-word-link}/cjk/stopwords.txt[CJK stop words]</p>
</dd>
</dl>
</div>
<div id="czech-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>czech</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/cz/stopwords.txt[Czech stop words]</p>
</dd>
</dl>
</div>
<div id="danish-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>danish</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/snowball/danish_stop.txt[Danish stop words]</p>
</dd>
</dl>
</div>
<div id="dutch-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>dutch</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/snowball/dutch_stop.txt[Dutch stop words]</p>
</dd>
</dl>
</div>
<div id="english-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>english</em></code></dt>
<dd>
<p><a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java#L46">English stop words</a></p>
</dd>
</dl>
</div>
<div id="estonian-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>estonian</em></code></dt>
<dd>
<p><a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/et/stopwords.txt">Estonian stop words</a></p>
</dd>
</dl>
</div>
<div id="finnish-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>finnish</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/snowball/finnish_stop.txt[Finnish stop words]</p>
</dd>
</dl>
</div>
<div id="french-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>french</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/snowball/french_stop.txt[French stop words]</p>
</dd>
</dl>
</div>
<div id="galician-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>galician</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/gl/stopwords.txt[Galician stop words]</p>
</dd>
</dl>
</div>
<div id="german-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>german</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/snowball/german_stop.txt[German stop words]</p>
</dd>
</dl>
</div>
<div id="greek-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>greek</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/el/stopwords.txt[Greek stop words]</p>
</dd>
</dl>
</div>
<div id="hindi-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>hindi</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/hi/stopwords.txt[Hindi stop words]</p>
</dd>
</dl>
</div>
<div id="hungarian-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>hungarian</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/snowball/hungarian_stop.txt[Hungarian stop words]</p>
</dd>
</dl>
</div>
<div id="indonesian-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>indonesian</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/id/stopwords.txt[Indonesian stop words]</p>
</dd>
</dl>
</div>
<div id="irish-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>irish</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/ga/stopwords.txt[Irish stop words]</p>
</dd>
</dl>
</div>
<div id="italian-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>italian</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/snowball/italian_stop.txt[Italian stop words]</p>
</dd>
</dl>
</div>
<div id="latvian-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>latvian</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/lv/stopwords.txt[Latvian stop words]</p>
</dd>
</dl>
</div>
<div id="lithuanian-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>lithuanian</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/lt/stopwords.txt[Lithuanian stop words]</p>
</dd>
</dl>
</div>
<div id="norwegian-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>norwegian</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/snowball/norwegian_stop.txt[Norwegian stop words]</p>
</dd>
</dl>
</div>
<div id="persian-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>persian</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/fa/stopwords.txt[Persian stop words]</p>
</dd>
</dl>
</div>
<div id="portuguese-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>portuguese</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/snowball/portuguese_stop.txt[Portuguese stop words]</p>
</dd>
</dl>
</div>
<div id="romanian-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>romanian</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/ro/stopwords.txt[Romanian stop words]</p>
</dd>
</dl>
</div>
<div id="russian-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>russian</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/snowball/russian_stop.txt[Russian stop words]</p>
</dd>
</dl>
</div>
<div id="sorani-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>sorani</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/ckb/stopwords.txt[Sorani stop words]</p>
</dd>
</dl>
</div>
<div id="spanish-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>spanish</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/snowball/spanish_stop.txt[Spanish stop words]</p>
</dd>
</dl>
</div>
<div id="swedish-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>swedish</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/snowball/swedish_stop.txt[Swedish stop words]</p>
</dd>
</dl>
</div>
<div id="thai-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>thai</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/th/stopwords.txt[Thai stop words]</p>
</dd>
</dl>
</div>
<div id="turkish-stop-words" class="dlist">
<dl>
<dt class="hdlist1"><code><em>turkish</em></code></dt>
<dd>
<p>{lucene-stop-word-link}/tr/stopwords.txt[Turkish stop words]</p>
</dd>
</dl>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-synonym-tokenfilter">Synonym token filter</h3>
<titleabbrev>Synonym</titleabbrev>
<div class="paragraph">
<p>The <code>synonym</code> token filter allows to easily handle synonyms during the
analysis process. Synonyms are configured using a configuration file.
Here is an example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "analyzer": {
          "synonym": {
            "tokenizer": "whitespace",
            "filter": [ "synonym" ]
          }
        },
        "filter": {
          "synonym": {
            "type": "synonym",
            "synonyms_path": "analysis/synonym.txt"
          }
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above configures a <code>synonym</code> filter, with a path of
<code>analysis/synonym.txt</code> (relative to the <code>config</code> location). The
<code>synonym</code> analyzer is then configured with the filter.</p>
</div>
<div class="paragraph">
<p>This filter tokenizes synonyms with whatever tokenizer and token filters
appear before it in the chain.</p>
</div>
<div class="paragraph">
<p>Additional settings are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>expand</code> (defaults to <code>true</code>).</p>
</li>
<li>
<p><code>lenient</code> (defaults to <code>false</code>). If <code>true</code> ignores exceptions while parsing the synonym configuration. It is important
to note that only those synonym rules which cannot get parsed are ignored. For instance consider the following request:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "analyzer": {
          "synonym": {
            "tokenizer": "standard",
            "filter": [ "my_stop", "synonym" ]
          }
        },
        "filter": {
          "my_stop": {
            "type": "stop",
            "stopwords": [ "bar" ]
          },
          "synonym": {
            "type": "synonym",
            "lenient": true,
            "synonyms": [ "foo, bar =&gt; baz" ]
          }
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>With the above request the word <code>bar</code> gets skipped but a mapping <code>foo &#8658; baz</code> is still added. However, if the mapping
being added was <code>foo, baz &#8658; bar</code> nothing would get added to the synonym list. This is because the target word for the
mapping is itself eliminated because it was a stop word. Similarly, if the mapping was "bar, foo, baz" and <code>expand</code> was
set to <code>false</code> no mapping would get added as when <code>expand=false</code> the target mapping is the first word. However, if
<code>expand=true</code> then the mappings added would be equivalent to <code>foo, baz &#8658; foo, baz</code> i.e, all mappings other than the
stop word.</p>
</div>
<h4 id="synonym-tokenizer-ignore_case-deprecated" class="discrete"><code>tokenizer</code> and <code>ignore_case</code> are deprecated</h4>
<div class="paragraph">
<p>The <code>tokenizer</code> parameter controls the tokenizers that will be used to
tokenize the synonym, this parameter is for backwards compatibility for indices that created before 6.0.
The <code>ignore_case</code> parameter works with <code>tokenizer</code> parameter only.</p>
</div>
<div class="paragraph">
<p>Two synonym formats are supported: Solr, WordNet.</p>
</div>
<h4 id="_solr_synonyms" class="discrete">Solr synonyms</h4>
<div class="paragraph">
<p>The following is a sample format of the file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-synonyms" data-lang="synonyms"># Blank lines and lines starting with pound are comments.

# Explicit mappings match any token sequence on the LHS of "=&gt;"
# and replace with all alternatives on the RHS.  These types of mappings
# ignore the expand parameter in the schema.
# Examples:
i-pod, i pod =&gt; ipod,
sea biscuit, sea biscit =&gt; seabiscuit

# Equivalent synonyms may be separated with commas and give
# no explicit mapping.  In this case the mapping behavior will
# be taken from the expand parameter in the schema.  This allows
# the same synonym file to be used in different synonym handling strategies.
# Examples:
ipod, i-pod, i pod
foozball , foosball
universe , cosmos
lol, laughing out loud

# If expand==true, "ipod, i-pod, i pod" is equivalent
# to the explicit mapping:
ipod, i-pod, i pod =&gt; ipod, i-pod, i pod
# If expand==false, "ipod, i-pod, i pod" is equivalent
# to the explicit mapping:
ipod, i-pod, i pod =&gt; ipod

# Multiple synonym mapping entries are merged.
foo =&gt; foo bar
foo =&gt; baz
# is equivalent to
foo =&gt; foo bar, baz</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can also define synonyms for the filter directly in the
configuration file (note use of <code>synonyms</code> instead of <code>synonyms_path</code>):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "filter": {
          "synonym": {
            "type": "synonym",
            "synonyms": [
              "i-pod, i pod =&gt; ipod",
              "universe, cosmos"
            ]
          }
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>However, it is recommended to define large synonyms set in a file using
<code>synonyms_path</code>, because specifying them inline increases cluster size unnecessarily.</p>
</div>
<h4 id="_wordnet_synonyms" class="discrete">WordNet synonyms</h4>
<div class="paragraph">
<p>Synonyms based on <a href="https://wordnet.princeton.edu/">WordNet</a> format can be
declared using <code>format</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "filter": {
          "synonym": {
            "type": "synonym",
            "format": "wordnet",
            "synonyms": [
              "s(100000001,1,'abstain',v,1,0).",
              "s(100000001,2,'refrain',v,1,0).",
              "s(100000001,3,'desist',v,1,0)."
            ]
          }
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Using <code>synonyms_path</code> to define WordNet synonyms in a file is supported
as well.</p>
</div>
<h3 id="_parsing_synonym_files" class="discrete">Parsing synonym files</h3>
<div class="paragraph">
<p>OpenSearch will use the token filters preceding the synonym filter
in a tokenizer chain to parse the entries in a synonym file.  So, for example, if a
synonym filter is placed after a stemmer, then the stemmer will also be applied
to the synonym entries.  Because entries in the synonym map cannot have stacked
positions, some token filters may cause issues here.  Token filters that produce
multiple versions of a token may choose which version of the token to emit when
parsing synonyms, e.g. <code>asciifolding</code> will only produce the folded version of the
token.  Others, e.g. <code>multiplexer</code>, <code>word_delimiter_graph</code> or <code>ngram</code> will throw an
error.</p>
</div>
<div class="paragraph">
<p>If you need to build analyzers that include both multi-token filters and synonym
filters, consider using the <a href="analysis-tokenfilters.html#analysis-multiplexer-tokenfilter">multiplexer</a> filter,
with the multi-token filters in one branch and the synonym filter in the other.</p>
</div>
</div>
<div class="sect2">
<h3 id="analysis-synonym-graph-tokenfilter">Synonym graph token filter</h3>
<titleabbrev>Synonym graph</titleabbrev>
<div class="paragraph">
<p>The <code>synonym_graph</code> token filter allows to easily handle synonyms,
including multi-word synonyms correctly during the analysis process.</p>
</div>
<div class="paragraph">
<p>In order to properly handle multi-word synonyms this token filter
creates a <a href="analysis-concepts.html#token-graphs">graph token stream</a> during processing.  For more
information on this topic and its various complexities, please read the
<a href="http://blog.mikemccandless.com/2012/04/lucenes-tokenstreams-are-actually.html">Lucene&#8217;s TokenStreams are actually graphs</a> blog post.</p>
</div>
<div id="synonym-graph-index-note" class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>This token filter is designed to be used as part of a search analyzer
only.  If you want to apply synonyms during indexing please use the
standard <a href="analysis-tokenfilters.html#analysis-synonym-tokenfilter">synonym token filter</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Synonyms are configured using a configuration file.
Here is an example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "analyzer": {
          "search_synonyms": {
            "tokenizer": "whitespace",
            "filter": [ "graph_synonyms" ]
          }
        },
        "filter": {
          "graph_synonyms": {
            "type": "synonym_graph",
            "synonyms_path": "analysis/synonym.txt"
          }
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above configures a <code>search_synonyms</code> filter, with a path of
<code>analysis/synonym.txt</code> (relative to the <code>config</code> location). The
<code>search_synonyms</code> analyzer is then configured with the filter.</p>
</div>
<div class="paragraph">
<p>Additional settings are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>expand</code> (defaults to <code>true</code>).</p>
</li>
<li>
<p><code>lenient</code> (defaults to <code>false</code>). If <code>true</code> ignores exceptions while parsing the synonym configuration. It is important
to note that only those synonym rules which cannot get parsed are ignored. For instance consider the following request:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "analyzer": {
          "synonym": {
            "tokenizer": "standard",
            "filter": [ "my_stop", "synonym_graph" ]
          }
        },
        "filter": {
          "my_stop": {
            "type": "stop",
            "stopwords": [ "bar" ]
          },
          "synonym_graph": {
            "type": "synonym_graph",
            "lenient": true,
            "synonyms": [ "foo, bar =&gt; baz" ]
          }
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>With the above request the word <code>bar</code> gets skipped but a mapping <code>foo &#8658; baz</code> is still added. However, if the mapping
being added was <code>foo, baz &#8658; bar</code> nothing would get added to the synonym list. This is because the target word for the
mapping is itself eliminated because it was a stop word. Similarly, if the mapping was "bar, foo, baz" and <code>expand</code> was
set to <code>false</code> no mapping would get added as when <code>expand=false</code> the target mapping is the first word. However, if
<code>expand=true</code> then the mappings added would be equivalent to <code>foo, baz &#8658; foo, baz</code> i.e, all mappings other than the
stop word.</p>
</div>
<h4 id="synonym-graph-tokenizer-ignore_case-deprecated" class="discrete"><code>tokenizer</code> and <code>ignore_case</code> are deprecated</h4>
<div class="paragraph">
<p>The <code>tokenizer</code> parameter controls the tokenizers that will be used to
tokenize the synonym, this parameter is for backwards compatibility for indices that created before 6.0..
The <code>ignore_case</code> parameter works with <code>tokenizer</code> parameter only.</p>
</div>
<div class="paragraph">
<p>Two synonym formats are supported: Solr, WordNet.</p>
</div>
<h4 id="_solr_synonyms_2" class="discrete">Solr synonyms</h4>
<div class="paragraph">
<p>The following is a sample format of the file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-synonyms" data-lang="synonyms"># Blank lines and lines starting with pound are comments.

# Explicit mappings match any token sequence on the LHS of "=&gt;"
# and replace with all alternatives on the RHS.  These types of mappings
# ignore the expand parameter in the schema.
# Examples:
i-pod, i pod =&gt; ipod,
sea biscuit, sea biscit =&gt; seabiscuit

# Equivalent synonyms may be separated with commas and give
# no explicit mapping.  In this case the mapping behavior will
# be taken from the expand parameter in the schema.  This allows
# the same synonym file to be used in different synonym handling strategies.
# Examples:
ipod, i-pod, i pod
foozball , foosball
universe , cosmos
lol, laughing out loud

# If expand==true, "ipod, i-pod, i pod" is equivalent
# to the explicit mapping:
ipod, i-pod, i pod =&gt; ipod, i-pod, i pod
# If expand==false, "ipod, i-pod, i pod" is equivalent
# to the explicit mapping:
ipod, i-pod, i pod =&gt; ipod

# Multiple synonym mapping entries are merged.
foo =&gt; foo bar
foo =&gt; baz
# is equivalent to
foo =&gt; foo bar, baz</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can also define synonyms for the filter directly in the
configuration file (note use of <code>synonyms</code> instead of <code>synonyms_path</code>):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "filter": {
          "synonym": {
            "type": "synonym_graph",
            "synonyms": [
              "lol, laughing out loud",
              "universe, cosmos"
            ]
          }
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>However, it is recommended to define large synonyms set in a file using
<code>synonyms_path</code>, because specifying them inline increases cluster size unnecessarily.</p>
</div>
<h4 id="_wordnet_synonyms_2" class="discrete">WordNet synonyms</h4>
<div class="paragraph">
<p>Synonyms based on <a href="https://wordnet.princeton.edu/">WordNet</a> format can be
declared using <code>format</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "filter": {
          "synonym": {
            "type": "synonym_graph",
            "format": "wordnet",
            "synonyms": [
              "s(100000001,1,'abstain',v,1,0).",
              "s(100000001,2,'refrain',v,1,0).",
              "s(100000001,3,'desist',v,1,0)."
            ]
          }
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Using <code>synonyms_path</code> to define WordNet synonyms in a file is supported
as well.</p>
</div>
<h4 id="_parsing_synonym_files_2" class="discrete">Parsing synonym files</h4>
<div class="paragraph">
<p>OpenSearch will use the token filters preceding the synonym filter
in a tokenizer chain to parse the entries in a synonym file.  So, for example, if a
synonym filter is placed after a stemmer, then the stemmer will also be applied
to the synonym entries.  Because entries in the synonym map cannot have stacked
positions, some token filters may cause issues here.  Token filters that produce
multiple versions of a token may choose which version of the token to emit when
parsing synonyms, e.g. <code>asciifolding</code> will only produce the folded version of the
token.  Others, e.g. <code>multiplexer</code>, <code>word_delimiter_graph</code> or <code>ngram</code> will throw an
error.</p>
</div>
<div class="paragraph">
<p>If you need to build analyzers that include both multi-token filters and synonym
filters, consider using the <a href="analysis-tokenfilters.html#analysis-multiplexer-tokenfilter">multiplexer</a> filter,
with the multi-token filters in one branch and the synonym filter in the other.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
The synonym rules should not contain words that are removed by
a filter that appears after in the chain (a <code>stop</code> filter for instance).
Removing a term from a synonym rule breaks the matching at query time.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="analysis-trim-tokenfilter">Trim token filter</h3>
<titleabbrev>Trim</titleabbrev>
<div class="paragraph">
<p>Removes leading and trailing whitespace from each token in a stream. While this
can change the length of a token, the <code>trim</code> filter does <em>not</em> change a token&#8217;s
offsets.</p>
</div>
<div class="paragraph">
<p>The <code>trim</code> filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/TrimFilter.html">TrimFilter</a>.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>Many commonly used tokenizers, such as the
<a href="analysis-tokenizers.html#analysis-standard-tokenizer"><code>standard</code></a> or
<a href="analysis-tokenizers.html#analysis-whitespace-tokenizer"><code>whitespace</code></a> tokenizer, remove whitespace by
default. When using these tokenizers, you don&#8217;t need to add a separate <code>trim</code>
filter.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="analysis-trim-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>To see how the <code>trim</code> filter works, you first need to produce a token
containing whitespace.</p>
</div>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the
<a href="analysis-tokenizers.html#analysis-keyword-tokenizer"><code>keyword</code></a> tokenizer to produce a token for
<code>" fox "</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer" : "keyword",
  "text" : " fox "
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The API returns the following response. Note the <code>" fox "</code> token contains the
original text&#8217;s whitespace. Note that despite changing the token&#8217;s length, the
<code>start_offset</code> and <code>end_offset</code> remain the same.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console-result" data-lang="console-result">{
  "tokens": [
    {
      "token": " fox ",
      "start_offset": 0,
      "end_offset": 5,
      "type": "word",
      "position": 0
    }
  ]
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>To remove the whitespace, add the <code>trim</code> filter to the previous analyze API
request.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer" : "keyword",
  "filter" : ["trim"],
  "text" : " fox "
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The API returns the following response. The returned <code>fox</code> token does not
include any leading or trailing whitespace.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console-result" data-lang="console-result">{
  "tokens": [
    {
      "token": "fox",
      "start_offset": 0,
      "end_offset": 5,
      "type": "word",
      "position": 0
    }
  ]
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-trim-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the <code>trim</code>
filter to configure a new <a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT trim_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "keyword_trim": {
          "tokenizer": "keyword",
          "filter": [ "trim" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-truncate-tokenfilter">Truncate token filter</h3>
<titleabbrev>Truncate</titleabbrev>
<div class="paragraph">
<p>Truncates tokens that exceed a specified character limit. This limit defaults to
<code>10</code> but can be customized using the <code>length</code> parameter.</p>
</div>
<div class="paragraph">
<p>For example, you can use the <code>truncate</code> filter to shorten all tokens to
<code>3</code> characters or fewer, changing <code>jumping fox</code> to <code>jum fox</code>.</p>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/miscellaneous/TruncateTokenFilter.html[TruncateTokenFilter].</p>
</div>
<div class="sect3">
<h4 id="analysis-truncate-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>truncate</code> filter
to shorten tokens that exceed 10 characters in
<code>the quinquennial extravaganza carried on</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer" : "whitespace",
  "filter" : ["truncate"],
  "text" : "the quinquennial extravaganza carried on"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ the, quinquenni, extravagan, carried, on ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-truncate-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>truncate</code> filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT custom_truncate_example
{
  "settings" : {
    "analysis" : {
      "analyzer" : {
        "standard_truncate" : {
        "tokenizer" : "standard",
        "filter" : ["truncate"]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-truncate-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>length</code></dt>
<dd>
<p>(Optional, integer)
Character limit for each token. Tokens exceeding this limit are truncated.
Defaults to <code>10</code>.</p>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-truncate-tokenfilter-customize">Customize</h4>
<div class="paragraph">
<p>To customize the <code>truncate</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following request creates a custom <code>truncate</code> filter,
<code>5_char_trunc</code>, that shortens tokens to a <code>length</code> of <code>5</code> or fewer characters:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT 5_char_words_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "lowercase_5_char": {
          "tokenizer": "lowercase",
          "filter": [ "5_char_trunc" ]
        }
      },
      "filter": {
        "5_char_trunc": {
          "type": "truncate",
          "length": 5
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-unique-tokenfilter">Unique token filter</h3>
<titleabbrev>Unique</titleabbrev>
<div class="paragraph">
<p>Removes duplicate tokens from a stream. For example, you can use the <code>unique</code>
filter to change <code>the lazy lazy dog</code> to <code>the lazy dog</code>.</p>
</div>
<div class="paragraph">
<p>If the <code>only_on_same_position</code> parameter is set to <code>true</code>, the <code>unique</code> filter
removes only duplicate tokens <em>in the same position</em>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>When <code>only_on_same_position</code> is <code>true</code>, the <code>unique</code> filter works the same as
<a href="analysis-tokenfilters.html#analysis-remove-duplicates-tokenfilter"><code>remove_duplicates</code></a> filter.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="analysis-unique-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the <code>unique</code> filter
to remove duplicate tokens from <code>the quick fox jumps the lazy fox</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer" : "whitespace",
  "filter" : ["unique"],
  "text" : "the quick fox jumps the lazy fox"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter removes duplicated tokens for <code>the</code> and <code>fox</code>, producing the
following output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ the, quick, fox, jumps, lazy ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-unique-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>unique</code> filter to configure a new <a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT custom_unique_example
{
  "settings" : {
    "analysis" : {
      "analyzer" : {
        "standard_truncate" : {
        "tokenizer" : "standard",
        "filter" : ["unique"]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-unique-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>only_on_same_position</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, only remove duplicate tokens in the same position.
Defaults to <code>false</code>.</p>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-unique-tokenfilter-customize">Customize</h4>
<div class="paragraph">
<p>To customize the <code>unique</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following request creates a custom <code>unique</code> filter with
<code>only_on_same_position</code> set to <code>true</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT letter_unique_pos_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "letter_unique_pos": {
          "tokenizer": "letter",
          "filter": [ "unique_pos" ]
        }
      },
      "filter": {
        "unique_pos": {
          "type": "unique",
          "only_on_same_position": true
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-uppercase-tokenfilter">Uppercase token filter</h3>
<titleabbrev>Uppercase</titleabbrev>
<div class="paragraph">
<p>Changes token text to uppercase. For example, you can use the <code>uppercase</code> filter
to change <code>the Lazy DoG</code> to <code>THE LAZY DOG</code>.</p>
</div>
<div class="paragraph">
<p>This filter uses Lucene&#8217;s
{lucene-analysis-docs}/core/UpperCaseFilter.html[UpperCaseFilter].</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph">
<p>Depending on the language, an uppercase character can map to multiple
lowercase characters. Using the <code>uppercase</code> filter could result in the loss of
lowercase character information.</p>
</div>
<div class="paragraph">
<p>To avoid this loss but still have a consistent letter case, use the
<a href="analysis-tokenfilters.html#analysis-lowercase-tokenfilter"><code>lowercase</code></a> filter instead.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="analysis-uppercase-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the default
<code>uppercase</code> filter to change the <code>the Quick FoX JUMPs</code> to uppercase:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET _analyze
{
  "tokenizer" : "standard",
  "filter" : ["uppercase"],
  "text" : "the Quick FoX JUMPs"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ THE, QUICK, FOX, JUMPS ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-uppercase-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>uppercase</code> filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT uppercase_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_uppercase": {
          "tokenizer": "whitespace",
          "filter": [ "uppercase" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-word-delimiter-tokenfilter">Word delimiter token filter</h3>
<titleabbrev>Word delimiter</titleabbrev>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph">
<p>We recommend using the
<a href="analysis-tokenfilters.html#analysis-word-delimiter-graph-tokenfilter"><code>word_delimiter_graph</code></a> instead of
the <code>word_delimiter</code> filter.</p>
</div>
<div class="paragraph">
<p>The <code>word_delimiter</code> filter can produce invalid token graphs. See
<a href="analysis-tokenfilters.html#analysis-word-delimiter-graph-differences">Differences between <code>word_delimiter_graph</code> and <code>word_delimiter</code></a>.</p>
</div>
<div class="paragraph">
<p>The <code>word_delimiter</code> filter also uses Lucene&#8217;s
{lucene-analysis-docs}/miscellaneous/WordDelimiterFilter.html[WordDelimiterFilter],
which is marked as deprecated.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Splits tokens at non-alphanumeric characters. The <code>word_delimiter</code> filter
also performs optional token normalization based on a set of rules. By default,
the filter uses the following rules:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Split tokens at non-alphanumeric characters.
The filter uses these characters as delimiters.
For example: <code>Super-Duper</code> &#8594; <code>Super</code>, <code>Duper</code></p>
</li>
<li>
<p>Remove leading or trailing delimiters from each token.
For example: <code>XL---42+'Autocoder'</code> &#8594; <code>XL</code>, <code>42</code>, <code>Autocoder</code></p>
</li>
<li>
<p>Split tokens at letter case transitions.
For example: <code>PowerShot</code> &#8594; <code>Power</code>, <code>Shot</code></p>
</li>
<li>
<p>Split tokens at letter-number transitions.
For example: <code>XL500</code> &#8594; <code>XL</code>, <code>500</code></p>
</li>
<li>
<p>Remove the English possessive (<code>'s</code>) from the end of each token.
For example: <code>Neil&#8217;s</code> &#8594; <code>Neil</code></p>
</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>The <code>word_delimiter</code> filter was designed to remove punctuation from complex
identifiers, such as product IDs or part numbers. For these use cases, we
recommend using the <code>word_delimiter</code> filter with the
<a href="analysis-tokenizers.html#analysis-keyword-tokenizer"><code>keyword</code></a> tokenizer.</p>
</div>
<div class="paragraph">
<p>Avoid using the <code>word_delimiter</code> filter to split hyphenated words, such as
<code>wi-fi</code>. Because users often search for these words both with and without
hyphens, we recommend using the
<a href="analysis-tokenfilters.html#analysis-synonym-graph-tokenfilter"><code>synonym_graph</code></a> filter instead.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="analysis-word-delimiter-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the
<code>word_delimiter</code> filter to split <code>Neil&#8217;s-Super-Duper-XL500&#8212;&#8203;42+AutoCoder</code>
into normalized tokens using the filter&#8217;s default rules:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "keyword",
  "filter": [ "word_delimiter" ],
  "text": "Neil's-Super-Duper-XL500--42+AutoCoder"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-txt" data-lang="txt">[ Neil, Super, Duper, XL, 500, 42, Auto, Coder ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_add_to_an_analyzer">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>word_delimiter</code> filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "keyword",
          "filter": [ "word_delimiter" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph">
<p>Avoid using the <code>word_delimiter</code> filter with tokenizers that remove punctuation,
such as the <a href="analysis-tokenizers.html#analysis-standard-tokenizer"><code>standard</code></a> tokenizer. This could
prevent the <code>word_delimiter</code> filter from splitting tokens correctly. It can also
interfere with the filter&#8217;s configurable parameters, such as <code>catenate_all</code> or
<code>preserve_original</code>. We recommend using the
<a href="analysis-tokenizers.html#analysis-keyword-tokenizer"><code>keyword</code></a> or
<a href="analysis-tokenizers.html#analysis-whitespace-tokenizer"><code>whitespace</code></a> tokenizer instead.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="word-delimiter-tokenfilter-configure-parms">Configurable parameters</h4>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>catenate_all</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, Boolean)
If <code>true</code>, the filter produces catenated tokens for chains of alphanumeric
characters separated by non-alphabetic delimiters. For example:
<code>super-duper-xl-500</code> &#8594; [ <code>super</code>, <strong><code>superduperxl500</code></strong>, <code>duper</code>, <code>xl</code>, <code>500</code>
]. Defaults to <code>false</code>.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph">
<p>When used for search analysis, catenated tokens can cause problems for the
<a href="full-text-queries.html#query-dsl-match-query-phrase"><code>match_phrase</code></a> query and other queries that
rely on token position for matching. Avoid setting this parameter to <code>true</code> if
you plan to use these queries.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>catenate_numbers</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, Boolean)
If <code>true</code>, the filter produces catenated tokens for chains of numeric characters
separated by non-alphabetic delimiters. For example: <code>01-02-03</code> &#8594;
[ <code>01</code>, <strong><code>010203</code></strong>, <code>02</code>, <code>03</code> ]. Defaults to <code>false</code>.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph">
<p>When used for search analysis, catenated tokens can cause problems for the
<a href="full-text-queries.html#query-dsl-match-query-phrase"><code>match_phrase</code></a> query and other queries that
rely on token position for matching. Avoid setting this parameter to <code>true</code> if
you plan to use these queries.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>catenate_words</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, Boolean)
If <code>true</code>, the filter produces catenated tokens for chains of alphabetical
characters separated by non-alphabetic delimiters. For example: <code>super-duper-xl</code>
&#8594; [ <code>super</code>, <strong><code>superduperxl</code></strong>, <code>duper</code>, <code>xl</code> ]. Defaults to <code>false</code>.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph">
<p>When used for search analysis, catenated tokens can cause problems for the
<a href="full-text-queries.html#query-dsl-match-query-phrase"><code>match_phrase</code></a> query and other queries that
rely on token position for matching. Avoid setting this parameter to <code>true</code> if
you plan to use these queries.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>generate_number_parts</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, the filter includes tokens consisting of only numeric characters in
the output. If <code>false</code>, the filter excludes these tokens from the output.
Defaults to <code>true</code>.</p>
</dd>
<dt class="hdlist1"><code>generate_word_parts</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, the filter includes tokens consisting of only alphabetical characters
in the output. If <code>false</code>, the filter excludes these tokens from the output.
Defaults to <code>true</code>.</p>
</dd>
<dt class="hdlist1"><code>preserve_original</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, the filter includes the original version of any split tokens in the
output. This original version includes non-alphanumeric delimiters. For example:
<code>super-duper-xl-500</code> &#8594; [ <strong><code>super-duper-xl-500</code></strong>, <code>super</code>, <code>duper</code>, <code>xl</code>,
<code>500</code> ]. Defaults to <code>false</code>.</p>
</dd>
<dt class="hdlist1"><code>protected_words</code></dt>
<dd>
<p>(Optional, array of strings)
Array of tokens the filter won&#8217;t split.</p>
</dd>
<dt class="hdlist1"><code>protected_words_path</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, string)
Path to a file that contains a list of tokens the filter won&#8217;t split.</p>
</div>
<div class="paragraph">
<p>This path must be absolute or relative to the <code>config</code> location, and the file
must be UTF-8 encoded. Each token in the file must be separated by a line
break.</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>split_on_case_change</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, the filter splits tokens at letter case transitions. For example:
<code>camelCase</code> &#8594; [ <code>camel</code>, <code>Case</code> ]. Defaults to <code>true</code>.</p>
</dd>
<dt class="hdlist1"><code>split_on_numerics</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, the filter splits tokens at letter-number transitions. For example:
<code>j2se</code> &#8594; [ <code>j</code>, <code>2</code>, <code>se</code> ]. Defaults to <code>true</code>.</p>
</dd>
<dt class="hdlist1"><code>stem_english_possessive</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, the filter removes the English possessive (<code>'s</code>) from the end of each
token. For example: <code>O&#8217;Neil&#8217;s</code> &#8594; [ <code>O</code>, <code>Neil</code> ]. Defaults to <code>true</code>.</p>
</dd>
<dt class="hdlist1"><code>type_table</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, array of strings)
Array of custom type mappings for characters. This allows you to map
non-alphanumeric characters as numeric or alphanumeric to avoid splitting on
those characters.</p>
</div>
<div class="paragraph">
<p>For example, the following array maps the plus (<code>+</code>) and hyphen (<code>-</code>) characters
as alphanumeric, which means they won&#8217;t be treated as delimiters:</p>
</div>
<div class="paragraph">
<p><code>[ "+ &#8658; ALPHA", "- &#8658; ALPHA" ]</code></p>
</div>
<div class="paragraph">
<p>Supported types include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>ALPHA</code> (Alphabetical)</p>
</li>
<li>
<p><code>ALPHANUM</code> (Alphanumeric)</p>
</li>
<li>
<p><code>DIGIT</code> (Numeric)</p>
</li>
<li>
<p><code>LOWER</code> (Lowercase alphabetical)</p>
</li>
<li>
<p><code>SUBWORD_DELIM</code> (Non-alphanumeric delimiter)</p>
</li>
<li>
<p><code>UPPER</code> (Uppercase alphabetical)</p>
</li>
</ul>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>type_table_path</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, string)
Path to a file that contains custom type mappings for characters. This allows
you to map non-alphanumeric characters as numeric or alphanumeric to avoid
splitting on those characters.</p>
</div>
<div class="paragraph">
<p>For example, the contents of this file may contain the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-txt" data-lang="txt"># Map the $, %, '.', and ',' characters to DIGIT
# This might be useful for financial data.
$ =&gt; DIGIT
% =&gt; DIGIT
. =&gt; DIGIT
\\u002C =&gt; DIGIT

# in some cases you might not want to split on ZWJ
# this also tests the case where we need a bigger byte[]
# see https://en.wikipedia.org/wiki/Zero-width_joiner
\\u200D =&gt; ALPHANUM</code></pre>
</div>
</div>
<div class="paragraph">
<p>Supported types include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>ALPHA</code> (Alphabetical)</p>
</li>
<li>
<p><code>ALPHANUM</code> (Alphanumeric)</p>
</li>
<li>
<p><code>DIGIT</code> (Numeric)</p>
</li>
<li>
<p><code>LOWER</code> (Lowercase alphabetical)</p>
</li>
<li>
<p><code>SUBWORD_DELIM</code> (Non-alphanumeric delimiter)</p>
</li>
<li>
<p><code>UPPER</code> (Uppercase alphabetical)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This file path must be absolute or relative to the <code>config</code> location, and the
file must be UTF-8 encoded. Each mapping in the file must be separated by a line
break.</p>
</div>
</div>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-word-delimiter-tokenfilter-customize">Customize</h4>
<div class="paragraph">
<p>To customize the <code>word_delimiter</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following request creates a <code>word_delimiter</code>
filter that uses the following rules:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Split tokens at non-alphanumeric characters, <em>except</em> the hyphen (<code>-</code>)
character.</p>
</li>
<li>
<p>Remove leading or trailing delimiters from each token.</p>
</li>
<li>
<p>Do <em>not</em> split tokens at letter case transitions.</p>
</li>
<li>
<p>Do <em>not</em> split tokens at letter-number transitions.</p>
</li>
<li>
<p>Remove the English possessive (<code>'s</code>) from the end of each token.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "keyword",
          "filter": [ "my_custom_word_delimiter_filter" ]
        }
      },
      "filter": {
        "my_custom_word_delimiter_filter": {
          "type": "word_delimiter",
          "type_table": [ "- =&gt; ALPHA" ],
          "split_on_case_change": false,
          "split_on_numerics": false,
          "stem_english_possessive": true
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-word-delimiter-graph-tokenfilter">Word delimiter graph token filter</h3>
<titleabbrev>Word delimiter graph</titleabbrev>
<div class="paragraph">
<p>Splits tokens at non-alphanumeric characters. The <code>word_delimiter_graph</code> filter
also performs optional token normalization based on a set of rules. By default,
the filter uses the following rules:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Split tokens at non-alphanumeric characters.
The filter uses these characters as delimiters.
For example: <code>Super-Duper</code> &#8594; <code>Super</code>, <code>Duper</code></p>
</li>
<li>
<p>Remove leading or trailing delimiters from each token.
For example: <code>XL---42+'Autocoder'</code> &#8594; <code>XL</code>, <code>42</code>, <code>Autocoder</code></p>
</li>
<li>
<p>Split tokens at letter case transitions.
For example: <code>PowerShot</code> &#8594; <code>Power</code>, <code>Shot</code></p>
</li>
<li>
<p>Split tokens at letter-number transitions.
For example: <code>XL500</code> &#8594; <code>XL</code>, <code>500</code></p>
</li>
<li>
<p>Remove the English possessive (<code>'s</code>) from the end of each token.
For example: <code>Neil&#8217;s</code> &#8594; <code>Neil</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The <code>word_delimiter_graph</code> filter uses Lucene&#8217;s
{lucene-analysis-docs}/miscellaneous/WordDelimiterGraphFilter.html[WordDelimiterGraphFilter].</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>The <code>word_delimiter_graph</code> filter was designed to remove punctuation from
complex identifiers, such as product IDs or part numbers. For these use cases,
we recommend using the <code>word_delimiter_graph</code> filter with the
<a href="analysis-tokenizers.html#analysis-keyword-tokenizer"><code>keyword</code></a> tokenizer.</p>
</div>
<div class="paragraph">
<p>Avoid using the <code>word_delimiter_graph</code> filter to split hyphenated words, such as
<code>wi-fi</code>. Because users often search for these words both with and without
hyphens, we recommend using the
<a href="analysis-tokenfilters.html#analysis-synonym-graph-tokenfilter"><code>synonym_graph</code></a> filter instead.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="analysis-word-delimiter-graph-tokenfilter-analyze-ex">Example</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-analyze">analyze API</a> request uses the
<code>word_delimiter_graph</code> filter to split <code>Neil&#8217;s-Super-Duper-XL500&#8212;&#8203;42+AutoCoder</code>
into normalized tokens using the filter&#8217;s default rules:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET /_analyze
{
  "tokenizer": "keyword",
  "filter": [ "word_delimiter_graph" ],
  "text": "Neil's-Super-Duper-XL500--42+AutoCoder"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The filter produces the following tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-txt" data-lang="txt">[ Neil, Super, Duper, XL, 500, 42, Auto, Coder ]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-word-delimiter-graph-tokenfilter-analyzer-ex">Add to an analyzer</h4>
<div class="paragraph">
<p>The following <a href="indices.html#indices-create-index">create index API</a> request uses the
<code>word_delimiter_graph</code> filter to configure a new
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzer</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "keyword",
          "filter": [ "word_delimiter_graph" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph">
<p>Avoid using the <code>word_delimiter_graph</code> filter with tokenizers that remove
punctuation, such as the <a href="analysis-tokenizers.html#analysis-standard-tokenizer"><code>standard</code></a> tokenizer.
This could prevent the <code>word_delimiter_graph</code> filter from splitting tokens
correctly. It can also interfere with the filter&#8217;s configurable parameters, such
as <a href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-catenate-all"><code>catenate_all</code></a> or
<a href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-preserve-original"><code>preserve_original</code></a>. We
recommend using the <a href="analysis-tokenizers.html#analysis-keyword-tokenizer"><code>keyword</code></a> or
<a href="analysis-tokenizers.html#analysis-whitespace-tokenizer"><code>whitespace</code></a> tokenizer instead.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="word-delimiter-graph-tokenfilter-configure-parms">Configurable parameters</h4>
<div id="word-delimiter-graph-tokenfilter-adjust-offsets" class="dlist">
<dl>
<dt class="hdlist1"><code>adjust_offsets</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, Boolean)
If <code>true</code>, the filter adjusts the offsets of split or catenated tokens to better
reflect their actual position in the token stream. Defaults to <code>true</code>.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph">
<p>Set <code>adjust_offsets</code> to <code>false</code> if your analyzer uses filters, such as the
<a href="analysis-tokenfilters.html#analysis-trim-tokenfilter"><code>trim</code></a> filter, that change the length of tokens
without changing their offsets. Otherwise, the <code>word_delimiter_graph</code> filter
could produce tokens with illegal offsets.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</dd>
</dl>
</div>
<div id="word-delimiter-graph-tokenfilter-catenate-all" class="dlist">
<dl>
<dt class="hdlist1"><code>catenate_all</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, Boolean)
If <code>true</code>, the filter produces catenated tokens for chains of alphanumeric
characters separated by non-alphabetic delimiters. For example:
<code>super-duper-xl-500</code> &#8594; [ <strong><code>superduperxl500</code></strong>, <code>super</code>, <code>duper</code>, <code>xl</code>, <code>500</code> ].
Defaults to <code>false</code>.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph">
<p>Setting this parameter to <code>true</code> produces multi-position tokens, which are not
supported by indexing.</p>
</div>
<div class="paragraph">
<p>If this parameter is <code>true</code>, avoid using this filter in an index analyzer or
use the <a href="analysis-tokenfilters.html#analysis-flatten-graph-tokenfilter"><code>flatten_graph</code></a> filter after
this filter to make the token stream suitable for indexing.</p>
</div>
<div class="paragraph">
<p>When used for search analysis, catenated tokens can cause problems for the
<a href="full-text-queries.html#query-dsl-match-query-phrase"><code>match_phrase</code></a> query and other queries that
rely on token position for matching. Avoid setting this parameter to <code>true</code> if
you plan to use these queries.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</dd>
</dl>
</div>
<div id="word-delimiter-graph-tokenfilter-catenate-numbers" class="dlist">
<dl>
<dt class="hdlist1"><code>catenate_numbers</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, Boolean)
If <code>true</code>, the filter produces catenated tokens for chains of numeric characters
separated by non-alphabetic delimiters. For example: <code>01-02-03</code> &#8594;
[ <strong><code>010203</code></strong>, <code>01</code>, <code>02</code>, <code>03</code> ]. Defaults to <code>false</code>.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph">
<p>Setting this parameter to <code>true</code> produces multi-position tokens, which are not
supported by indexing.</p>
</div>
<div class="paragraph">
<p>If this parameter is <code>true</code>, avoid using this filter in an index analyzer or
use the <a href="analysis-tokenfilters.html#analysis-flatten-graph-tokenfilter"><code>flatten_graph</code></a> filter after
this filter to make the token stream suitable for indexing.</p>
</div>
<div class="paragraph">
<p>When used for search analysis, catenated tokens can cause problems for the
<a href="full-text-queries.html#query-dsl-match-query-phrase"><code>match_phrase</code></a> query and other queries that
rely on token position for matching. Avoid setting this parameter to <code>true</code> if
you plan to use these queries.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</dd>
</dl>
</div>
<div id="word-delimiter-graph-tokenfilter-catenate-words" class="dlist">
<dl>
<dt class="hdlist1"><code>catenate_words</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, Boolean)
If <code>true</code>, the filter produces catenated tokens for chains of alphabetical
characters separated by non-alphabetic delimiters. For example: <code>super-duper-xl</code>
&#8594; [ <strong><code>superduperxl</code></strong>, <code>super</code>, <code>duper</code>, <code>xl</code> ]. Defaults to <code>false</code>.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph">
<p>Setting this parameter to <code>true</code> produces multi-position tokens, which are not
supported by indexing.</p>
</div>
<div class="paragraph">
<p>If this parameter is <code>true</code>, avoid using this filter in an index analyzer or
use the <a href="analysis-tokenfilters.html#analysis-flatten-graph-tokenfilter"><code>flatten_graph</code></a> filter after
this filter to make the token stream suitable for indexing.</p>
</div>
<div class="paragraph">
<p>When used for search analysis, catenated tokens can cause problems for the
<a href="full-text-queries.html#query-dsl-match-query-phrase"><code>match_phrase</code></a> query and other queries that
rely on token position for matching. Avoid setting this parameter to <code>true</code> if
you plan to use these queries.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>generate_number_parts</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, the filter includes tokens consisting of only numeric characters in
the output. If <code>false</code>, the filter excludes these tokens from the output.
Defaults to <code>true</code>.</p>
</dd>
<dt class="hdlist1"><code>generate_word_parts</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, the filter includes tokens consisting of only alphabetical characters
in the output. If <code>false</code>, the filter excludes these tokens from the output.
Defaults to <code>true</code>.</p>
</dd>
<dt class="hdlist1"><code>ignore_keywords</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, the filter skips tokens with
a <code>keyword</code> attribute of <code>true</code>.
Defaults to <code>false</code>.</p>
</dd>
</dl>
</div>
<div id="word-delimiter-graph-tokenfilter-preserve-original" class="dlist">
<dl>
<dt class="hdlist1"><code>preserve_original</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, Boolean)
If <code>true</code>, the filter includes the original version of any split tokens in the
output. This original version includes non-alphanumeric delimiters. For example:
<code>super-duper-xl-500</code> &#8594; [ <strong><code>super-duper-xl-500</code></strong>, <code>super</code>, <code>duper</code>, <code>xl</code>,
<code>500</code> ]. Defaults to <code>false</code>.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph">
<p>Setting this parameter to <code>true</code> produces multi-position tokens, which are not
supported by indexing.</p>
</div>
<div class="paragraph">
<p>If this parameter is <code>true</code>, avoid using this filter in an index analyzer or
use the <a href="analysis-tokenfilters.html#analysis-flatten-graph-tokenfilter"><code>flatten_graph</code></a> filter after
this filter to make the token stream suitable for indexing.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>protected_words</code></dt>
<dd>
<p>(Optional, array of strings)
Array of tokens the filter won&#8217;t split.</p>
</dd>
<dt class="hdlist1"><code>protected_words_path</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, string)
Path to a file that contains a list of tokens the filter won&#8217;t split.</p>
</div>
<div class="paragraph">
<p>This path must be absolute or relative to the <code>config</code> location, and the file
must be UTF-8 encoded. Each token in the file must be separated by a line
break.</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>split_on_case_change</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, the filter splits tokens at letter case transitions. For example:
<code>camelCase</code> &#8594; [ <code>camel</code>, <code>Case</code> ]. Defaults to <code>true</code>.</p>
</dd>
<dt class="hdlist1"><code>split_on_numerics</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, the filter splits tokens at letter-number transitions. For example:
<code>j2se</code> &#8594; [ <code>j</code>, <code>2</code>, <code>se</code> ]. Defaults to <code>true</code>.</p>
</dd>
<dt class="hdlist1"><code>stem_english_possessive</code></dt>
<dd>
<p>(Optional, Boolean)
If <code>true</code>, the filter removes the English possessive (<code>'s</code>) from the end of each
token. For example: <code>O&#8217;Neil&#8217;s</code> &#8594; [ <code>O</code>, <code>Neil</code> ]. Defaults to <code>true</code>.</p>
</dd>
<dt class="hdlist1"><code>type_table</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, array of strings)
Array of custom type mappings for characters. This allows you to map
non-alphanumeric characters as numeric or alphanumeric to avoid splitting on
those characters.</p>
</div>
<div class="paragraph">
<p>For example, the following array maps the plus (<code>+</code>) and hyphen (<code>-</code>) characters
as alphanumeric, which means they won&#8217;t be treated as delimiters:</p>
</div>
<div class="paragraph">
<p><code>[ "+ &#8658; ALPHA", "- &#8658; ALPHA" ]</code></p>
</div>
<div class="paragraph">
<p>Supported types include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>ALPHA</code> (Alphabetical)</p>
</li>
<li>
<p><code>ALPHANUM</code> (Alphanumeric)</p>
</li>
<li>
<p><code>DIGIT</code> (Numeric)</p>
</li>
<li>
<p><code>LOWER</code> (Lowercase alphabetical)</p>
</li>
<li>
<p><code>SUBWORD_DELIM</code> (Non-alphanumeric delimiter)</p>
</li>
<li>
<p><code>UPPER</code> (Uppercase alphabetical)</p>
</li>
</ul>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>type_table_path</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>(Optional, string)
Path to a file that contains custom type mappings for characters. This allows
you to map non-alphanumeric characters as numeric or alphanumeric to avoid
splitting on those characters.</p>
</div>
<div class="paragraph">
<p>For example, the contents of this file may contain the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-txt" data-lang="txt"># Map the $, %, '.', and ',' characters to DIGIT
# This might be useful for financial data.
$ =&gt; DIGIT
% =&gt; DIGIT
. =&gt; DIGIT
\\u002C =&gt; DIGIT

# in some cases you might not want to split on ZWJ
# this also tests the case where we need a bigger byte[]
# see https://en.wikipedia.org/wiki/Zero-width_joiner
\\u200D =&gt; ALPHANUM</code></pre>
</div>
</div>
<div class="paragraph">
<p>Supported types include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>ALPHA</code> (Alphabetical)</p>
</li>
<li>
<p><code>ALPHANUM</code> (Alphanumeric)</p>
</li>
<li>
<p><code>DIGIT</code> (Numeric)</p>
</li>
<li>
<p><code>LOWER</code> (Lowercase alphabetical)</p>
</li>
<li>
<p><code>SUBWORD_DELIM</code> (Non-alphanumeric delimiter)</p>
</li>
<li>
<p><code>UPPER</code> (Uppercase alphabetical)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This file path must be absolute or relative to the <code>config</code> location, and the
file must be UTF-8 encoded. Each mapping in the file must be separated by a line
break.</p>
</div>
</div>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="analysis-word-delimiter-graph-tokenfilter-customize">Customize</h4>
<div class="paragraph">
<p>To customize the <code>word_delimiter_graph</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
</div>
<div class="paragraph">
<p>For example, the following request creates a <code>word_delimiter_graph</code>
filter that uses the following rules:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Split tokens at non-alphanumeric characters, <em>except</em> the hyphen (<code>-</code>)
character.</p>
</li>
<li>
<p>Remove leading or trailing delimiters from each token.</p>
</li>
<li>
<p>Do <em>not</em> split tokens at letter case transitions.</p>
</li>
<li>
<p>Do <em>not</em> split tokens at letter-number transitions.</p>
</li>
<li>
<p>Remove the English possessive (<code>'s</code>) from the end of each token.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "keyword",
          "filter": [ "my_custom_word_delimiter_graph_filter" ]
        }
      },
      "filter": {
        "my_custom_word_delimiter_graph_filter": {
          "type": "word_delimiter_graph",
          "type_table": [ "- =&gt; ALPHA" ],
          "split_on_case_change": false,
          "split_on_numerics": false,
          "stem_english_possessive": true
        }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="analysis-word-delimiter-graph-differences">Differences between <code>word_delimiter_graph</code> and <code>word_delimiter</code></h4>
<div class="paragraph">
<p>Both the <code>word_delimiter_graph</code> and
<a href="analysis-tokenfilters.html#analysis-word-delimiter-tokenfilter"><code>word_delimiter</code></a> filters produce tokens
that span multiple positions when any of the following parameters are <code>true</code>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-catenate-all"><code>catenate_all</code></a></p>
</li>
<li>
<p><a href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-catenate-numbers"><code>catenate_numbers</code></a></p>
</li>
<li>
<p><a href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-catenate-words"><code>catenate_words</code></a></p>
</li>
<li>
<p><a href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-preserve-original"><code>preserve_original</code></a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>However, only the <code>word_delimiter_graph</code> filter assigns multi-position tokens a
<code>positionLength</code> attribute, which indicates the number of positions a token
spans. This ensures the <code>word_delimiter_graph</code> filter always produces valid
<a href="analysis-concepts.html#token-graphs">token graphs</a>.</p>
</div>
<div class="paragraph">
<p>The <code>word_delimiter</code> filter does not assign multi-position tokens a
<code>positionLength</code> attribute. This means it produces invalid graphs for streams
including these tokens.</p>
</div>
<div class="paragraph">
<p>While indexing does not support token graphs containing multi-position tokens,
queries, such as the <a href="full-text-queries.html#query-dsl-match-query-phrase"><code>match_phrase</code></a> query, can
use these graphs to generate multiple sub-queries from a single query string.</p>
</div>
<div class="paragraph">
<p>To see how token graphs produced by the <code>word_delimiter</code> and
<code>word_delimiter_graph</code> filters differ, check out the following example.</p>
</div>
<details>
<summary class="title"><strong>Example</strong></summary>
<div class="content">
<div id="analysis-word-delimiter-graph-basic-token-graph" class="paragraph">
<p><strong>Basic token graph</strong></p>
</div>
<div class="paragraph">
<p>Both the <code>word_delimiter</code> and <code>word_delimiter_graph</code> produce the following token
graph for <code>PowerShot2000</code> when the following parameters are <code>false</code>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-catenate-all"><code>catenate_all</code></a></p>
</li>
<li>
<p><a href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-catenate-numbers"><code>catenate_numbers</code></a></p>
</li>
<li>
<p><a href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-catenate-words"><code>catenate_words</code></a></p>
</li>
<li>
<p><a href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-preserve-original"><code>preserve_original</code></a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This graph does not contain multi-position tokens. All tokens span only one
position.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="images/analysis/token-graph-basic.svg" alt="token graph basic">
</div>
</div>
<div id="analysis-word-delimiter-graph-wdg-token-graph" class="paragraph">
<p><strong><code>word_delimiter_graph</code> graph with a multi-position token</strong></p>
</div>
<div class="paragraph">
<p>The <code>word_delimiter_graph</code> filter produces the following token graph for
<code>PowerShot2000</code> when <code>catenate_words</code> is <code>true</code>.</p>
</div>
<div class="paragraph">
<p>This graph correctly indicates the catenated <code>PowerShot</code> token spans two
positions.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="images/analysis/token-graph-wdg.svg" alt="token graph wdg">
</div>
</div>
<div id="analysis-word-delimiter-graph-wd-token-graph" class="paragraph">
<p><strong><code>word_delimiter</code> graph with a multi-position token</strong></p>
</div>
<div class="paragraph">
<p>When <code>catenate_words</code> is <code>true</code>, the <code>word_delimiter</code> filter produces
the following token graph for <code>PowerShot2000</code>.</p>
</div>
<div class="paragraph">
<p>Note that the catenated <code>PowerShot</code> token should span two positions but only
spans one in the token graph, making it invalid.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="images/analysis/token-graph-wd.svg" alt="token graph wd">
</div>
</div>
</div>
</details>
</div>
</div>
</div>
</div>
<div class="paragraph nav-footer">
<p>← Previous: <a href="analysis-tokenizers.html">Tokenizer reference</a> | ↑ Up: <a href="analysis.html">Text analysis</a> | ⌂ Home: <a href="index.html">OpenSearch Reference</a> | Next: <a href="analysis-charfilters.html">Character filters reference</a> →</p>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2021-04-12 14:08:20 -0700
</div>
</div>
</body>
</html>