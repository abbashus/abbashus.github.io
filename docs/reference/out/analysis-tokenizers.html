<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.12">
<title>OpenSearch Reference</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/* Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment @import statement to use as custom stylesheet */
/*@import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700";*/
article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section{display:block}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
abbr[title]{border-bottom:1px dotted}
b,strong{font-weight:bold}
dfn{font-style:italic}
hr{-moz-box-sizing:content-box;box-sizing:content-box;height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,*::before,*::after{-moz-box-sizing:border-box;-webkit-box-sizing:border-box;box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;font-weight:400;font-style:normal;line-height:1;position:relative;cursor:auto;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{font-family:inherit;font-weight:400;font-size:1em;line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em;height:0}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{font-size:1em;line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0;font-size:1em}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
abbr,acronym{text-transform:uppercase;font-size:90%;color:rgba(0,0,0,.8);border-bottom:1px dotted #ddd;cursor:help}
abbr{text-transform:none}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote cite{display:block;font-size:.9375em;color:rgba(0,0,0,.6)}
blockquote cite::before{content:"\2014 \0020"}
blockquote cite a,blockquote cite a:visited{color:rgba(0,0,0,.6)}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:solid 1px #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;-webkit-border-radius:4px;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;-webkit-border-radius:3px;border-radius:3px;-webkit-box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em white inset;box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em #fff inset;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin-left:auto;margin-right:auto;margin-top:0;margin-bottom:0;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:-ms-flexbox;display:-webkit-flex;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:rgba(255,255,255,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details>summary:first-of-type{cursor:pointer;display:list-item;outline:none;margin-bottom:.75em}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class="paragraph"]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
table.tableblock #preamble>.sectionbody>[class="paragraph"]:first-of-type p{font-size:inherit}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border-style:solid;border-width:1px;border-color:#e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;-webkit-border-radius:4px;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border-style:solid;border-width:1px;border-color:#dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;-webkit-border-radius:4px;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{-webkit-border-radius:4px;border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class="highlight"],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;-webkit-border-radius:4px;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos{border-right:1px solid currentColor;opacity:.35;padding-right:.5em}
pre.pygments .lineno{border-right:1px solid currentColor;opacity:.35;display:inline-block;margin-right:.75em}
pre.pygments .lineno::before{content:"";margin-right:-.125em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all tr,table.stripes-odd tr:nth-of-type(odd),table.stripes-even tr:nth-of-type(even),table.stripes-hover tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
ol>li p,ul>li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
ul.checklist{margin-left:.625em}
ul.checklist li>p:first-child>.fa-square-o:first-child,ul.checklist li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist li>p:first-child>input[type="checkbox"]:first-child{margin-right:.25em}
ul.inline{display:-ms-flexbox;display:-webkit-box;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:solid 4px #fff;-webkit-box-shadow:0 0 0 1px #ddd;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
.gist .file-data>table{border:0;background:#fff;width:100%;margin-bottom:0}
.gist .file-data>table td.line-data{width:99%}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);-webkit-border-radius:50%;border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,span.alt{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;-webkit-box-shadow:0 1px 4px #e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{-webkit-box-shadow:none!important;box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media print,amzn-kf8{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<style>.toc-current{font-weight: bold;} .toc-root{font-family: "Open Sans","DejaVu Sans",sans-serif;
                       font-size: 0.9em;} #content{display: flex; flex-direction: column; flex: 1 1 auto;}
             .nav-footer{text-align: center; margin-top: auto;}
             .nav-footer > p > a {white-space: nowrap;}</style>
</head>
<body id="analysis-tokenizers" class="book toc2 toc-left">
<div id="header">
<h1>OpenSearch Reference</h1>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<p><span class="toc-root"><a href="index.html">OpenSearch Reference</a></span></p><ul class="sectlevel1">
<li><a href="opensearch-intro.html">What is OpenSearch?</a>
</li>
<li><a href="getting-started.html">Getting started with OpenSearch</a>
</li>
<li><a href="setup.html">Set up OpenSearch</a>
</li>
<li><a href="setup-upgrade.html">Upgrade OpenSearch</a>
</li>
<li><a href="index-modules.html">Index modules</a>
</li>
<li><a href="mapping.html">Mapping</a>
</li>
<li><a href="analysis.html">Text analysis</a>
<ul class="sectlevel1">
<li><a href="analysis-overview.html">Text analysis overview</a>
</li>
<li><a href="analysis-concepts.html">Text analysis concepts</a>
</li>
<li><a href="configure-text-analysis.html">Configure text analysis</a>
</li>
<li><a href="analysis-analyzers.html">Built-in analyzer reference</a>
</li>
<li><a href="analysis-tokenizers.html"><span class="toc-current">Tokenizer reference</span></a>
<ul class="sectlevel2">
<li><a href="analysis-tokenizers.html#analysis-chargroup-tokenizer">Character group tokenizer</a>
</li>
<li><a href="analysis-tokenizers.html#analysis-classic-tokenizer">Classic tokenizer</a>
</li>
<li><a href="analysis-tokenizers.html#analysis-edgengram-tokenizer">Edge n-gram tokenizer</a>
</li>
<li><a href="analysis-tokenizers.html#analysis-keyword-tokenizer">Keyword tokenizer</a>
</li>
<li><a href="analysis-tokenizers.html#analysis-letter-tokenizer">Letter tokenizer</a>
</li>
<li><a href="analysis-tokenizers.html#analysis-lowercase-tokenizer">Lowercase tokenizer</a>
</li>
<li><a href="analysis-tokenizers.html#analysis-ngram-tokenizer">N-gram tokenizer</a>
</li>
<li><a href="analysis-tokenizers.html#analysis-pathhierarchy-tokenizer">Path hierarchy tokenizer</a>
</li>
<li><a href="analysis-tokenizers.html#analysis-pattern-tokenizer">Pattern tokenizer</a>
</li>
<li><a href="analysis-tokenizers.html#analysis-simplepattern-tokenizer">Simple pattern tokenizer</a>
</li>
<li><a href="analysis-tokenizers.html#analysis-simplepatternsplit-tokenizer">Simple pattern split tokenizer</a>
</li>
<li><a href="analysis-tokenizers.html#analysis-standard-tokenizer">Standard tokenizer</a>
</li>
<li><a href="analysis-tokenizers.html#analysis-thai-tokenizer">Thai tokenizer</a>
</li>
<li><a href="analysis-tokenizers.html#analysis-uaxurlemail-tokenizer">UAX URL email tokenizer</a>
</li>
<li><a href="analysis-tokenizers.html#analysis-whitespace-tokenizer">Whitespace tokenizer</a>
</li>
</ul>
</li>
<li><a href="analysis-tokenfilters.html">Token filter reference</a>
</li>
<li><a href="analysis-charfilters.html">Character filters reference</a>
</li>
<li><a href="analysis-normalizers.html">Normalizers</a>
</li>
</ul>
</li>
<li><a href="index-templates.html">Index templates</a>
</li>
<li><a href="ingest.html">Ingest node</a>
</li>
<li><a href="search-your-data.html">Search your data</a>
</li>
<li><a href="query-dsl.html">Query DSL</a>
</li>
<li><a href="search-aggregations.html">Aggregations</a>
</li>
<li><a href="modules-scripting.html">Scripting</a>
</li>
<li><a href="high-availability.html">Set up a cluster for high availability</a>
</li>
<li><a href="snapshot-restore.html">Snapshot and restore</a>
</li>
<li><a href="commands.html">Command line tools</a>
</li>
<li><a href="how-to.html">How To</a>
</li>
<li><a href="glossary.html">Glossary of terms</a>
</li>
<li><a href="rest-apis.html">REST APIs</a>
</li>
<li><a href="breaking-changes.html">Migration guide</a>
</li>
<li><a href="opensearch-release-notes.html">Release notes</a>
</li>
</ul>
</div>
</div>
<div id="content">
<div class="sect1">
<h2 id="analysis-tokenizers">Tokenizer reference</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A <em>tokenizer</em>  receives a stream of characters, breaks it up into individual
<em>tokens</em> (usually individual words), and outputs a stream of <em>tokens</em>. For
instance, a <a href="analysis-tokenizers.html#analysis-whitespace-tokenizer"><code>whitespace</code></a> tokenizer breaks
text into tokens whenever it sees any whitespace.  It would convert the text
<code>"Quick brown fox!"</code> into the terms <code>[Quick, brown, fox!]</code>.</p>
</div>
<div class="paragraph">
<p>The tokenizer is also responsible for recording the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Order or <em>position</em> of each term (used for phrase and word proximity queries)</p>
</li>
<li>
<p>Start and end <em>character offsets</em> of the original word which the term
represents (used for highlighting search snippets).</p>
</li>
<li>
<p><em>Token type</em>, a classification of each term produced, such as <code>&lt;ALPHANUM&gt;</code>,
<code>&lt;HANGUL&gt;</code>, or <code>&lt;NUM&gt;</code>. Simpler analyzers only produce the <code>word</code> token type.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>OpenSearch has a number of built in tokenizers which can be used to build
<a href="configure-text-analysis.html#analysis-custom-analyzer">custom analyzers</a>.</p>
</div>
<h3 id="_word_oriented_tokenizers" class="discrete">Word Oriented Tokenizers</h3>
<div class="paragraph">
<p>The following tokenizers are usually used for tokenizing full text into
individual words:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><a href="analysis-tokenizers.html#analysis-standard-tokenizer">Standard Tokenizer</a></dt>
<dd>
<p>The <code>standard</code> tokenizer divides text into terms on word boundaries, as
defined by the Unicode Text Segmentation algorithm. It removes most
punctuation symbols. It is the best choice for most languages.</p>
</dd>
<dt class="hdlist1"><a href="analysis-tokenizers.html#analysis-letter-tokenizer">Letter Tokenizer</a></dt>
<dd>
<p>The <code>letter</code> tokenizer divides text into terms whenever it encounters a
character which is not a letter.</p>
</dd>
<dt class="hdlist1"><a href="analysis-tokenizers.html#analysis-lowercase-tokenizer">Lowercase Tokenizer</a></dt>
<dd>
<p>The <code>lowercase</code> tokenizer, like the <code>letter</code> tokenizer,  divides text into
terms whenever it encounters a character which is not a letter, but it also
lowercases all terms.</p>
</dd>
<dt class="hdlist1"><a href="analysis-tokenizers.html#analysis-whitespace-tokenizer">Whitespace Tokenizer</a></dt>
<dd>
<p>The <code>whitespace</code> tokenizer divides text into terms whenever it encounters any
whitespace character.</p>
</dd>
<dt class="hdlist1"><a href="analysis-tokenizers.html#analysis-uaxurlemail-tokenizer">UAX URL Email Tokenizer</a></dt>
<dd>
<p>The <code>uax_url_email</code> tokenizer is like the <code>standard</code> tokenizer except that it
recognises URLs and email addresses as single tokens.</p>
</dd>
<dt class="hdlist1"><a href="analysis-tokenizers.html#analysis-classic-tokenizer">Classic Tokenizer</a></dt>
<dd>
<p>The <code>classic</code> tokenizer is a grammar based tokenizer for the English Language.</p>
</dd>
<dt class="hdlist1"><a href="analysis-tokenizers.html#analysis-thai-tokenizer">Thai Tokenizer</a></dt>
<dd>
<p>The <code>thai</code> tokenizer segments Thai text into words.</p>
</dd>
</dl>
</div>
<h3 id="_partial_word_tokenizers" class="discrete">Partial Word Tokenizers</h3>
<div class="paragraph">
<p>These tokenizers break up text or words into small fragments, for partial word
matching:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><a href="analysis-tokenizers.html#analysis-ngram-tokenizer">N-Gram Tokenizer</a></dt>
<dd>
<p>The <code>ngram</code> tokenizer can break up text into words when it encounters any of
a list of specified characters (e.g. whitespace or punctuation), then it returns
n-grams of each word: a sliding window of continuous letters, e.g. <code>quick</code> &#8594;
<code>[qu, ui, ic, ck]</code>.</p>
</dd>
<dt class="hdlist1"><a href="analysis-tokenizers.html#analysis-edgengram-tokenizer">Edge N-Gram Tokenizer</a></dt>
<dd>
<p>The <code>edge_ngram</code> tokenizer can break up text into words when it encounters any of
a list of specified characters (e.g. whitespace or punctuation), then it returns
n-grams of each word which are anchored to the start of the word, e.g. <code>quick</code> &#8594;
<code>[q, qu, qui, quic, quick]</code>.</p>
</dd>
</dl>
</div>
<h3 id="_structured_text_tokenizers" class="discrete">Structured Text Tokenizers</h3>
<div class="paragraph">
<p>The following tokenizers are usually used with structured text like
identifiers, email addresses, zip codes, and paths, rather than with full
text:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><a href="analysis-tokenizers.html#analysis-keyword-tokenizer">Keyword Tokenizer</a></dt>
<dd>
<p>The <code>keyword</code> tokenizer is a <code>`noop'' tokenizer that accepts whatever text it
is given and outputs the exact same text as a single term.  It can be combined
with token filters like <a href="analysis-tokenfilters.html#analysis-lowercase-tokenfilter">`lowercase</code></a> to
normalise the analysed terms.</p>
</dd>
<dt class="hdlist1"><a href="analysis-tokenizers.html#analysis-pattern-tokenizer">Pattern Tokenizer</a></dt>
<dd>
<p>The <code>pattern</code> tokenizer uses a regular expression to either split text into
terms whenever it matches a word separator, or to capture matching text as
terms.</p>
</dd>
<dt class="hdlist1"><a href="analysis-tokenizers.html#analysis-simplepattern-tokenizer">Simple Pattern Tokenizer</a></dt>
<dd>
<p>The <code>simple_pattern</code> tokenizer uses a regular expression to capture matching
text as terms. It uses a restricted subset of regular expression features
and is generally faster than the <code>pattern</code> tokenizer.</p>
</dd>
<dt class="hdlist1"><a href="analysis-tokenizers.html#analysis-chargroup-tokenizer">Char Group Tokenizer</a></dt>
<dd>
<p>The <code>char_group</code> tokenizer is configurable through sets of characters to split
on, which is usually less expensive than running regular expressions.</p>
</dd>
<dt class="hdlist1"><a href="analysis-tokenizers.html#analysis-simplepatternsplit-tokenizer">Simple Pattern Split Tokenizer</a></dt>
<dd>
<p>The <code>simple_pattern_split</code> tokenizer uses the same restricted regular expression
subset as the <code>simple_pattern</code> tokenizer, but splits the input at matches rather
than returning the matches as terms.</p>
</dd>
<dt class="hdlist1"><a href="analysis-tokenizers.html#analysis-pathhierarchy-tokenizer">Path Tokenizer</a></dt>
<dd>
<p>The <code>path_hierarchy</code> tokenizer takes a hierarchical value like a filesystem
path, splits on the path separator, and emits a term for each component in the
tree, e.g. <code>/foo/bar/baz</code> &#8594; <code>[/foo, /foo/bar, /foo/bar/baz ]</code>.</p>
</dd>
</dl>
</div>
<div class="sect2">
<h3 id="analysis-chargroup-tokenizer">Character group tokenizer</h3>
<titleabbrev>Character group</titleabbrev>
<div class="paragraph">
<p>The <code>char_group</code> tokenizer breaks text into terms whenever it encounters a
character which is in a defined set. It is mostly useful for cases where a simple
custom tokenization is desired, and the overhead of use of the <a href="analysis-tokenizers.html#analysis-pattern-tokenizer"><code>pattern</code> tokenizer</a>
is not acceptable.</p>
</div>
<h3 id="_configuration_8" class="discrete">Configuration</h3>
<div class="paragraph">
<p>The <code>char_group</code> tokenizer accepts one parameter:</p>
</div>
<div class="hdlist">
<table>
<tr>
<td class="hdlist1">
<code>tokenize_on_chars</code>
</td>
<td class="hdlist2">
<p>A list containing a list of characters to tokenize the string on. Whenever a character
from this list is encountered, a new token is started. This accepts either single
characters like e.g. <code>-</code>, or character groups: <code>whitespace</code>, <code>letter</code>, <code>digit</code>,
<code>punctuation</code>, <code>symbol</code>.</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<code>max_token_length</code>
</td>
<td class="hdlist2">
<p>The maximum token length. If a token is seen that exceeds this length then
it is split at <code>max_token_length</code> intervals. Defaults to <code>255</code>.</p>
</td>
</tr>
</table>
</div>
<h3 id="_example_output_7" class="discrete">Example output</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">POST _analyze
{
  "tokenizer": {
    "type": "char_group",
    "tokenize_on_chars": [
      "whitespace",
      "-",
      "\n"
    ]
  },
  "text": "The QUICK brown-fox"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>returns</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console-result" data-lang="console-result">{
  "tokens": [
    {
      "token": "The",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0
    },
    {
      "token": "QUICK",
      "start_offset": 4,
      "end_offset": 9,
      "type": "word",
      "position": 1
    },
    {
      "token": "brown",
      "start_offset": 10,
      "end_offset": 15,
      "type": "word",
      "position": 2
    },
    {
      "token": "fox",
      "start_offset": 16,
      "end_offset": 19,
      "type": "word",
      "position": 3
    }
  ]
}</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-classic-tokenizer">Classic tokenizer</h3>
<titleabbrev>Classic</titleabbrev>
<div class="paragraph">
<p>The <code>classic</code> tokenizer is a grammar based tokenizer that is good for English
language documents. This tokenizer has heuristics for special treatment of
acronyms, company names, email addresses, and internet host names. However,
these rules don&#8217;t always work, and the tokenizer doesn&#8217;t work well for most
languages other than English:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>It splits words at most punctuation characters, removing punctuation. However, a
dot that&#8217;s not followed by whitespace is considered part of a token.</p>
</li>
<li>
<p>It splits words at hyphens, unless there&#8217;s a number in the token, in which case
the whole token is interpreted as a product number and is not split.</p>
</li>
<li>
<p>It recognizes email addresses and internet hostnames as one token.</p>
</li>
</ul>
</div>
<h3 id="_example_output_8" class="discrete">Example output</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">POST _analyze
{
  "tokenizer": "classic",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above sentence would produce the following terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ The, 2, QUICK, Brown, Foxes, jumped, over, the, lazy, dog's, bone ]</code></pre>
</div>
</div>
<h3 id="_configuration_9" class="discrete">Configuration</h3>
<div class="paragraph">
<p>The <code>classic</code> tokenizer accepts the following parameters:</p>
</div>
<div class="hdlist">
<table>
<tr>
<td class="hdlist1">
<code>max_token_length</code>
</td>
<td class="hdlist2">
<p>The maximum token length. If a token is seen that exceeds this length then
it is split at <code>max_token_length</code> intervals. Defaults to <code>255</code>.</p>
</td>
</tr>
</table>
</div>
<h3 id="_example_configuration_6" class="discrete">Example configuration</h3>
<div class="paragraph">
<p>In this example, we configure the <code>classic</code> tokenizer to have a
<code>max_token_length</code> of 5 (for demonstration purposes):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "classic",
          "max_token_length": 5
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above example produces the following terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ The, 2, QUICK, Brown, Foxes, jumpe, d, over, the, lazy, dog's, bone ]</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-edgengram-tokenizer">Edge n-gram tokenizer</h3>
<titleabbrev>Edge n-gram</titleabbrev>
<div class="paragraph">
<p>The <code>edge_ngram</code> tokenizer first breaks text down into words whenever it
encounters one of a list of specified characters, then it emits
<a href="https://en.wikipedia.org/wiki/N-gram">N-grams</a> of each word where the start of
the N-gram is anchored to the beginning of the word.</p>
</div>
<div class="paragraph">
<p>Edge N-Grams are useful for <em>search-as-you-type</em> queries.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
When you need <em>search-as-you-type</em> for text which has a widely known
order, such as movie or song titles, the
<a href="search.html#completion-suggester">completion suggester</a> is a much more efficient
choice than edge N-grams.  Edge N-grams have the advantage when trying to
autocomplete words that can appear in any order.
</td>
</tr>
</table>
</div>
<h3 id="_example_output_9" class="discrete">Example output</h3>
<div class="paragraph">
<p>With the default settings, the <code>edge_ngram</code> tokenizer treats the initial text as a
single token and produces N-grams with minimum length <code>1</code> and maximum length
<code>2</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">POST _analyze
{
  "tokenizer": "edge_ngram",
  "text": "Quick Fox"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above sentence would produce the following terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ Q, Qu ]</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
These default gram lengths are almost entirely useless.  You need to
configure the <code>edge_ngram</code> before using it.
</td>
</tr>
</table>
</div>
<h3 id="_configuration_10" class="discrete">Configuration</h3>
<div class="paragraph">
<p>The <code>edge_ngram</code> tokenizer accepts the following parameters:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>min_gram</code></dt>
<dd>
<p>Minimum length of characters in a gram.  Defaults to <code>1</code>.</p>
</dd>
<dt class="hdlist1"><code>max_gram</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>Maximum length of characters in a gram.  Defaults to <code>2</code>.</p>
</div>
<div class="paragraph">
<p>See <a href="analysis-tokenizers.html#max-gram-limits">Limitations of the <code>max_gram</code> parameter</a>.</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>token_chars</code></dt>
<dd>
<p>Character classes that should be included in a token.  OpenSearch
will split on characters that don&#8217;t belong to the classes specified.
Defaults to <code>[]</code> (keep all characters).</p>
<div class="paragraph">
<p>Character classes may be any of the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>letter</code>&#8201;&#8212;&#8201;     for example <code>a</code>, <code>b</code>, <code></code> or <code></code></p>
</li>
<li>
<p><code>digit</code>&#8201;&#8212;&#8201;      for example <code>3</code> or <code>7</code></p>
</li>
<li>
<p><code>whitespace</code>&#8201;&#8212;&#8201; for example <code>" "</code> or <code>"\n"</code></p>
</li>
<li>
<p><code>punctuation</code>&#8201;&#8212;&#8201;for example <code>!</code> or <code>"</code></p>
</li>
<li>
<p><code>symbol</code>&#8201;&#8212;&#8201;     for example <code>$</code> or <code></code></p>
</li>
<li>
<p><code>custom</code>&#8201;&#8212;&#8201;     custom characters which need to be set using the
<code>custom_token_chars</code> setting.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1"><code>custom_token_chars</code></dt>
<dd>
<p>Custom characters that should be treated as part of a token. For example,
setting this to <code>+-_</code> will make the tokenizer treat the plus, minus and
underscore sign  as part of a token.</p>
</dd>
</dl>
</div>
<h3 id="max-gram-limits" class="discrete">Limitations of the <code>max_gram</code> parameter</h3>
<div class="paragraph">
<p>The <code>edge_ngram</code> tokenizer&#8217;s <code>max_gram</code> value limits the character length of
tokens. When the <code>edge_ngram</code> tokenizer is used with an index analyzer, this
means search terms longer than the <code>max_gram</code> length may not match any indexed
terms.</p>
</div>
<div class="paragraph">
<p>For example, if the <code>max_gram</code> is <code>3</code>, searches for <code>apple</code> won&#8217;t match the
indexed term <code>app</code>.</p>
</div>
<div class="paragraph">
<p>To account for this, you can use the
<a href="analysis-tokenfilters.html#analysis-truncate-tokenfilter"><code>truncate</code></a> token filter with a search analyzer
to shorten search terms to the <code>max_gram</code> character length. However, this could
return irrelevant results.</p>
</div>
<div class="paragraph">
<p>For example, if the <code>max_gram</code> is <code>3</code> and search terms are truncated to three
characters, the search term <code>apple</code> is shortened to <code>app</code>. This means searches
for <code>apple</code> return any indexed terms matching <code>app</code>, such as <code>apply</code>, <code>snapped</code>,
and <code>apple</code>.</p>
</div>
<div class="paragraph">
<p>We recommend testing both approaches to see which best fits your
use case and desired search experience.</p>
</div>
<h3 id="_example_configuration_7" class="discrete">Example configuration</h3>
<div class="paragraph">
<p>In this example, we configure the <code>edge_ngram</code> tokenizer to treat letters and
digits as tokens, and to produce grams with minimum length <code>2</code> and maximum
length <code>10</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT my-index-00001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "edge_ngram",
          "min_gram": 2,
          "max_gram": 10,
          "token_chars": [
            "letter",
            "digit"
          ]
        }
      }
    }
  }
}

POST my-index-00001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "2 Quick Foxes."
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above example produces the following terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ Qu, Qui, Quic, Quick, Fo, Fox, Foxe, Foxes ]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Usually we recommend using the same <code>analyzer</code> at index time and at search
time. In the case of the <code>edge_ngram</code> tokenizer, the advice is different. It
only makes sense to use the <code>edge_ngram</code> tokenizer at index time, to ensure
that partial words are available for matching in the index. At search time,
just search for the terms the user has typed in, for instance: <code>Quick Fo</code>.</p>
</div>
<div class="paragraph">
<p>Below is an example of how to set up a field for <em>search-as-you-type</em>.</p>
</div>
<div class="paragraph">
<p>Note that the <code>max_gram</code> value for the index analyzer is <code>10</code>, which limits
indexed terms to 10 characters. Search terms are not truncated, meaning that
search terms longer than 10 characters may not match any indexed terms.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT my-index-00001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "autocomplete": {
          "tokenizer": "autocomplete",
          "filter": [
            "lowercase"
          ]
        },
        "autocomplete_search": {
          "tokenizer": "lowercase"
        }
      },
      "tokenizer": {
        "autocomplete": {
          "type": "edge_ngram",
          "min_gram": 2,
          "max_gram": 10,
          "token_chars": [
            "letter"
          ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "title": {
        "type": "text",
        "analyzer": "autocomplete",
        "search_analyzer": "autocomplete_search"
      }
    }
  }
}

PUT my-index-00001/_doc/1
{
  "title": "Quick Foxes" <b class="conum">(1)</b>
}

POST my-index-00001/_refresh

GET my-index-00001/_search
{
  "query": {
    "match": {
      "title": {
        "query": "Quick Fo", <b class="conum">(2)</b>
        "operator": "and"
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>The <code>autocomplete</code> analyzer indexes the terms <code>[qu, qui, quic, quick, fo, fox, foxe, foxes]</code>.</p>
</li>
<li>
<p>The <code>autocomplete_search</code> analyzer searches for the terms <code>[quick, fo]</code>, both of which appear in the index.</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="analysis-keyword-tokenizer">Keyword tokenizer</h3>
<titleabbrev>Keyword</titleabbrev>
<div class="paragraph">
<p>The <code>keyword</code> tokenizer  is a ``noop'' tokenizer that accepts whatever text it
is given and outputs the exact same text as a single term.  It can be combined
with token filters to normalise output, e.g. lower-casing email addresses.</p>
</div>
<h3 id="_example_output_10" class="discrete">Example output</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">POST _analyze
{
  "tokenizer": "keyword",
  "text": "New York"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above sentence would produce the following term:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ New York ]</code></pre>
</div>
</div>
<h3 id="analysis-keyword-tokenizer-token-filters" class="discrete">Combine with token filters</h3>
<div class="paragraph">
<p>You can combine the <code>keyword</code> tokenizer with token filters to normalise
structured data, such as product IDs or email addresses.</p>
</div>
<div class="paragraph">
<p>For example, the following <a href="indices.html#indices-analyze">analyze API</a> request uses the
<code>keyword</code> tokenizer and <a href="analysis-tokenfilters.html#analysis-lowercase-tokenfilter"><code>lowercase</code></a> filter to
convert an email address to lowercase.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">POST _analyze
{
  "tokenizer": "keyword",
  "filter": [ "lowercase" ],
  "text": "john.SMITH@example.COM"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The request produces the following token:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ john.smith@example.com ]</code></pre>
</div>
</div>
<h3 id="_configuration_11" class="discrete">Configuration</h3>
<div class="paragraph">
<p>The <code>keyword</code> tokenizer accepts the following parameters:</p>
</div>
<div class="hdlist">
<table>
<tr>
<td class="hdlist1">
<code>buffer_size</code>
</td>
<td class="hdlist2">
<p>The number of characters read into the term buffer in a single pass.
Defaults to <code>256</code>.  The term buffer will grow by this size until all the
text has been consumed.  It is advisable not to change this setting.</p>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="analysis-letter-tokenizer">Letter tokenizer</h3>
<titleabbrev>Letter</titleabbrev>
<div class="paragraph">
<p>The <code>letter</code> tokenizer breaks text into terms whenever it encounters a
character which is not a letter. It does a reasonable job for most European
languages, but does a terrible job for some Asian languages, where words are
not separated by spaces.</p>
</div>
<h3 id="_example_output_11" class="discrete">Example output</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">POST _analyze
{
  "tokenizer": "letter",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above sentence would produce the following terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ The, QUICK, Brown, Foxes, jumped, over, the, lazy, dog, s, bone ]</code></pre>
</div>
</div>
<h3 id="_configuration_12" class="discrete">Configuration</h3>
<div class="paragraph">
<p>The <code>letter</code> tokenizer is not configurable.</p>
</div>
</div>
<div class="sect2">
<h3 id="analysis-lowercase-tokenizer">Lowercase tokenizer</h3>
<titleabbrev>Lowercase</titleabbrev>
<div class="paragraph">
<p>The <code>lowercase</code> tokenizer, like the
<a href="analysis-tokenizers.html#analysis-letter-tokenizer"><code>letter</code> tokenizer</a> breaks text into terms
whenever it encounters a character which is not a letter, but it also
lowercases all terms.  It is functionally equivalent to the
<a href="analysis-tokenizers.html#analysis-letter-tokenizer"><code>letter</code> tokenizer</a> combined with the
<a href="analysis-tokenfilters.html#analysis-lowercase-tokenfilter"><code>lowercase</code> token filter</a>, but is more
efficient as it performs both steps in a single pass.</p>
</div>
<h3 id="_example_output_12" class="discrete">Example output</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">POST _analyze
{
  "tokenizer": "lowercase",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above sentence would produce the following terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ the, quick, brown, foxes, jumped, over, the, lazy, dog, s, bone ]</code></pre>
</div>
</div>
<h3 id="_configuration_13" class="discrete">Configuration</h3>
<div class="paragraph">
<p>The <code>lowercase</code> tokenizer is not configurable.</p>
</div>
</div>
<div class="sect2">
<h3 id="analysis-ngram-tokenizer">N-gram tokenizer</h3>
<titleabbrev>N-gram</titleabbrev>
<div class="paragraph">
<p>The <code>ngram</code> tokenizer first breaks text down into words whenever it encounters
one of a list of specified characters, then it emits
<a href="https://en.wikipedia.org/wiki/N-gram">N-grams</a> of each word of the specified
length.</p>
</div>
<div class="paragraph">
<p>N-grams are like a sliding window that moves across the word - a continuous
sequence of characters of the specified length. They are useful for querying
languages that don&#8217;t use spaces or that have long compound words, like German.</p>
</div>
<h3 id="_example_output_13" class="discrete">Example output</h3>
<div class="paragraph">
<p>With the default settings, the <code>ngram</code> tokenizer treats the initial text as a
single token and produces N-grams with minimum length <code>1</code> and maximum length
<code>2</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">POST _analyze
{
  "tokenizer": "ngram",
  "text": "Quick Fox"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above sentence would produce the following terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ Q, Qu, u, ui, i, ic, c, ck, k, "k ", " ", " F", F, Fo, o, ox, x ]</code></pre>
</div>
</div>
<h3 id="_configuration_14" class="discrete">Configuration</h3>
<div class="paragraph">
<p>The <code>ngram</code> tokenizer accepts the following parameters:</p>
</div>
<div class="hdlist">
<table>
<tr>
<td class="hdlist1">
<code>min_gram</code>
</td>
<td class="hdlist2">
<p>Minimum length of characters in a gram.  Defaults to <code>1</code>.</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<code>max_gram</code>
</td>
<td class="hdlist2">
<p>Maximum length of characters in a gram.  Defaults to <code>2</code>.</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<code>token_chars</code>
</td>
<td class="hdlist2">
<p>Character classes that should be included in a token. OpenSearch
will split on characters that don&#8217;t belong to the classes specified.
Defaults to <code>[]</code> (keep all characters).</p>
<div class="paragraph">
<p>Character classes may be any of the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>letter</code>&#8201;&#8212;&#8201;     for example <code>a</code>, <code>b</code>, <code></code> or <code></code></p>
</li>
<li>
<p><code>digit</code>&#8201;&#8212;&#8201;      for example <code>3</code> or <code>7</code></p>
</li>
<li>
<p><code>whitespace</code>&#8201;&#8212;&#8201; for example <code>" "</code> or <code>"\n"</code></p>
</li>
<li>
<p><code>punctuation</code>&#8201;&#8212;&#8201;for example <code>!</code> or <code>"</code></p>
</li>
<li>
<p><code>symbol</code>&#8201;&#8212;&#8201;     for example <code>$</code> or <code></code></p>
</li>
<li>
<p><code>custom</code>&#8201;&#8212;&#8201;     custom characters which need to be set using the
<code>custom_token_chars</code> setting.</p>
</li>
</ul>
</div>
</td>
</tr>
<tr>
<td class="hdlist1">
<code>custom_token_chars</code>
</td>
<td class="hdlist2">
<p>Custom characters that should be treated as part of a token. For example,
setting this to <code>+-_</code> will make the tokenizer treat the plus, minus and
underscore sign  as part of a token.</p>
</td>
</tr>
</table>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
It usually makes sense to set <code>min_gram</code> and <code>max_gram</code> to the same
value.  The smaller the length, the more documents will match but the lower
the quality of the matches.  The longer the length, the more specific the
matches.  A tri-gram (length <code>3</code>) is a good place to start.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The index level setting <code>index.max_ngram_diff</code> controls the maximum allowed
difference between <code>max_gram</code> and <code>min_gram</code>.</p>
</div>
<h3 id="_example_configuration_8" class="discrete">Example configuration</h3>
<div class="paragraph">
<p>In this example, we configure the <code>ngram</code> tokenizer to treat letters and
digits as tokens, and to produce tri-grams (grams of length <code>3</code>):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "ngram",
          "min_gram": 3,
          "max_gram": 3,
          "token_chars": [
            "letter",
            "digit"
          ]
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "2 Quick Foxes."
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above example produces the following terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ Qui, uic, ick, Fox, oxe, xes ]</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-pathhierarchy-tokenizer">Path hierarchy tokenizer</h3>
<titleabbrev>Path hierarchy</titleabbrev>
<div class="paragraph">
<p>The <code>path_hierarchy</code> tokenizer takes a hierarchical value like a filesystem
path, splits on the path separator, and emits a term for each component in the
tree.</p>
</div>
<h3 id="_example_output_14" class="discrete">Example output</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">POST _analyze
{
  "tokenizer": "path_hierarchy",
  "text": "/one/two/three"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above text would produce the following terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ /one, /one/two, /one/two/three ]</code></pre>
</div>
</div>
<h3 id="_configuration_15" class="discrete">Configuration</h3>
<div class="paragraph">
<p>The <code>path_hierarchy</code> tokenizer accepts the following parameters:</p>
</div>
<div class="hdlist">
<table>
<tr>
<td class="hdlist1">
<code>delimiter</code>
</td>
<td class="hdlist2">
<p>The character to use as the path separator.  Defaults to <code>/</code>.</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<code>replacement</code>
</td>
<td class="hdlist2">
<p>An optional replacement character to use for the delimiter.
Defaults to the <code>delimiter</code>.</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<code>buffer_size</code>
</td>
<td class="hdlist2">
<p>The number of characters read into the term buffer in a single pass.
Defaults to <code>1024</code>.  The term buffer will grow by this size until all the
text has been consumed.  It is advisable not to change this setting.</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<code>reverse</code>
</td>
<td class="hdlist2">
<p>If set to <code>true</code>, emits the tokens in reverse order.  Defaults to <code>false</code>.</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<code>skip</code>
</td>
<td class="hdlist2">
<p>The number of initial tokens to skip.  Defaults to <code>0</code>.</p>
</td>
</tr>
</table>
</div>
<h3 id="_example_configuration_9" class="discrete">Example configuration</h3>
<div class="paragraph">
<p>In this example, we configure the <code>path_hierarchy</code> tokenizer to split on <code>-</code>
characters, and to replace them with <code>/</code>.  The first two tokens are skipped:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "path_hierarchy",
          "delimiter": "-",
          "replacement": "/",
          "skip": 2
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "one-two-three-four-five"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above example produces the following terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ /three, /three/four, /three/four/five ]</code></pre>
</div>
</div>
<div class="paragraph">
<p>If we were to set <code>reverse</code> to <code>true</code>, it would produce the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ one/two/three/, two/three/, three/ ]</code></pre>
</div>
</div>
<h3 id="analysis-pathhierarchy-tokenizer-detailed-examples" class="discrete">Detailed examples</h3>
<div class="paragraph">
<p>A common use-case for the <code>path_hierarchy</code> tokenizer is filtering results by
file paths. If indexing a file path along with the data, the use of the
<code>path_hierarchy</code> tokenizer to analyze the path allows filtering the results
by different parts of the file path string.</p>
</div>
<div class="paragraph">
<p>This example configures an index to have two custom analyzers and applies
those analyzers to multifields of the <code>file_path</code> text field that will
store filenames. One of the two analyzers uses reverse tokenization.
Some sample documents are then indexed to represent some file paths
for photos inside photo folders of two different users.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT file-path-test
{
  "settings": {
    "analysis": {
      "analyzer": {
        "custom_path_tree": {
          "tokenizer": "custom_hierarchy"
        },
        "custom_path_tree_reversed": {
          "tokenizer": "custom_hierarchy_reversed"
        }
      },
      "tokenizer": {
        "custom_hierarchy": {
          "type": "path_hierarchy",
          "delimiter": "/"
        },
        "custom_hierarchy_reversed": {
          "type": "path_hierarchy",
          "delimiter": "/",
          "reverse": "true"
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "file_path": {
        "type": "text",
        "fields": {
          "tree": {
            "type": "text",
            "analyzer": "custom_path_tree"
          },
          "tree_reversed": {
            "type": "text",
            "analyzer": "custom_path_tree_reversed"
          }
        }
      }
    }
  }
}

POST file-path-test/_doc/1
{
  "file_path": "/User/alice/photos/2017/05/16/my_photo1.jpg"
}

POST file-path-test/_doc/2
{
  "file_path": "/User/alice/photos/2017/05/16/my_photo2.jpg"
}

POST file-path-test/_doc/3
{
  "file_path": "/User/alice/photos/2017/05/16/my_photo3.jpg"
}

POST file-path-test/_doc/4
{
  "file_path": "/User/alice/photos/2017/05/15/my_photo1.jpg"
}

POST file-path-test/_doc/5
{
  "file_path": "/User/bob/photos/2017/05/16/my_photo1.jpg"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>A search for a particular file path string against the text field matches all
the example documents, with Bob&#8217;s documents ranking highest due to <code>bob</code> also
being one of the terms created by the standard analyzer boosting relevance for
Bob&#8217;s documents.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET file-path-test/_search
{
  "query": {
    "match": {
      "file_path": "/User/bob/photos/2017/05"
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>It&#8217;s simple to match or filter documents with file paths that exist within a
particular directory using the <code>file_path.tree</code> field.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET file-path-test/_search
{
  "query": {
    "term": {
      "file_path.tree": "/User/alice/photos/2017/05/16"
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>With the reverse parameter for this tokenizer, it&#8217;s also possible to match
from the other end of the file path, such as individual file names or a deep
level subdirectory. The following example shows a search for all files named
<code>my_photo1.jpg</code> within any directory via the <code>file_path.tree_reversed</code> field
configured to use the reverse parameter in the mapping.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET file-path-test/_search
{
  "query": {
    "term": {
      "file_path.tree_reversed": {
        "value": "my_photo1.jpg"
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Viewing the tokens generated with both forward and reverse is instructive
in showing the tokens created for the same file path value.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">POST file-path-test/_analyze
{
  "analyzer": "custom_path_tree",
  "text": "/User/alice/photos/2017/05/16/my_photo1.jpg"
}

POST file-path-test/_analyze
{
  "analyzer": "custom_path_tree_reversed",
  "text": "/User/alice/photos/2017/05/16/my_photo1.jpg"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>It&#8217;s also useful to be able to filter with file paths when combined with other
types of searches, such as this example looking for any files paths with <code>16</code>
that also must be in Alice&#8217;s photo directory.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">GET file-path-test/_search
{
  "query": {
    "bool" : {
      "must" : {
        "match" : { "file_path" : "16" }
      },
      "filter": {
        "term" : { "file_path.tree" : "/User/alice" }
      }
    }
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-pattern-tokenizer">Pattern tokenizer</h3>
<titleabbrev>Pattern</titleabbrev>
<div class="paragraph">
<p>The <code>pattern</code> tokenizer uses a regular expression to either split text into
terms whenever it matches a word separator, or to capture matching text as
terms.</p>
</div>
<div class="paragraph">
<p>The default pattern is <code>\W+</code>, which splits text whenever it encounters
non-word characters.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="title">Beware of Pathological Regular Expressions</div>
<div class="paragraph">
<p>The pattern tokenizer uses
<a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html">Java Regular Expressions</a>.</p>
</div>
<div class="paragraph">
<p>A badly written regular expression could run very slowly or even throw a
StackOverflowError and cause the node it is running on to exit suddenly.</p>
</div>
<div class="paragraph">
<p>Read more about <a href="https://www.regular-expressions.info/catastrophic.html">pathological regular expressions and how to avoid them</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<h3 id="_example_output_15" class="discrete">Example output</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">POST _analyze
{
  "tokenizer": "pattern",
  "text": "The foo_bar_size's default is 5."
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above sentence would produce the following terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ The, foo_bar_size, s, default, is, 5 ]</code></pre>
</div>
</div>
<h3 id="_configuration_16" class="discrete">Configuration</h3>
<div class="paragraph">
<p>The <code>pattern</code> tokenizer accepts the following parameters:</p>
</div>
<div class="hdlist">
<table>
<tr>
<td class="hdlist1">
<code>pattern</code>
</td>
<td class="hdlist2">
<p>A <a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html">Java regular expression</a>, defaults to <code>\W+</code>.</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<code>flags</code>
</td>
<td class="hdlist2">
<p>Java regular expression <a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html#field.summary">flags</a>.
Flags should be pipe-separated, eg <code>"CASE_INSENSITIVE|COMMENTS"</code>.</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<code>group</code>
</td>
<td class="hdlist2">
<p>Which capture group to extract as tokens.  Defaults to <code>-1</code> (split).</p>
</td>
</tr>
</table>
</div>
<h3 id="_example_configuration_10" class="discrete">Example configuration</h3>
<div class="paragraph">
<p>In this example, we configure the <code>pattern</code> tokenizer to break text into
tokens when it encounters commas:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "pattern",
          "pattern": ","
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "comma,separated,values"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above example produces the following terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ comma, separated, values ]</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the next example, we configure the <code>pattern</code> tokenizer to capture values
enclosed in double quotes (ignoring embedded escaped quotes <code>\"</code>).  The regex
itself looks like this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"((?:\\"|[^"]|\\")*)"</pre>
</div>
</div>
<div class="paragraph">
<p>And reads as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A literal <code>"</code></p>
</li>
<li>
<p>Start capturing:</p>
<div class="ulist">
<ul>
<li>
<p>A literal <code>\"</code> OR any character except <code>"</code></p>
</li>
<li>
<p>Repeat until no more characters match</p>
</li>
</ul>
</div>
</li>
<li>
<p>A literal closing <code>"</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>When the pattern is specified in JSON, the <code>"</code> and <code>\</code> characters need to be
escaped, so the pattern ends up looking like:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>\"((?:\\\\\"|[^\"]|\\\\\")+)\"</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "pattern",
          "pattern": "\"((?:\\\\\"|[^\"]|\\\\\")+)\"",
          "group": 1
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "\"value\", \"value with embedded \\\" quote\""
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above example produces the following two terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ value, value with embedded \" quote ]</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-simplepattern-tokenizer">Simple pattern tokenizer</h3>
<titleabbrev>Simple pattern</titleabbrev>
<div class="paragraph">
<p>The <code>simple_pattern</code> tokenizer uses a regular expression to capture matching
text as terms. The set of regular expression features it supports is more
limited than the <a href="analysis-tokenizers.html#analysis-pattern-tokenizer"><code>pattern</code></a> tokenizer, but the
tokenization is generally faster.</p>
</div>
<div class="paragraph">
<p>This tokenizer does not support splitting the input on a pattern match, unlike
the <a href="analysis-tokenizers.html#analysis-pattern-tokenizer"><code>pattern</code></a> tokenizer. To split on pattern
matches using the same restricted regular expression subset, see the
<a href="analysis-tokenizers.html#analysis-simplepatternsplit-tokenizer"><code>simple_pattern_split</code></a> tokenizer.</p>
</div>
<div class="paragraph">
<p>This tokenizer uses <a href="https://lucene.apache.org/core/8_7_0/core/org/apache/lucene/util/automaton/RegExp.html">Lucene regular expressions</a>.
For an explanation of the supported features and syntax, see <a href="regexp-syntax.html">Regular Expression Syntax</a>.</p>
</div>
<div class="paragraph">
<p>The default pattern is the empty string, which produces no terms. This
tokenizer should always be configured with a non-default pattern.</p>
</div>
<h3 id="_configuration_17" class="discrete">Configuration</h3>
<div class="paragraph">
<p>The <code>simple_pattern</code> tokenizer accepts the following parameters:</p>
</div>
<div class="hdlist">
<table>
<tr>
<td class="hdlist1">
<code>pattern</code>
</td>
<td class="hdlist2">
<p><a href="https://lucene.apache.org/core/8_7_0/core/org/apache/lucene/util/automaton/RegExp.html">Lucene regular expression</a>, defaults to the empty string.</p>
</td>
</tr>
</table>
</div>
<h3 id="_example_configuration_11" class="discrete">Example configuration</h3>
<div class="paragraph">
<p>This example configures the <code>simple_pattern</code> tokenizer to produce terms that are
three-digit numbers</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "simple_pattern",
          "pattern": "[0123456789]{3}"
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "fd-786-335-514-x"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above example produces these terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ 786, 335, 514 ]</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-simplepatternsplit-tokenizer">Simple pattern split tokenizer</h3>
<titleabbrev>Simple pattern split</titleabbrev>
<div class="paragraph">
<p>The <code>simple_pattern_split</code> tokenizer uses a regular expression to split the
input into terms at pattern matches. The set of regular expression features it
supports is more limited than the <a href="analysis-tokenizers.html#analysis-pattern-tokenizer"><code>pattern</code></a>
tokenizer, but the tokenization is generally faster.</p>
</div>
<div class="paragraph">
<p>This tokenizer does not produce terms from the matches themselves. To produce
terms from matches using patterns in the same restricted regular expression
subset, see the <a href="analysis-tokenizers.html#analysis-simplepattern-tokenizer"><code>simple_pattern</code></a>
tokenizer.</p>
</div>
<div class="paragraph">
<p>This tokenizer uses <a href="https://lucene.apache.org/core/8_7_0/core/org/apache/lucene/util/automaton/RegExp.html">Lucene regular expressions</a>.
For an explanation of the supported features and syntax, see <a href="regexp-syntax.html">Regular Expression Syntax</a>.</p>
</div>
<div class="paragraph">
<p>The default pattern is the empty string, which produces one term containing the
full input. This tokenizer should always be configured with a non-default
pattern.</p>
</div>
<h3 id="_configuration_18" class="discrete">Configuration</h3>
<div class="paragraph">
<p>The <code>simple_pattern_split</code> tokenizer accepts the following parameters:</p>
</div>
<div class="hdlist">
<table>
<tr>
<td class="hdlist1">
<code>pattern</code>
</td>
<td class="hdlist2">
<p>A <a href="https://lucene.apache.org/core/8_7_0/core/org/apache/lucene/util/automaton/RegExp.html">Lucene regular expression</a>, defaults to the empty string.</p>
</td>
</tr>
</table>
</div>
<h3 id="_example_configuration_12" class="discrete">Example configuration</h3>
<div class="paragraph">
<p>This example configures the <code>simple_pattern_split</code> tokenizer to split the input
text on underscores.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "simple_pattern_split",
          "pattern": "_"
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "an_underscored_phrase"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above example produces these terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ an, underscored, phrase ]</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-standard-tokenizer">Standard tokenizer</h3>
<titleabbrev>Standard</titleabbrev>
<div class="paragraph">
<p>The <code>standard</code> tokenizer provides grammar based tokenization (based on the
Unicode Text Segmentation algorithm, as specified in
<a href="https://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>) and works well
for most languages.</p>
</div>
<h3 id="_example_output_16" class="discrete">Example output</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">POST _analyze
{
  "tokenizer": "standard",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above sentence would produce the following terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ The, 2, QUICK, Brown, Foxes, jumped, over, the, lazy, dog's, bone ]</code></pre>
</div>
</div>
<h3 id="_configuration_19" class="discrete">Configuration</h3>
<div class="paragraph">
<p>The <code>standard</code> tokenizer accepts the following parameters:</p>
</div>
<div class="hdlist">
<table>
<tr>
<td class="hdlist1">
<code>max_token_length</code>
</td>
<td class="hdlist2">
<p>The maximum token length. If a token is seen that exceeds this length then
it is split at <code>max_token_length</code> intervals. Defaults to <code>255</code>.</p>
</td>
</tr>
</table>
</div>
<h3 id="_example_configuration_13" class="discrete">Example configuration</h3>
<div class="paragraph">
<p>In this example, we configure the <code>standard</code> tokenizer to have a
<code>max_token_length</code> of 5 (for demonstration purposes):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "standard",
          "max_token_length": 5
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above example produces the following terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ The, 2, QUICK, Brown, Foxes, jumpe, d, over, the, lazy, dog's, bone ]</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-thai-tokenizer">Thai tokenizer</h3>
<titleabbrev>Thai</titleabbrev>
<div class="paragraph">
<p>The <code>thai</code> tokenizer segments Thai text into words, using the Thai
segmentation algorithm included with Java. Text in other languages in general
will be treated the same as the
<a href="analysis-tokenizers.html#analysis-standard-tokenizer"><code>standard</code> tokenizer</a>.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
This tokenizer may not be supported by all JREs. It is known to work
with Sun/Oracle and OpenJDK. If your application needs to be fully portable,
consider using the <a href="https://www.opensearch.org/guide/en/opensearch/plugins/{branch}/analysis-icu-tokenizer.html">ICU Tokenizer</a> instead.
</td>
</tr>
</table>
</div>
<h3 id="_example_output_17" class="discrete">Example output</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">POST _analyze
{
  "tokenizer": "thai",
  "text": ""
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above sentence would produce the following terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ , , , , , , ,  ]</code></pre>
</div>
</div>
<h3 id="_configuration_20" class="discrete">Configuration</h3>
<div class="paragraph">
<p>The <code>thai</code> tokenizer is not configurable.</p>
</div>
</div>
<div class="sect2">
<h3 id="analysis-uaxurlemail-tokenizer">UAX URL email tokenizer</h3>
<titleabbrev>UAX URL email</titleabbrev>
<div class="paragraph">
<p>The <code>uax_url_email</code> tokenizer is like the <a href="analysis-tokenizers.html#analysis-standard-tokenizer"><code>standard</code> tokenizer</a> except that it
recognises URLs and email addresses as single tokens.</p>
</div>
<h3 id="_example_output_18" class="discrete">Example output</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">POST _analyze
{
  "tokenizer": "uax_url_email",
  "text": "Email me at john.smith@global-international.com"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above sentence would produce the following terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ Email, me, at, john.smith@global-international.com ]</code></pre>
</div>
</div>
<div class="paragraph">
<p>while the <code>standard</code> tokenizer would produce:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ Email, me, at, john.smith, global, international.com ]</code></pre>
</div>
</div>
<h3 id="_configuration_21" class="discrete">Configuration</h3>
<div class="paragraph">
<p>The <code>uax_url_email</code> tokenizer accepts the following parameters:</p>
</div>
<div class="hdlist">
<table>
<tr>
<td class="hdlist1">
<code>max_token_length</code>
</td>
<td class="hdlist2">
<p>The maximum token length. If a token is seen that exceeds this length then
it is split at <code>max_token_length</code> intervals. Defaults to <code>255</code>.</p>
</td>
</tr>
</table>
</div>
<h3 id="_example_configuration_14" class="discrete">Example configuration</h3>
<div class="paragraph">
<p>In this example, we configure the <code>uax_url_email</code> tokenizer to have a
<code>max_token_length</code> of 5 (for demonstration purposes):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "uax_url_email",
          "max_token_length": 5
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "john.smith@global-international.com"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above example produces the following terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ john, smith, globa, l, inter, natio, nal.c, om ]</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="analysis-whitespace-tokenizer">Whitespace tokenizer</h3>
<titleabbrev>Whitespace</titleabbrev>
<div class="paragraph">
<p>The <code>whitespace</code> tokenizer breaks text into terms whenever it encounters a
whitespace character.</p>
</div>
<h3 id="_example_output_19" class="discrete">Example output</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">POST _analyze
{
  "tokenizer": "whitespace",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above sentence would produce the following terms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[ The, 2, QUICK, Brown-Foxes, jumped, over, the, lazy, dog's, bone. ]</code></pre>
</div>
</div>
<h3 id="_configuration_22" class="discrete">Configuration</h3>
<div class="paragraph">
<p>The <code>whitespace</code> tokenizer accepts the following parameters:</p>
</div>
<div class="hdlist">
<table>
<tr>
<td class="hdlist1">
<code>max_token_length</code>
</td>
<td class="hdlist2">
<p>The maximum token length. If a token is seen that exceeds this length then
it is split at <code>max_token_length</code> intervals. Defaults to <code>255</code>.</p>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="paragraph nav-footer">
<p>Previous:<a href="analysis-analyzers.html">Built-in analyzer reference</a>| Up:<a href="analysis.html">Text analysis</a>| Home:<a href="index.html">OpenSearch Reference</a>| Next:<a href="analysis-tokenfilters.html">Token filter reference</a></p>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2021-04-12 14:08:20 -0700
</div>
</div>
</body>
</html>